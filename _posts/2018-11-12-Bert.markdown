---
layout: post
title:  "BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding"
subtitle:   "Bert"
categories: cslog
tags: deeplearning
comments: true
---

NLP계의 ImageNet 시대가 열렸다 라고 말할정도로.. 요즘 NLP의 발전이 가속화되었는데, 그 중싱엔 Self-Attention -> Transformer -> BERT (ELMo + Transformer) 가 있다. 오늘 그 핫하다는 BERT에 대해서 한번 알아보자.

### BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding
- 저자:Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova (**Google AI** ~~Google AI니 말다했지~~)


#### Abstract
- BERT는 Bidirectional Encoder Representations from Transformers의 약자임
- 기존 모델의 architecture를 크게 수정 안해도 잘 적용되고 성능 오름
- 8개의 NLP Task에서 new SOTA기록하고, SQuAD v1.1 QA에서 사람보다 2.0 높은 성능 기록함

#### Introduction
- pre-trained LM은 예로부터 NLP의 성능을 올리기에 효과적인 방법이었음
- Pre-trained Language Representation을 적용하는데는 2가지 전략이 있음 (~feature-based~ and ~fine-tuning~)
- feature-based: ELMo (Peters al., 2018)
- fine-tuning: GPT(Generative Pre-trained Transformer) (Radford et al., 2018; OpenAI)
- 기존 연구에선 두 접근법 모두 unidirectional LM이 language representation을 학습하기 위한 pre-trainining시에 같은 objective function을 사용함
- 본 연구에서는 그러한 현재의 기법이 특별히 fine-tuning approach에서는 pre-trained representation의 power를 serverly restrict한다고 주장함
- 주된 한계는 보통의 LM이 unidirectional하다는 것임. 이는 아키텍처의 선택을 제한하게됨.
- 예를들면, OpenAI의 GPT의 경우 left-to-right 구조로써, self-attetnion에서 모든 토큰들이 previous token에만 attention을 갖게됨 (Vaswani et al., 2017)
- 이러한 제한들은 sentence level에서 sub-optimal에 도달할 수 밖에 없게 함(SQuAD같은 Task에서 이러한 구조의 fine-tuning은 치명적)
- 결론: Bidirectional 하게 해야함
- 본 논문에서는 fine-tuning based approach를 BERT라는 기법을 통해 개선시킴! (현재로썬 살짝 GPT에 숟가락 얹은것 같기도..)
- BERT에서는 기존에 비판했던 objective function을 쓰진 않음(left-to-right 구조에 dependent했던). 대신에 MLM(Masked Language Model; Taylor, 1953)의 objective function을 사용함
- MLM은 랜덤하게 input token의 일부를 masking처리 후 그 부분을 예측하는 것을 목표로함.
- MLM objective allows the representation to fuse the left and the right context (해석보단 원문으로)
- 본 논문의 contribution은 [1] Bidirectional!! by MLM [2] model engineering 안해도됨 [3] 8개의 NLP Task에서 SOTA 찍었음. 코드도 공개함(https://github.com/google-research/bert)
- 



#### Related work
##### Feature-based Approaches
- non-neural과 neural(word2vec)한 방법으로 나뉨 
- pre-trained word embedding은 learned from scratch로부터 얻은 embedding보다 확연히 개선된 결과를 보였었음
- ELMo는 traditional word embeddign research를 다른 차원에서 일반화시킴
- ELMo는 context-sensitive features를 LM으로부터 추출함
- contextual word embedding과 기존에 존재하던 모델의 구조들의 결합은 여러 NLP task(QA on SQuAD, SA, NER)에서 SOTA를 기록함

##### Fine-tuning Approaches
- 최근 트렌드라고 할 수 있음, LM에 transfer learning을 적용하는 것임
- LM Objective에 대해서 pre-training 후에 fine-tuning하는 것임
- 장점중 하나는 few parameter가 필요하다는거고, 이러한 기법을 사용한 OpenAI GPT가 GOLUE bechmark에서 SOTA 찍었었음

##### Transfer Learning from Supervised Data
- 최근 Natural Language Inference나 Machine Translation등의 supervised task에 transfer learning이 많이 사용되었음
- CV에서는 transfer learning이 이미 많이 사용됨

##### BERT
- BERT는 multi-layer Bidirectional Transformer encoder를 기반으로 함(tensor2tensor 참고)
- Transformer 자체는 요즘 어디에서나 쓰임(the use of Transformer has become ubiquitous)
- Transformers의 상세한 구조는 본 논문에서 스킵함(다음 링크 참고: http://nlp.seas.harvard.edu/2018/04/03/attention.html)
