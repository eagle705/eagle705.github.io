<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>LLaMA (Open and Efficient Foundation Language Models) - Luke&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Luke&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Luke&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Note Meta가 쏘아올린 작은공 LLaMA 꽤 잘 만든 모델들, 이전의 OPT와는 다르다, 들리는 소문으로는 실험을 꽤 많이 했을 것! 공개함  https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;facebookresearch&amp;#x2F;llama    Author Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A"><meta property="og:type" content="blog"><meta property="og:title" content="LLaMA (Open and Efficient Foundation Language Models)"><meta property="og:url" content="https://eagle705.github.io/llama/"><meta property="og:site_name" content="Luke&#039;s Blog"><meta property="og:description" content="Note Meta가 쏘아올린 작은공 LLaMA 꽤 잘 만든 모델들, 이전의 OPT와는 다르다, 들리는 소문으로는 실험을 꽤 많이 했을 것! 공개함  https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;facebookresearch&amp;#x2F;llama    Author Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/231371204-503a806d-e533-420c-b26b-f690c892d184.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/231387084-1a497915-8506-4277-a5e3-c8cd23427aaf.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234509465-8d712c74-3ecf-4206-a8c5-5259d415c9b9.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234794016-ca375363-8d66-484e-ab6a-d860ad318a87.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234855729-54d3e6c4-f596-450c-a625-397d0d12f790.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234855805-8fb8a69a-3563-419e-9003-2284c69130b2.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234857285-b94a8c1a-81ee-40a6-9a07-b066487535e3.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234858217-547e4fa0-6408-4585-b486-1fea4e8dafe1.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234862505-438dee3a-c574-481b-acbc-74e1c8a49bb6.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234861116-1ae585d0-c627-4228-b55e-8b069c954e7b.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234862954-9dde6a74-71c1-4c51-a5e9-9d45cd49044e.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234863489-f599ac06-5f97-4480-8627-8726172045ad.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234863808-122615e5-2d4b-4ec4-a775-88c6d9a03862.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234864645-4fb4ab64-2f87-455b-a9da-31ed4aedd002.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234865014-d50b8c1c-5604-49c1-b42b-f658da694e10.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234887911-d3634eed-fb4f-4fd9-9697-26aa05323afd.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234888847-8af97d40-cdfc-4d15-8e11-a58049fd2d85.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234891034-91fdba64-8afe-4828-92be-4d6a4e9981f8.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234892982-a52cc245-fe96-48f3-af2f-b23bf5189565.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234894148-8f351900-e558-4e63-85f0-bf1b52a8aab7.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234895870-05df78cd-b40e-4705-8a44-8e39a7ab38ed.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234896381-b3e34a6d-e252-4b3a-a92f-e43902db425a.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/234896349-422c9c27-c78a-41da-8e04-1f72318c8a51.png"><meta property="article:published_time" content="2023-05-09T02:54:39.000Z"><meta property="article:modified_time" content="2023-05-09T04:32:45.455Z"><meta property="article:author" content="Joosung Yoon"><meta property="article:tag" content="nlp"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://user-images.githubusercontent.com/7252598/231371204-503a806d-e533-420c-b26b-f690c892d184.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://eagle705.github.io/llama/"},"headline":"LLaMA (Open and Efficient Foundation Language Models)","image":["https://user-images.githubusercontent.com/7252598/231371204-503a806d-e533-420c-b26b-f690c892d184.png","https://user-images.githubusercontent.com/7252598/231387084-1a497915-8506-4277-a5e3-c8cd23427aaf.png","https://user-images.githubusercontent.com/7252598/234509465-8d712c74-3ecf-4206-a8c5-5259d415c9b9.png","https://user-images.githubusercontent.com/7252598/234794016-ca375363-8d66-484e-ab6a-d860ad318a87.png","https://user-images.githubusercontent.com/7252598/234855729-54d3e6c4-f596-450c-a625-397d0d12f790.png","https://user-images.githubusercontent.com/7252598/234855805-8fb8a69a-3563-419e-9003-2284c69130b2.png","https://user-images.githubusercontent.com/7252598/234857285-b94a8c1a-81ee-40a6-9a07-b066487535e3.png","https://user-images.githubusercontent.com/7252598/234858217-547e4fa0-6408-4585-b486-1fea4e8dafe1.png","https://user-images.githubusercontent.com/7252598/234862505-438dee3a-c574-481b-acbc-74e1c8a49bb6.png","https://user-images.githubusercontent.com/7252598/234861116-1ae585d0-c627-4228-b55e-8b069c954e7b.png","https://user-images.githubusercontent.com/7252598/234862954-9dde6a74-71c1-4c51-a5e9-9d45cd49044e.png","https://user-images.githubusercontent.com/7252598/234863489-f599ac06-5f97-4480-8627-8726172045ad.png","https://user-images.githubusercontent.com/7252598/234863808-122615e5-2d4b-4ec4-a775-88c6d9a03862.png","https://user-images.githubusercontent.com/7252598/234864645-4fb4ab64-2f87-455b-a9da-31ed4aedd002.png","https://user-images.githubusercontent.com/7252598/234865014-d50b8c1c-5604-49c1-b42b-f658da694e10.png","https://user-images.githubusercontent.com/7252598/234887911-d3634eed-fb4f-4fd9-9697-26aa05323afd.png","https://user-images.githubusercontent.com/7252598/234888847-8af97d40-cdfc-4d15-8e11-a58049fd2d85.png","https://user-images.githubusercontent.com/7252598/234891034-91fdba64-8afe-4828-92be-4d6a4e9981f8.png","https://user-images.githubusercontent.com/7252598/234892982-a52cc245-fe96-48f3-af2f-b23bf5189565.png","https://user-images.githubusercontent.com/7252598/234894148-8f351900-e558-4e63-85f0-bf1b52a8aab7.png","https://user-images.githubusercontent.com/7252598/234895870-05df78cd-b40e-4705-8a44-8e39a7ab38ed.png","https://user-images.githubusercontent.com/7252598/234896381-b3e34a6d-e252-4b3a-a92f-e43902db425a.png","https://user-images.githubusercontent.com/7252598/234896349-422c9c27-c78a-41da-8e04-1f72318c8a51.png"],"datePublished":"2023-05-09T02:54:39.000Z","dateModified":"2023-05-09T04:32:45.455Z","author":{"@type":"Person","name":"Joosung Yoon"},"publisher":{"@type":"Organization","name":"Luke's Blog","logo":{"@type":"ImageObject","url":"https://eagle705.github.io/img/eagle705-logo.png"}},"description":"Note Meta가 쏘아올린 작은공 LLaMA 꽤 잘 만든 모델들, 이전의 OPT와는 다르다, 들리는 소문으로는 실험을 꽤 많이 했을 것! 공개함  https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;llama    Author Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A"}</script><link rel="canonical" href="https://eagle705.github.io/llama/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-110980734-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-110980734-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-2655870716902046" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-05-09T02:54:39.000Z" title="5/9/2023, 11:54:39 AM">2023-05-09</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2023-05-09T04:32:45.455Z" title="5/9/2023, 1:32:45 PM">2023-05-09</time>&nbsp;업데이트 됨</span><span class="level-item">22분안에 읽기 (약 3231 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">LLaMA (Open and Efficient Foundation Language Models)</h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>Meta가 쏘아올린 작은공 LLaMA</li>
<li>꽤 잘 만든 모델들, 이전의 OPT와는 다르다, 들리는 소문으로는 실험을 꽤 많이 했을 것!</li>
<li>공개함 <ul>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama">https://github.com/facebookresearch/llama</a></li>
</ul>
</li>
</ul>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><ul>
<li>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., … &amp; Lample, G. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.<ul>
<li>Meta AI</li>
</ul>
</li>
</ul>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><h1 id="Abastract"><a href="#Abastract" class="headerlink" title="Abastract"></a>Abastract</h1><ul>
<li>LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul>
<li>recent work from Hoffmann et al. (2022)(친칠라 논문&#x2F;딥마인드) shows that, for a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data.</li>
<li>For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we find that the performance of a 7B model <code>continues</code> to improve even after 1T tokens.<ul>
<li>토큰수 많을수록 성능은 계속 증가하는 편이더라~</li>
</ul>
</li>
<li>Unlike Chinchilla, PaLM, or GPT-3, <code>we only use publicly available data</code>, making our work compatible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g. “Books – 2TB” or “Social media conversations”).</li>
<li>In the rest of this paper, we present an overview of the modifications we made to the transformer architecture (Vaswani et al., 2017), as well as our training method.</li>
</ul>
<h1 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h1><h2 id="Pre-training-Data"><a href="#Pre-training-Data" class="headerlink" title="Pre-training Data"></a>Pre-training Data</h2><ul>
<li>Our training dataset is a mixture of several sources, reported in Table 1<br><img src="https://user-images.githubusercontent.com/7252598/231371204-503a806d-e533-420c-b26b-f690c892d184.png" alt="image"></li>
</ul>
<h3 id="English-CommonCrawl-67"><a href="#English-CommonCrawl-67" class="headerlink" title="English CommonCrawl [67%]"></a>English CommonCrawl [67%]</h3><ul>
<li>preprocess five CommonCrawl dumps, ranging from 2017 to 2020, with the CCNet pipeline</li>
<li>This process deduplicates the data at the <code>line level</code>, performs <code>language identification</code> with a fastText linear classifier to remove non-English pages and filters <code>low quality content with an n-gram language model</code></li>
<li>we trained a <code>linear model</code> to classify pages used as <code>references</code> in <code>Wikipedia v.s. randomly sampled pages</code><ul>
<li>discarded pages not classified as references.<ul>
<li>Wikipedia 스타일을 살렸다는걸까? 뭐지</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="C4-15"><a href="#C4-15" class="headerlink" title="C4 [15%]"></a>C4 [15%]</h3><ul>
<li>During exploratory experiments, we observed that using diverse pre-processed CommonCrawl datasets improves performance. <ul>
<li>실험해보니 C4 넣으면 성능이 올라가더라</li>
</ul>
</li>
<li>The preprocessing of C4 also contains deduplication and language identification steps:<ul>
<li>the main difference with CCNet is the <code>quality filtering</code>, which mostly relies on <code>heuristics</code> such as <code>presence of punctuation marks</code> or the <code>number of words</code> and <code>sentences</code> <code>in a webpage</code>.<ul>
<li>단어수, 문장수, <code>.</code>이 있냐없냐 등등을 기준으로 휴리스틱하게 퀄리티 평가</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Github-4-5"><a href="#Github-4-5" class="headerlink" title="Github [4.5%]"></a>Github [4.5%]</h3><ul>
<li>We use the <a target="_blank" rel="noopener" href="https://cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open-source-code?hl=en">public GitHub dataset available on Google BigQuery</a></li>
<li>We only kept projects that are distributed under the Apache, BSD and MIT licenses<ul>
<li>라이센스별로 필터링</li>
</ul>
</li>
<li>Additionally, we filtered low quality files with heuristics based on the line length or proportion of alphanumeric characters, and removed boilerplate, such as headers, with regular expressions.<ul>
<li>km^2 같은 캐릭터 비율이나 보일러플레이트나 헤더등을 정규식으로 파악 후 파일 제외시킴</li>
</ul>
</li>
</ul>
<h3 id="Wikipedia-4-5"><a href="#Wikipedia-4-5" class="headerlink" title="Wikipedia [4.5%]"></a>Wikipedia [4.5%]</h3><ul>
<li>Wikipedia [4.5%]. We add Wikipedia dumps from the June-August 2022 period, covering 20 languages, which use either the Latin or Cyrillic scripts: <code>bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk.</code></li>
<li>We process the data to remove hyperlinks, comments and other formatting boilerplate. (링크, 댓글제거)</li>
</ul>
<h3 id="Gutenberg-and-Books3-4-5"><a href="#Gutenberg-and-Books3-4-5" class="headerlink" title="Gutenberg and Books3 [4.5%]"></a>Gutenberg and Books3 [4.5%]</h3><ul>
<li>We perform deduplication at the <code>book level</code>, <code>removing books with more than 90% content overlap</code>.<ul>
<li>90퍼 겹치면 책 제거 (책 레벨에서 어떻게 했을까 양이 많은데)</li>
</ul>
</li>
</ul>
<h3 id="ArXiv-2-5"><a href="#ArXiv-2-5" class="headerlink" title="ArXiv [2.5%]"></a>ArXiv [2.5%]</h3><ul>
<li>We process arXiv <code>Latex files</code> to add scientific data to our dataset.</li>
<li>Following Lewkowycz et al. (2022), we <code>removed everything before the first section</code>, <code>as well as the bibliography</code>. We also <code>removed the comments from the .tex files</code>, and inline-expanded definitions and macros written by users to increase consistency across papers.</li>
</ul>
<h3 id="Stack-Exchange-2"><a href="#Stack-Exchange-2" class="headerlink" title="Stack Exchange [2%]"></a>Stack Exchange [2%]</h3><ul>
<li>a website of high quality questions and answers that covers a diverse set of domains, ranging from computer science to chemistry. </li>
<li>We kept the data from the 28 largest websites, <code>removed the HTML tags</code> from text and <code>sorted the answers by score</code></li>
</ul>
<h3 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h3><ul>
<li>byte-pair encoding (BPE) algorithm<ul>
<li>using the implementation from SentencePiece (Kudo and Richardson, 2018)</li>
</ul>
</li>
<li>we <code>split all numbers into individual digits</code>, and <code>fallback to bytes</code> to <code>decompose unknown UTF-8 characters</code></li>
<li>Overall, our <code>entire training dataset contains roughly 1.4T tokens</code> after tokenization.</li>
<li>For most of our training data, each token is used only once dur- ing training, with the exception of the Wikipedia and Books domains, over which we perform approximately two epochs.<ul>
<li>거의 한번만 보고 위키나 책 몇개는 두번정도 본다</li>
</ul>
</li>
</ul>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><ul>
<li>We leverage various improvements that were subsequently proposed, and used in different models such as PaLM. Here are the main difference with the original architec- ture, and where we were found the inspiration for this change (in bracket):</li>
</ul>
<h3 id="Pre-normalization-GPT3"><a href="#Pre-normalization-GPT3" class="headerlink" title="Pre-normalization [GPT3]"></a>Pre-normalization [GPT3]</h3><ul>
<li>To improve the training stability, we normalize the input of each transformer sub-layer, instead of normalizing the output</li>
<li>We use the RMSNorm normalizing function, introduced by Zhang and Sennrich (2019)<ul>
<li>이거 T5쪽에서도 apex 쓰면 사용되던건데</li>
</ul>
</li>
</ul>
<h3 id="SwiGLU-activation-function-PaLM"><a href="#SwiGLU-activation-function-PaLM" class="headerlink" title="SwiGLU activation function [PaLM]"></a>SwiGLU activation function [PaLM]</h3><ul>
<li>We replace the ReLU non-linearity by the SwiGLU activation function, introduced by Shazeer (2020)</li>
<li>We use a dimension of <code>2/3 * 4d</code> instead of <code>4d as in PaLM</code> (이건 나중에 코드 봐야할듯)</li>
</ul>
<h3 id="Rotary-Embeddings-GPTNeo"><a href="#Rotary-Embeddings-GPTNeo" class="headerlink" title="Rotary Embeddings [GPTNeo]"></a>Rotary Embeddings [GPTNeo]</h3><ul>
<li>We <code>remove the absolute positional embeddings</code>, and instead, add rotary positional embeddings (RoPE)</li>
</ul>
<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><ul>
<li>Our models are trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with the following hyper-parameters: β1 &#x3D; 0.9, β2 &#x3D; 0.95.</li>
<li>We use a <code>cosine learning rate schedule</code>, such that the <code>final learning rate is equal to 10% of the maximal</code> learning rate</li>
<li><code>weight decay of 0.1</code> and <code>gradient clipping of 1.0</code><ul>
<li>clipping 값이 꽤 높다?! 요즘 0.3을 많이 쓰는거 같던데</li>
</ul>
</li>
<li>use 2,000 warmup steps, and vary the learning rate and batch size with the size of the model (see Table 2 for details).<br><img src="https://user-images.githubusercontent.com/7252598/231387084-1a497915-8506-4277-a5e3-c8cd23427aaf.png" alt="image"></li>
</ul>
<h2 id="Efficient-Implementation"><a href="#Efficient-Implementation" class="headerlink" title="Efficient Implementation"></a>Efficient Implementation</h2><ul>
<li>First, we use an <code>efficient implementation</code> of the <code>causal multi-head attention</code> <code>to reduce memory usage and runtime</code>. This implementation, available in the <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/xformers">xformers library</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/xformers">xformers library</a> is inspired by Rabe and Staats (2021 &#x2F; <code>self-attention does not need o(n2) memory</code>.) and uses the backward from Dao et al. (2022 &#x2F; <code>Flashattention</code>: Fast and memory-efficient exact attention with io-awareness)</li>
<li>This is achieved by <strong>not storing the attention weights and not computing the key&#x2F;query scores that are masked</strong> due to the causal nature of the language modeling task.<ul>
<li>마스킹되는 애들은 계산하지 않는다?!!로 구현됨<ul>
<li><img src="https://user-images.githubusercontent.com/7252598/234509465-8d712c74-3ecf-4206-a8c5-5259d415c9b9.png" alt="image"></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>To <strong>further</strong> improve training <strong>efficiency</strong>, we <strong>reduced the amount of activations that are recomputed during the backward</strong> pass with checkpointing<ul>
<li>we save the activations that are <strong>expensive to compute</strong>, such as the outputs of linear layers</li>
<li>This is achieved by manually implementing the backward function for the transformer layers, instead of relying on the PyTorch autograd. (수작업!!)</li>
<li>reduce the memory usage of the model by using <strong>model and sequence parallelism</strong>, as described by Korthikanti et al. (2022)</li>
<li>we also overlap the computation of activations and the communication between GPUs over the network (due to all_reduce operations) as much as possible. (GPU들 최대한 잘 통신할 수 있게 했다?)</li>
</ul>
</li>
<li>When training a 65B-parameter model, our code processes around <code>380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM</code>. This means that training over our dataset containing 1.4T tokens takes approximately <strong>21 days</strong>.</li>
</ul>
<h1 id="Main-results"><a href="#Main-results" class="headerlink" title="Main results"></a>Main results</h1><ul>
<li>zero-shot and few-shot(1~64) tasks, and report results on a total of 20 benchmarks:<ul>
<li>Zero-shot: provides an answer using open-ended generation, or ranks the proposed answers.</li>
<li>Few-shot: (between 1 and 64) and a test example. The model takes this text as input and generates the answer or ranks different options</li>
</ul>
</li>
<li>We evaluate LLaMA on free-form generation tasks and multiple choice tasks</li>
<li>We follow Gao et al. (2021) and use the likelihood normalized by the number of characters in the completion, except for certain datasets (OpenBookQA, BoolQ), for which we follow Brown et al. (2020), and select a completion based on the <code>likelihood normalized by the likelihood of the completion given “Answer:” as context: P (completion|context)/P(completion|“Answer:”)</code></li>
</ul>
<h2 id="Common-Sense-Reasoning"><a href="#Common-Sense-Reasoning" class="headerlink" title="Common Sense Reasoning"></a>Common Sense Reasoning</h2><ul>
<li>consider <code>eight standard common sense reasoning benchmarks</code>: <code>BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC easy and challenge (Clark et al., 2018) and OpenBookQA (Mihaylov et al., 2018)</code>. </li>
<li>These datasets include <code>Cloze</code> and <code>Winograd style</code> tasks, as well as <code>multiple choice question answering</code></li>
<li>We evaluate in the <code>zero-shot setting</code> as done in the language modeling community<br><img src="https://user-images.githubusercontent.com/7252598/234794016-ca375363-8d66-484e-ab6a-d860ad318a87.png" alt="image"></li>
<li>First, <code>LLaMA-65B outperforms Chinchilla-70B</code> on all reported benchmarks but BoolQ.</li>
<li>Similarly, this model surpasses PaLM- 540B everywhere but on BoolQ and WinoGrande</li>
<li>LLaMA-13B model also outperforms GPT-3 on most benchmarks despite being 10× smaller.</li>
</ul>
<h2 id="Closed-book-Question-Answering"><a href="#Closed-book-Question-Answering" class="headerlink" title="Closed-book Question Answering"></a>Closed-book Question Answering</h2><ul>
<li>compare LLaMA to existing large language models on <code>two closed-book question answering</code> benchmarks: <code>Natural Questions</code> (Kwiatkowski et al., 2019) and <code>TriviaQA</code> (Joshi et al., 2017)</li>
<li>LLaMA-65B achieve state-of-the-arts performance in the zero-shot and few-shot settings</li>
<li>More importantly, the LLaMA-13B is also competitive on these benchmarks with GPT-3 and Chinchilla, despite being 5-10× smaller. This model runs on a single V100 GPU during inference</li>
</ul>
<table>
<thead>
<tr>
<th>Table 4</th>
<th>Table 5</th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://user-images.githubusercontent.com/7252598/234855729-54d3e6c4-f596-450c-a625-397d0d12f790.png" alt="image"></td>
<td><img src="https://user-images.githubusercontent.com/7252598/234855805-8fb8a69a-3563-419e-9003-2284c69130b2.png" alt="image"></td>
</tr>
</tbody></table>
<h2 id="Reading-Comprehension"><a href="#Reading-Comprehension" class="headerlink" title="Reading Comprehension"></a>Reading Comprehension</h2><ul>
<li>RACE reading comprehension benchmark<ul>
<li>English reading comprehension exams designed for middle and high school Chinese students.</li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/234857285-b94a8c1a-81ee-40a6-9a07-b066487535e3.png" alt="image"></p>
<h2 id="Mathematical-reasoning"><a href="#Mathematical-reasoning" class="headerlink" title="Mathematical reasoning"></a>Mathematical reasoning</h2><ul>
<li>MATH is a dataset of 12K <code>middle school and high school mathematics problems</code> written in &#96;LaTeX&#96;&#96;</li>
<li>GSM8k is a set of <code>middle</code> school mathematical problems</li>
<li><code>Minerva is a series of PaLM models</code> finetuned on 38.5B tokens extracted from ArXiv and Math Web Pages <strong>while neither PaLM or LLaMA</strong> are finetuned on mathematical data.</li>
<li>we compare with and without <code>maj1@k. maj1@k</code> denotes evaluations where we generate <code>k samples</code> for each problem and perform a <code>majority voting</code> (Wang et al., 2022)</li>
<li><code>LLaMA- 65B outperforms</code> Minerva-62B, although it has <code>not been fine-tuned on mathematical data</code>.</li>
<li>이정도 성능이면 진짜 좋은편이긴함</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/234858217-547e4fa0-6408-4585-b486-1fea4e8dafe1.png" alt="image"></p>
<h2 id="Code-generation"><a href="#Code-generation" class="headerlink" title="Code generation"></a>Code generation</h2><ul>
<li>model receives a <code>description of the program in a few sentences</code>, as well as a few <code>input-output examples</code></li>
<li>In HumanEval, it also receives a function signature, and the prompt is formatted as natural code with the textual description and tests in a docstring<ul>
<li>The model needs to generate a Python program that fits the description and satisfies the test cases.</li>
</ul>
</li>
<li>we compare the <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.03374.pdf">pass@1 scores</a> of our models with existing language models that have not been finetuned on code, namely PaLM and LaMDA (Thoppilan et al., 2022). PaLM and LLaMA were trained on datasets that contain a <code>similar number of code tokens</code>.</li>
<li>LLaMA outperforms other general models such as LaMDA and PaLM, which are not trained or finetuned specifically for code.</li>
<li>추가적인 파인튜닝통해서 개선도 가능<ul>
<li>PaLM-Coder (Chowdhery et al., 2022) increases the pass@1 score of PaLM on HumanEval from 26.2% for PaLM to 36%</li>
</ul>
</li>
<li>metric은 논문 참조 (Evaluating Large Language Models Trained on Code)<ul>
<li>k는 문제당 생성되는 k개의 코드 샘플을 의미함 (pass@k metric, where k code samples are generated per problem)</li>
<li><img src="https://user-images.githubusercontent.com/7252598/234862505-438dee3a-c574-481b-acbc-74e1c8a49bb6.png" alt="image"></li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/234861116-1ae585d0-c627-4228-b55e-8b069c954e7b.png" alt="image"></p>
<h2 id="Massive-Multitask-Language-Understanding-MMLU"><a href="#Massive-Multitask-Language-Understanding-MMLU" class="headerlink" title="Massive Multitask Language Understanding (MMLU)"></a>Massive Multitask Language Understanding (MMLU)</h2><ul>
<li>The massive multitask language understanding benchmark, or MMLU<ul>
<li>consists of multiple choice questions covering various domains of knowledge, including humanities, STEM and social sciences (중고등학교 문제같은것들)</li>
</ul>
</li>
<li>evaluate our models in the <code>5-shot setting</code>, using the examples provided by the benchmark</li>
<li>A potential explanation is that we have used a <code>limited amount</code> of books and academic papers in our pre-training data, i.e., ArXiv, Gutenberg and Books3, that sums up to only 177GB, while these models(Chinchilla- 70B and PaLM-540B) were trained on up to 2TB of books</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/234862954-9dde6a74-71c1-4c51-a5e9-9d45cd49044e.png" alt="image"></p>
<h2 id="Evolution-of-performance-during-training"><a href="#Evolution-of-performance-during-training" class="headerlink" title="Evolution of performance during training"></a>Evolution of performance during training</h2><ul>
<li>During training, we tracked the performance of our models on a few question answering and common sense benchmarks<br><img src="https://user-images.githubusercontent.com/7252598/234863489-f599ac06-5f97-4480-8627-8726172045ad.png" alt="image"></li>
<li>the performance improves steadily, and correlates with the training perplexity of the model (see Figure 1).</li>
<li>해석<ul>
<li>on SIQA, we observe a lot of variance in performance, that may indicate that this benchmark <code>is not reliable</code></li>
<li>On WinoGrande, the performance does not correlate as well with training perplexity: the LLaMA-33B and LLaMA-65B have similar performance during the training.</li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/234863808-122615e5-2d4b-4ec4-a775-88c6d9a03862.png" alt="image"></p>
<h1 id="Instruction-Finetuning"><a href="#Instruction-Finetuning" class="headerlink" title="Instruction Finetuning"></a>Instruction Finetuning</h1><ul>
<li>show that briefly <code>finetuning on instructions</code> data rapidly leads to <code>improvements on MMLU</code></li>
<li>Since this is <code>not the focus</code> of this paper, we only conducted a single experiment following the same protocol as Chung et al. (2022) <code>to train an instruct model, LLaMA-I</code> (Instruction 튜닝이 논문 목표는 아니라서 짧게만 보겠다)<br><img src="https://user-images.githubusercontent.com/7252598/234864645-4fb4ab64-2f87-455b-a9da-31ed4aedd002.png" alt="image"></li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/234865014-d50b8c1c-5604-49c1-b42b-f658da694e10.png" alt="image"></p>
<h1 id="Bias-Toxicity-and-Misinformation"><a href="#Bias-Toxicity-and-Misinformation" class="headerlink" title="Bias, Toxicity and Misinformation"></a>Bias, Toxicity and Misinformation</h1><ul>
<li>Large language models have been showed to reproduce and amplify biases that are existing in the training data and to generate toxic or offensive content</li>
<li>we evaluate on different benchmarks that <code>measure toxic content production and stereotypes detection</code></li>
</ul>
<h2 id="RealToxicityPrompts"><a href="#RealToxicityPrompts" class="headerlink" title="RealToxicityPrompts"></a>RealToxicityPrompts</h2><ul>
<li>RealToxicityPrompts consists of about <code>100k prompts</code> that the model <code>must complete</code></li>
<li>toxicity score is automatically evaluated by making a request to <a target="_blank" rel="noopener" href="https://perspectiveapi.com/">PerspectiveAPI</a> </li>
<li>The score per prompt ranges from 0 (non-toxic) to 1 (toxic)</li>
<li>These scores are “comparable” with what we observe in the literature (e.g., 0.087 for Chinchilla) but the methodologies differ between these work and ours<br><img src="https://user-images.githubusercontent.com/7252598/234887911-d3634eed-fb4f-4fd9-9697-26aa05323afd.png" alt="image"></li>
</ul>
<h2 id="CrowS-Pairs"><a href="#CrowS-Pairs" class="headerlink" title="CrowS-Pairs"></a>CrowS-Pairs</h2><ul>
<li>This dataset allows to measure biases in 9 categories: <code>gender, religion, race/color, sexual orientation, age, nationality, disability, physical appearance and socioeconomic status</code>.</li>
<li>Each example is composed of a <code>stereotype and an anti-stereotype</code>, we measure the model <code>preference</code> for the stereotypical sentence <code>using the perplexity</code> of both sentences in a <code>zero-shot</code> setting</li>
<li>평균값 자체는 우위에 있는데 앞서는 영역 개수는 GPT3가 제일 나아보이기도함</li>
<li>Pythia에서도 사용한 벤치마크셋</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/234888847-8af97d40-cdfc-4d15-8e11-a58049fd2d85.png" alt="image"></p>
<h2 id="WinoGender"><a href="#WinoGender" class="headerlink" title="WinoGender"></a>WinoGender</h2><ul>
<li>WinoGender benchmark (Rudinger et al., 2018), a co-reference resolution dataset. WinoGender is made of Winograd schema, and biases are evaluated by determining if a model <code>co-reference resolution</code> performance is impacted by the <code>gender</code> of the <code>pronoun</code>.</li>
<li>대명사에 대한 젠더 바이어스를 corefence 문제로 측정함 Pythia에서도 사용한 벤치마크셋</li>
<li>More precisely, each sentence has three mentions: an “occupation”, a “participant”, and a “pronoun” where the pronoun is co-referencing either the occupation or participant</li>
<li><strong>“The nurse notified the patient that his shift would be ending in an hour.”,</strong> which is followed by <strong>‘His’ refers to</strong>. We then compare the <strong>perplexity</strong> of the continuations the <strong>nurse</strong> and the <strong>patient</strong> to perform co-reference resolution with the model. We evaluate the performance when using 3 pronouns: “her&#x2F;her&#x2F;she”, “his&#x2F;him&#x2F;he” and “their&#x2F;them&#x2F;someone” (the different choices corresponding to the grammatical function of the pronoun.<br><img src="https://user-images.githubusercontent.com/7252598/234891034-91fdba64-8afe-4828-92be-4d6a4e9981f8.png" alt="image"></li>
</ul>
<h2 id="TruthfulQA"><a href="#TruthfulQA" class="headerlink" title="TruthfulQA"></a>TruthfulQA</h2><ul>
<li>measure the truthfulness of a model, i.e., its ability to identify when a claim is true.</li>
<li>the definition of “true” in the sense of “literal truth about the real world”</li>
<li>This benchmark can evaluate the risks of a model to generate misinformation or false claims. The questions are written in diverse style, <code>cover 38 categories</code> and are designed to be adversarial.</li>
<li>Compared to GPT-3, our model scores higher in both categories, but the rate of correct answers is still low, showing that our model is likely to <code>hallucinate incorrect answers</code>.</li>
<li>수박씨 문제 같은 것! 을 잘 대답할 수 있는지 측정하는 벤치마크<br><img src="https://user-images.githubusercontent.com/7252598/234892982-a52cc245-fe96-48f3-af2f-b23bf5189565.png" alt="image"></li>
</ul>
<h1 id="Carbon-footprint"><a href="#Carbon-footprint" class="headerlink" title="Carbon footprint"></a>Carbon footprint</h1><ul>
<li>The training of our models have consumed a massive quantity of energy, responsible for the emission of carbon dioxide</li>
<li><code>Wh = GPU-h×(GPU power consumption)×Power Usage Effectiveness(PUE -&gt; 1.1)</code></li>
<li><code>tCO2eq = MWh × 0.385</code></li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/234894148-8f351900-e558-4e63-85f0-bf1b52a8aab7.png" alt="image"></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul>
<li>presented a series of language models that are released openly, and competitive with state-of-the-art foundation models</li>
<li>LLaMA-13B outperforms GPT-3 while being more than 10× smaller, and LLaMA-65B is competitive with Chinchilla-70B and PaLM-540B</li>
<li>Unlike previous studies, we show that it is possible to achieve state-of-the-art performance by training <code>exclusively on publicly available data</code> (이게 중요하지~)</li>
<li>Finally, we plan to release larger models trained on larger pretraining corpora in the future</li>
</ul>
<h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><p><img src="https://user-images.githubusercontent.com/7252598/234895870-05df78cd-b40e-4705-8a44-8e39a7ab38ed.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/234896381-b3e34a6d-e252-4b3a-a92f-e43902db425a.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/234896349-422c9c27-c78a-41da-8e04-1f72318c8a51.png" alt="image"></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>LLaMA (Open and Efficient Foundation Language Models)</p><p><a href="https://eagle705.github.io/llama/">https://eagle705.github.io/llama/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Joosung Yoon</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-05-09</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-05-09</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/nlp/">nlp</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=63297c6228f9450019a5f574&amp;product=sop" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/pythia/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Pythia (A Suite for Analyzing Large Language Models Across Training and Scaling)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/ia3/"><span class="level-item">(IA3) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://eagle705.github.io/llama/';
            this.page.identifier = 'llama/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eagle705-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/eagle705-logo.png" alt="Joosung Yoon"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joosung Yoon</p><p class="is-size-6 is-block">Machine Learning Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>There and Back Again</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">54</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/eagle705" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/eagle705"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hisdevelopers"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JSYoon53859120"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/joosung-yoon/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cslog/"><span class="level-start"><span class="level-item">cslog</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/categories/photo/"><span class="level-start"><span class="level-item">photo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">5월 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">3월 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">2월 2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">1월 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">12월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">11월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">10월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">9월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">8월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">5월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">12월 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">12월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">11월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">10월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">9월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">8월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">7월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">5월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">4월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">11월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">6월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">5월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">4월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/drone/"><span class="tag">drone</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/grpc/"><span class="tag">grpc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">43</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp-kb/"><span class="tag">nlp, kb</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"><span class="tag">생각정리</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">광고</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-2655870716902046" data-ad-slot="2764253071" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Note"><span class="level-left"><span class="level-item">1</span><span class="level-item">Note</span></span></a></li><li><a class="level is-mobile" href="#Author"><span class="level-left"><span class="level-item">2</span><span class="level-item">Author</span></span></a></li><li><a class="level is-mobile" href="#Summary"><span class="level-left"><span class="level-item">3</span><span class="level-item">Summary</span></span></a></li><li><a class="level is-mobile" href="#Abastract"><span class="level-left"><span class="level-item">4</span><span class="level-item">Abastract</span></span></a></li><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">5</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Approach"><span class="level-left"><span class="level-item">6</span><span class="level-item">Approach</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Pre-training-Data"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">Pre-training Data</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#English-CommonCrawl-67"><span class="level-left"><span class="level-item">6.1.1</span><span class="level-item">English CommonCrawl [67%]</span></span></a></li><li><a class="level is-mobile" href="#C4-15"><span class="level-left"><span class="level-item">6.1.2</span><span class="level-item">C4 [15%]</span></span></a></li><li><a class="level is-mobile" href="#Github-4-5"><span class="level-left"><span class="level-item">6.1.3</span><span class="level-item">Github [4.5%]</span></span></a></li><li><a class="level is-mobile" href="#Wikipedia-4-5"><span class="level-left"><span class="level-item">6.1.4</span><span class="level-item">Wikipedia [4.5%]</span></span></a></li><li><a class="level is-mobile" href="#Gutenberg-and-Books3-4-5"><span class="level-left"><span class="level-item">6.1.5</span><span class="level-item">Gutenberg and Books3 [4.5%]</span></span></a></li><li><a class="level is-mobile" href="#ArXiv-2-5"><span class="level-left"><span class="level-item">6.1.6</span><span class="level-item">ArXiv [2.5%]</span></span></a></li><li><a class="level is-mobile" href="#Stack-Exchange-2"><span class="level-left"><span class="level-item">6.1.7</span><span class="level-item">Stack Exchange [2%]</span></span></a></li><li><a class="level is-mobile" href="#Tokenizer"><span class="level-left"><span class="level-item">6.1.8</span><span class="level-item">Tokenizer</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Architecture"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">Architecture</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Pre-normalization-GPT3"><span class="level-left"><span class="level-item">6.2.1</span><span class="level-item">Pre-normalization [GPT3]</span></span></a></li><li><a class="level is-mobile" href="#SwiGLU-activation-function-PaLM"><span class="level-left"><span class="level-item">6.2.2</span><span class="level-item">SwiGLU activation function [PaLM]</span></span></a></li><li><a class="level is-mobile" href="#Rotary-Embeddings-GPTNeo"><span class="level-left"><span class="level-item">6.2.3</span><span class="level-item">Rotary Embeddings [GPTNeo]</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Optimizer"><span class="level-left"><span class="level-item">6.3</span><span class="level-item">Optimizer</span></span></a></li><li><a class="level is-mobile" href="#Efficient-Implementation"><span class="level-left"><span class="level-item">6.4</span><span class="level-item">Efficient Implementation</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Main-results"><span class="level-left"><span class="level-item">7</span><span class="level-item">Main results</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Common-Sense-Reasoning"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">Common Sense Reasoning</span></span></a></li><li><a class="level is-mobile" href="#Closed-book-Question-Answering"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">Closed-book Question Answering</span></span></a></li><li><a class="level is-mobile" href="#Reading-Comprehension"><span class="level-left"><span class="level-item">7.3</span><span class="level-item">Reading Comprehension</span></span></a></li><li><a class="level is-mobile" href="#Mathematical-reasoning"><span class="level-left"><span class="level-item">7.4</span><span class="level-item">Mathematical reasoning</span></span></a></li><li><a class="level is-mobile" href="#Code-generation"><span class="level-left"><span class="level-item">7.5</span><span class="level-item">Code generation</span></span></a></li><li><a class="level is-mobile" href="#Massive-Multitask-Language-Understanding-MMLU"><span class="level-left"><span class="level-item">7.6</span><span class="level-item">Massive Multitask Language Understanding (MMLU)</span></span></a></li><li><a class="level is-mobile" href="#Evolution-of-performance-during-training"><span class="level-left"><span class="level-item">7.7</span><span class="level-item">Evolution of performance during training</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Instruction-Finetuning"><span class="level-left"><span class="level-item">8</span><span class="level-item">Instruction Finetuning</span></span></a></li><li><a class="level is-mobile" href="#Bias-Toxicity-and-Misinformation"><span class="level-left"><span class="level-item">9</span><span class="level-item">Bias, Toxicity and Misinformation</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#RealToxicityPrompts"><span class="level-left"><span class="level-item">9.1</span><span class="level-item">RealToxicityPrompts</span></span></a></li><li><a class="level is-mobile" href="#CrowS-Pairs"><span class="level-left"><span class="level-item">9.2</span><span class="level-item">CrowS-Pairs</span></span></a></li><li><a class="level is-mobile" href="#WinoGender"><span class="level-left"><span class="level-item">9.3</span><span class="level-item">WinoGender</span></span></a></li><li><a class="level is-mobile" href="#TruthfulQA"><span class="level-left"><span class="level-item">9.4</span><span class="level-item">TruthfulQA</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Carbon-footprint"><span class="level-left"><span class="level-item">10</span><span class="level-item">Carbon footprint</span></span></a></li><li><a class="level is-mobile" href="#Conclusion"><span class="level-left"><span class="level-item">11</span><span class="level-item">Conclusion</span></span></a></li><li><a class="level is-mobile" href="#Appendix"><span class="level-left"><span class="level-item">12</span><span class="level-item">Appendix</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:53.000Z">2023-05-09</time></p><p class="title"><a href="/pythia/">Pythia (A Suite for Analyzing Large Language Models Across Training and Scaling)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:39.000Z">2023-05-09</time></p><p class="title"><a href="/llama/">LLaMA (Open and Efficient Foundation Language Models)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:01.000Z">2023-05-09</time></p><p class="title"><a href="/ia3/">(IA3) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-23T08:14:37.000Z">2023-03-23</time></p><p class="title"><a href="/Alpaca/">Alpaca (A Strong Instruction-Following Model)</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-20T13:07:50.000Z">2023-02-20</time></p><p class="title"><a href="/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/">SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기</a></p><p class="categories"><a href="/categories/ML/">ML</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 Joosung Yoon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>