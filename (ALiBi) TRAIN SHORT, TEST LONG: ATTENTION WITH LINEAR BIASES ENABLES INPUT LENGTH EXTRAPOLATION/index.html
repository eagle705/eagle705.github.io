<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION - Luke&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Luke&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Luke&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Ref https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;ofirpress&amp;#x2F;attention_with_linear_biases huggingface 구현 https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;huggingface&amp;#x2F;transformers&amp;#x2F;blob&amp;#x2F;ad28ca291bf851b48d7f2d4becf96ca90c98f8f1&amp;#x2F;src&amp;#x2F;transformers&amp;#x2F;models&amp;#x2F;bloom&amp;#x2F;mod"><meta property="og:type" content="blog"><meta property="og:title" content="(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION"><meta property="og:url" content="https://eagle705.github.io/(ALiBi)%20TRAIN%20SHORT,%20TEST%20LONG:%20ATTENTION%20WITH%20LINEAR%20BIASES%20ENABLES%20INPUT%20LENGTH%20EXTRAPOLATION/"><meta property="og:site_name" content="Luke&#039;s Blog"><meta property="og:description" content="Ref https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;ofirpress&amp;#x2F;attention_with_linear_biases huggingface 구현 https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;huggingface&amp;#x2F;transformers&amp;#x2F;blob&amp;#x2F;ad28ca291bf851b48d7f2d4becf96ca90c98f8f1&amp;#x2F;src&amp;#x2F;transformers&amp;#x2F;models&amp;#x2F;bloom&amp;#x2F;mod"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/178661915-adcc3a20-c0c3-4199-be75-9b68b005d9cb.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/178676014-ac42f75b-245d-4c12-9ef8-2b262d1d0e29.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/178683688-0c6fdf7d-ea04-4f42-a26e-94213625793a.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/178702621-584132b5-03f9-4fa5-a4b9-ca35f09c6421.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/178723716-a70eeb0f-1dd2-49a6-970a-e9892241b800.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/178723943-1dbae7de-0099-4e29-b54b-2783a1b6e564.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/178724180-84712785-16f1-4c42-a67a-ce833fc6ba56.png"><meta property="article:published_time" content="2022-07-13T03:00:00.000Z"><meta property="article:modified_time" content="2022-08-30T04:36:53.547Z"><meta property="article:author" content="Joosung Yoon"><meta property="article:tag" content="nlp"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://user-images.githubusercontent.com/7252598/178661915-adcc3a20-c0c3-4199-be75-9b68b005d9cb.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://eagle705.github.io/(ALiBi)%20TRAIN%20SHORT,%20TEST%20LONG:%20ATTENTION%20WITH%20LINEAR%20BIASES%20ENABLES%20INPUT%20LENGTH%20EXTRAPOLATION/"},"headline":"(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION","image":["https://user-images.githubusercontent.com/7252598/178661915-adcc3a20-c0c3-4199-be75-9b68b005d9cb.png","https://user-images.githubusercontent.com/7252598/178676014-ac42f75b-245d-4c12-9ef8-2b262d1d0e29.png","https://user-images.githubusercontent.com/7252598/178683688-0c6fdf7d-ea04-4f42-a26e-94213625793a.png","https://user-images.githubusercontent.com/7252598/178702621-584132b5-03f9-4fa5-a4b9-ca35f09c6421.png","https://user-images.githubusercontent.com/7252598/178723716-a70eeb0f-1dd2-49a6-970a-e9892241b800.png","https://user-images.githubusercontent.com/7252598/178723943-1dbae7de-0099-4e29-b54b-2783a1b6e564.png","https://user-images.githubusercontent.com/7252598/178724180-84712785-16f1-4c42-a67a-ce833fc6ba56.png"],"datePublished":"2022-07-13T03:00:00.000Z","dateModified":"2022-08-30T04:36:53.547Z","author":{"@type":"Person","name":"Joosung Yoon"},"publisher":{"@type":"Organization","name":"Luke's Blog","logo":{"@type":"ImageObject","url":"https://eagle705.github.io/img/eagle705-logo.png"}},"description":"Ref https:&#x2F;&#x2F;github.com&#x2F;ofirpress&#x2F;attention_with_linear_biases huggingface 구현 https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;blob&#x2F;ad28ca291bf851b48d7f2d4becf96ca90c98f8f1&#x2F;src&#x2F;transformers&#x2F;models&#x2F;bloom&#x2F;mod"}</script><link rel="canonical" href="https://eagle705.github.io/(ALiBi)%20TRAIN%20SHORT,%20TEST%20LONG:%20ATTENTION%20WITH%20LINEAR%20BIASES%20ENABLES%20INPUT%20LENGTH%20EXTRAPOLATION/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-110980734-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-110980734-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-07-13T03:00:00.000Z" title="2022. 7. 13. 오후 12:00:00">2022-07-13</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-30T04:36:53.547Z" title="2022. 8. 30. 오후 1:36:53">2022-08-30</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">13분안에 읽기 (약 1898 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION</h1><div class="content"><h2 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ofirpress/attention_with_linear_biases">https://github.com/ofirpress/attention_with_linear_biases</a></li>
<li>huggingface 구현<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/ad28ca291bf851b48d7f2d4becf96ca90c98f8f1/src/transformers/models/bloom/modeling_bloom.py#L96">https://github.com/huggingface/transformers/blob/ad28ca291bf851b48d7f2d4becf96ca90c98f8f1/src/transformers/models/bloom/modeling_bloom.py#L96</a></li>
</ul>
</li>
</ul>
<h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자: Ofir Press1,2 Noah A. Smith1,3 Mike Lewis2<br>1Paul G. Allen School of Computer Science &amp; Engineering, University of Washington 2Facebook AI Research 3Allen Institute for AI</li>
</ul>
<h2 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h2><ul>
<li>extrapolation 잘됨</li>
<li>11% 빠름 속도, 11% 메모리 적게씀</li>
<li>동일 길이로 학습한 모델 대비 짧은 길이로 학습해도 ppl 유지됨</li>
<li>구현도 간단하다</li>
<li>position embedding 지우고 대신에 길이에 linear하게 비례해서 attention score 깎아버리자!</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>이 질문에 대한 답변이 요구되어져왔음<ul>
<li><code>how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training?</code></li>
</ul>
</li>
<li>position representation만 변경하면됨<ul>
<li><code>Attention with Linear Biases (ALiBi)</code><ul>
<li>not add positional embeddings to word embeddings; instead, it <code>biases query-key attention scores with a penalty that is proportional to their distance</code>.</li>
</ul>
</li>
</ul>
</li>
<li>1.3B모델에서 1024 -&gt; 2048로 바꿔도 2048모델을 sinusoidal position embedding으로 학습한 모델과 pl 같았고, 학습속도는 11%빠르고 11% 메모리 적게 사용함<ul>
<li>여기서 말하고자 하는건 긴 길이로 사용할꺼면 학습도 거기에 맞게 해놔야된다 라는 것 같음</li>
<li>맨 처음 질문 자체가 짧은 길이에 대해서 학습한 후 긴길이에 대해서 사용해도 되냐는거 였으니</li>
</ul>
</li>
<li>ALiBis’ inductive bias는 token의 recency(거리)와 관련있는데, 이게 WikiText-103 benchmark에 있던 multiple strong position methods보다 성능이 좋았음</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>More context, achieved by larger L, improves predictions at inference time. But longer sequences are more expensive to train on</li>
<li>We define <strong>extrapolation</strong> as a model’s ability to continue performing well as the number of input tokens during validation increases beyond the number of tokens on which the the model was trained</li>
<li>transformer language models (LMs) that use sinusoidal position embeddings have very weak extrapolation abilities; see Figure 1.<br><img src="https://user-images.githubusercontent.com/7252598/178661915-adcc3a20-c0c3-4199-be75-9b68b005d9cb.png" alt="image"></li>
<li>최근에 나온 포지셔널 인코딩은 성능 좋긴함.. 하지만!! 느리고 추가 메모리 필요함<ul>
<li>However, the better of these, the T5 bias, is considerably slower than the sinusoidal approach and uses extra memory and parameters  (Figure 2).</li>
<li><img src="https://user-images.githubusercontent.com/7252598/178676014-ac42f75b-245d-4c12-9ef8-2b262d1d0e29.png" alt="image"></li>
</ul>
</li>
<li>introduce Attention with Linear Biases (ALiBi) to facilitate efficient extrapolation. <ul>
<li>한마디로 길이에 따라서 QK로 매칭되는 attention score를 좀 깍아버리고 position embedding은 삭제시켜버리겠다는 것!</li>
<li>ALiBi <strong>negatively biases attention scores</strong> with a linearly decreasing penalty proportional to the distance between the relevant key and query. <strong>Our simple approach eliminates position embeddings</strong></li>
</ul>
</li>
<li>ALiBi can be imple- mented by changing only a few lines of existing transformer code.</li>
</ul>
<h2 id="Background-and-Experimental-Setup"><a href="#Background-and-Experimental-Setup" class="headerlink" title="Background and Experimental Setup"></a>Background and Experimental Setup</h2><ul>
<li>During both training and perplexity evaluation (i.e., scoring a fixed sequence), many predictions can<br>be calculated at once; this is done using a “causal mask” that ensures each position’s prediction is<br>influenced only by tokens to its left</li>
<li>Let L be the length of each input subsequence during training;<br>it includes L predictions, which on average have access to (L+1)&#x2F;2 tokens of (left) context</li>
</ul>
<h3 id="sinusoidal"><a href="#sinusoidal" class="headerlink" title="sinusoidal"></a>sinusoidal</h3><ul>
<li>transformer에서 사용됨</li>
<li>constant, non-learned vectors that are added to token embeddings on input to the first layer of the transformer.</li>
</ul>
<h3 id="Rotary"><a href="#Rotary" class="headerlink" title="Rotary"></a>Rotary</h3><ul>
<li><code>Roformer</code>: Enhanced transformer with rotary position embedding, 2021.에서 첫 등장함</li>
<li>has recently been popularized by the open source GPT-3 (Brown et al., 2020) implementation GPT- J (Wang &amp; Komatsuzaki, 2021)에서 자주 사용됨</li>
<li>Instead of adding sinusoidal embeddings at the bottom of the transformer, they multiply the keys and queries of every attention layer by sinusoidal embeddings.<ul>
<li><code>every attn layer의 key랑 query쪽에 sin embedding을 다 곱한건가</code></li>
<li>Unlike the sinusoidal or learned positional embedding approach, the rotary method injects position information into the model at every layer, not just at the initial one</li>
<li>In addition, it adds no position information to the values of the self-attention sublayer. The output of a self-attention sublayer is a linearly transformed, weighted sum of the input value vectors; therefore, by not inserting position information into the values, the outputs of each transformer-layer contain no explicit position information. We suspect that this segregation of position information may be beneficial for extrapolation, and we draw inspiration from it in the design of our method<ul>
<li>포지션 임베딩을 직접 더해주지 않는 방식이 오히려 extrapolation에 더 좋을거라 판단했다는게 그 이유는 뭔지 안알려줌</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="T5-bias"><a href="#T5-bias" class="headerlink" title="T5 bias"></a>T5 bias</h2><ul>
<li>the T5 model of Raffel et al. (2020) uses a <code>relative position method</code> (Shaw et al., 2018; Huang et al., 2019) that <code>adds no position information to word embeddings</code> (as in the previous method)</li>
<li>Instead, it modifies the way attention values are computed. We refer to this as the “T5 bias” method.</li>
<li>In this method, we compute the attention values as before, but then we add a learned, shared bias to each query-key score that is dependent on just the distance between the query and key.<ul>
<li>QK score에 +learnable bias를 더해주되 그 bias는 query key distance에 의존성이있도록한다라.. ALiBi처럼 어떻게 보면 attention Score에 직접 개입하는것 같은데..</li>
<li>그래서 성능이 Rotary보단 좋았나보다</li>
<li>그래서 추가적인 메모리가 필요하고 느리다고 했나보다 (모든 레이어에 대해서 bias가 필요해서)</li>
</ul>
</li>
<li>As in the rotary method, the T5 bias injects position information into the model at every layer and integrates no explicit position information into the self-attention value vectors.</li>
</ul>
<h2 id="ATTENTION-WITH-LINEAR-BIASES-ALIBI"><a href="#ATTENTION-WITH-LINEAR-BIASES-ALIBI" class="headerlink" title="ATTENTION WITH LINEAR BIASES (ALIBI)"></a>ATTENTION WITH LINEAR BIASES (ALIBI)</h2><p><img src="https://user-images.githubusercontent.com/7252598/178683688-0c6fdf7d-ea04-4f42-a26e-94213625793a.png" alt="image"></p>
<ul>
<li>그림을 보면 learnable한게 없다 (m도 안배움..) 그래서 변수가 없으니 속도도 빠르고, 메모리도 절약했네</li>
<li>궁금한건 T5 relative position method는 learnable한걸 쓰는데 겨우 저걸로 어떻게 이긴거지 싶은거야</li>
<li>we <strong>do not add position embeddings at any point in the network. The only modification we apply is after the query-key dot product</strong>, where we add a static, non-learned bias:</li>
<li><img src="https://user-images.githubusercontent.com/7252598/178702621-584132b5-03f9-4fa5-a4b9-ca35f09c6421.png" alt="image"><ul>
<li>slope 개념이 잘 이해가 안가네, m에 대한 값을 head따라 다르게 줘보겠다가 핵심인거 같긴한데, 좋은 head에 나쁜 m값이 할당 될 수 있는 리스크를 안고 가는 느낌 (애초에 좋은 head라는걸 우린 알수도없지만)</li>
<li>The ALiBi bias is not multiplied by the √dk scaling factor from Equation 1 of Vaswani et al. (2017).</li>
</ul>
</li>
<li>ALiBi has an inductive bias towards recency; it penalizes attention scores between distant query-key pairs, with the penalty increasing as the distance between a key and a query grows. The different heads increase their penalties at different rates, depending on the slope magnitude.</li>
<li>We initially experimented with making the slopes trainable, but this did not yield strong extrapolation results<ul>
<li>처음에 slope(m)을 trainable하게 해봤지만 좋은 결과를 얻진 못했다고함</li>
<li>왜일까..?</li>
<li>trainable하게 하면 속도도 3%정도 느려짐</li>
</ul>
</li>
<li>Our main insight from this exploration is that the slope sets that work best are those with slopes in the (0, 1) range, with the slopes’ density increasing as we get closer to 0</li>
<li>We also found our method to be robust to slope choice. Even randomly sampling from the exponential distribution worked well in some cases (although that method had high variance).<ul>
<li>이렇게 지수적으로 하도록 (exp 분포에서 랜덤하게 뽑아봐도) m을 정하면 꽤 robust한 결과가 나온다고..</li>
</ul>
</li>
<li>Since ALiBi is a relative position method, we add position information at every layer to the keys and queries but not to the values, as is done in the T5 bias and rotary methods. We hypothesize that these properties might be beneficial for extrapolation.</li>
</ul>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><ul>
<li>implement it by modifying the mask matrix by adding the linear biases to it (in practice, when training a transformer LM, query qi attends only to keys 1 to i; this is implemented by adding a mask matrix to the query-key dot product before the softmax operation is applied ). This means that there is no runtime penalty when using our method since we add no operations to the network<ul>
<li>왜 런타임에는 operation이 없을까.. 학습할때만쓴다? 뭐지..코드봐야되나</li>
</ul>
</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img src="https://user-images.githubusercontent.com/7252598/178723716-a70eeb0f-1dd2-49a6-970a-e9892241b800.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/178723943-1dbae7de-0099-4e29-b54b-2783a1b6e564.png" alt="image"></p>
<ul>
<li>이 그래프는 왜 놨는지 잘 모르겠음, ALiBi의 길이가 더 길때 ppl이 낮다를 보여줘야될것같은데 뭐지..</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/178724180-84712785-16f1-4c42-a67a-ce833fc6ba56.png" alt="image"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li>showed that the sinusoidal position embedding approach does not enable transformers to extrapolate to inputs longer than the ones they were trained on</li>
<li>established that extrapolation in transformers can be enabled by just changing the position method</li>
<li>showed that our ALiBi method offers an extremely simple replacement for existing position approaches and allow models to extrapolate</li>
<li>ALiBi is simple to implement and does not slow down runtime or require extra parameters</li>
<li>sped up the training of a 1.3 billion parameter model evaluated on the same input sequence length as GPT-3 (2048)</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION</p><p><a href="https://eagle705.github.io/(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION/">https://eagle705.github.io/(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Joosung Yoon</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-07-13</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-08-30</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/nlp/">nlp</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle%20(FIM)/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Efficient Training of Language Models to Fill in the Middle (FIM)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining/"><span class="level-item">COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://eagle705.github.io/(ALiBi)%20TRAIN%20SHORT,%20TEST%20LONG:%20ATTENTION%20WITH%20LINEAR%20BIASES%20ENABLES%20INPUT%20LENGTH%20EXTRAPOLATION/';
            this.page.identifier = '(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eagle705-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/eagle705-logo.png" alt="Joosung Yoon"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joosung Yoon</p><p class="is-size-6 is-block">Machine Learning Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>There and Back Again</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">39</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/eagle705" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/eagle705"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hisdevelopers"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JSYoon53859120"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/joosung-yoon/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/cslog/"><span class="level-start"><span class="level-item">cslog</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">27</span></span></a></li><li><a class="level is-mobile" href="/categories/photo/"><span class="level-start"><span class="level-item">photo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">8월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">5월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">12월 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">12월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">11월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">10월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">9월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">8월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">7월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">5월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">4월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">11월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">6월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">5월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">4월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/drone/"><span class="tag">drone</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/grpc/"><span class="tag">grpc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">28</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp-kb/"><span class="tag">nlp, kb</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"><span class="tag">생각정리</span><span class="tag">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Ref"><span class="level-left"><span class="level-item">1</span><span class="level-item">Ref</span></span></a></li><li><a class="level is-mobile" href="#Author"><span class="level-left"><span class="level-item">2</span><span class="level-item">Author</span></span></a></li><li><a class="level is-mobile" href="#요약"><span class="level-left"><span class="level-item">3</span><span class="level-item">요약</span></span></a></li><li><a class="level is-mobile" href="#Abstract"><span class="level-left"><span class="level-item">4</span><span class="level-item">Abstract</span></span></a></li><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">5</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Background-and-Experimental-Setup"><span class="level-left"><span class="level-item">6</span><span class="level-item">Background and Experimental Setup</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#sinusoidal"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">sinusoidal</span></span></a></li><li><a class="level is-mobile" href="#Rotary"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">Rotary</span></span></a></li></ul></li><li><a class="level is-mobile" href="#T5-bias"><span class="level-left"><span class="level-item">7</span><span class="level-item">T5 bias</span></span></a></li><li><a class="level is-mobile" href="#ATTENTION-WITH-LINEAR-BIASES-ALIBI"><span class="level-left"><span class="level-item">8</span><span class="level-item">ATTENTION WITH LINEAR BIASES (ALIBI)</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Implementation"><span class="level-left"><span class="level-item">8.1</span><span class="level-item">Implementation</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Results"><span class="level-left"><span class="level-item">9</span><span class="level-item">Results</span></span></a></li><li><a class="level is-mobile" href="#Conclusion"><span class="level-left"><span class="level-item">10</span><span class="level-item">Conclusion</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-08-30T03:00:00.000Z">2022-08-30</time></p><p class="title"><a href="/LLM%EC%9D%84%20%EC%9C%84%ED%95%9C%20%EB%84%93%EA%B3%A0%20%EC%96%95%EC%9D%80%20%EC%A7%80%EC%8B%9D%EB%93%A4%20%EC%A7%80%EC%8B%9D%EB%93%A4/">LLM(Large-ScaleLanguageModel)을 위한 넓고 얕은 지식들</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-08-01T03:00:00.000Z">2022-08-01</time></p><p class="title"><a href="/Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle%20(FIM)/">Efficient Training of Language Models to Fill in the Middle (FIM)</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-13T03:00:00.000Z">2022-07-13</time></p><p class="title"><a href="/(ALiBi)%20TRAIN%20SHORT,%20TEST%20LONG:%20ATTENTION%20WITH%20LINEAR%20BIASES%20ENABLES%20INPUT%20LENGTH%20EXTRAPOLATION/">(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-20T03:00:00.000Z">2022-06-20</time></p><p class="title"><a href="COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining/">COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-23T03:00:00.000Z">2022-05-23</time></p><p class="title"><a href="Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism/">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Joosung Yoon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>