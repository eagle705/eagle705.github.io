<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations - Luke&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Luke&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Luke&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Author 저자: Zhenzhong Lan, Sebastian Goodman, Piyush Sharma Radu Soricut (Google Research) Mingda Chen,  Kevin Gimpel  (Toyota Technological Institute at Chicago)    Who is an Author? 원래는 CV를 위주로 하던 친구"><meta property="og:type" content="blog"><meta property="og:title" content="ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"><meta property="og:url" content="https://eagle705.github.io/2019-11-11-Albert/"><meta property="og:site_name" content="Luke&#039;s Blog"><meta property="og:description" content="Author 저자: Zhenzhong Lan, Sebastian Goodman, Piyush Sharma Radu Soricut (Google Research) Mingda Chen,  Kevin Gimpel  (Toyota Technological Institute at Chicago)    Who is an Author? 원래는 CV를 위주로 하던 친구"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-2019111214553733.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191111112603203.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191111113004913.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-2019111115453932.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191111175447714.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112115816716.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112120120393.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112120723931.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112121058671.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-2019111213351073.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112134323631.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112135024847.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112135257813.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112135823400.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112140551726.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112142949229.png"><meta property="og:image" content="https://eagle705.github.io/img/markdown-img-paste-20191112143016109.png"><meta property="article:published_time" content="2019-11-11T03:00:00.000Z"><meta property="article:modified_time" content="2022-08-27T15:52:46.434Z"><meta property="article:author" content="Joosung Yoon"><meta property="article:tag" content="nlp"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://eagle705.github.io/img/markdown-img-paste-2019111214553733.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://eagle705.github.io/2019-11-11-Albert/"},"headline":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","image":["https://eagle705.github.io/img/markdown-img-paste-2019111214553733.png","https://eagle705.github.io/img/markdown-img-paste-20191111112603203.png","https://eagle705.github.io/img/markdown-img-paste-20191111113004913.png","https://eagle705.github.io/img/markdown-img-paste-2019111115453932.png","https://eagle705.github.io/img/markdown-img-paste-20191111175447714.png","https://eagle705.github.io/img/markdown-img-paste-20191112115816716.png","https://eagle705.github.io/img/markdown-img-paste-20191112120120393.png","https://eagle705.github.io/img/markdown-img-paste-20191112120723931.png","https://eagle705.github.io/img/markdown-img-paste-20191112121058671.png","https://eagle705.github.io/img/markdown-img-paste-2019111213351073.png","https://eagle705.github.io/img/markdown-img-paste-20191112134323631.png","https://eagle705.github.io/img/markdown-img-paste-20191112135024847.png","https://eagle705.github.io/img/markdown-img-paste-20191112135257813.png","https://eagle705.github.io/img/markdown-img-paste-20191112135823400.png","https://eagle705.github.io/img/markdown-img-paste-20191112140551726.png","https://eagle705.github.io/img/markdown-img-paste-20191112142949229.png","https://eagle705.github.io/img/markdown-img-paste-20191112143016109.png"],"datePublished":"2019-11-11T03:00:00.000Z","dateModified":"2022-08-27T15:52:46.434Z","author":{"@type":"Person","name":"Joosung Yoon"},"publisher":{"@type":"Organization","name":"Luke's Blog","logo":{"@type":"ImageObject","url":"https://eagle705.github.io/img/eagle705-logo.png"}},"description":"Author 저자: Zhenzhong Lan, Sebastian Goodman, Piyush Sharma Radu Soricut (Google Research) Mingda Chen,  Kevin Gimpel  (Toyota Technological Institute at Chicago)    Who is an Author? 원래는 CV를 위주로 하던 친구"}</script><link rel="canonical" href="https://eagle705.github.io/2019-11-11-Albert/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-110980734-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-110980734-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-11-11T03:00:00.000Z" title="2019. 11. 11. 오후 12:00:00">2019-11-11</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:52:46.434Z" title="2022. 8. 28. 오전 12:52:46">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">33분안에 읽기 (약 5015 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</h1><div class="content"><h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자:<ul>
<li>Zhenzhong Lan, Sebastian Goodman, Piyush Sharma Radu Soricut (<strong>Google Research</strong>)</li>
<li>Mingda Chen,  Kevin Gimpel  (<strong>Toyota Technological Institute at Chicago</strong>)</li>
</ul>
</li>
</ul>
<h3 id="Who-is-an-Author"><a href="#Who-is-an-Author" class="headerlink" title="Who is an Author?"></a>Who is an Author?</h3><ul>
<li>원래는 CV를 위주로 하던 친구인데 이번에 NLP꺼도 해본듯 (CVPR도 들고 있고..)</li>
<li>논문 인용수도 꽤 됨</li>
<li>Google VR팀에서도 인턴했었음<br><img src="/img/markdown-img-paste-2019111214553733.png">{: height&#x3D;”50%” width&#x3D;”50%”}<br><a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~lanzhzh/">http://www.cs.cmu.edu/~lanzhzh/</a></li>
</ul>
<h4 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h4><ul>
<li>간단한 아이디어인데 실험을 엄청 많이 해놔서 paper를 만든느낌</li>
<li>실험이 의미는 있지만 직관적으로 예측가능한 결과임</li>
<li>간단한 아이디어도 사실 예전부터 적용되어야 했음 (weight sharing, decomposition)</li>
<li>transformer 논문이 처음에 pretraining용이 아니다보니 당시 그 논문에서 빼먹었지만 당연히 앞으론 적용되었어야할 아이디어가 2년이 지나서야 적용된 느낌</li>
<li>SOP가 NSP보단 Good이다</li>
<li>SOP 할때 문장 단위가 아니라 textual segments로 한거 괜찮았음 (SEP도 그러면 segment단위로 넣겠네)</li>
<li>MLM 을 n-gram masking 한건 좀 신기하네 나쁘지 않음</li>
<li>transformer에서 dropout을 없애는게 pretraining할 때 진짜 좋은지는 좀 더 검증해봐야할 듯</li>
<li>이 논문은 모델 그림이 없다(?)</li>
</ul>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul>
<li>Natural langauge representation을 pretraining을 통해 학습시키려 할때 모델 사이즈를 키우는건 성능향상을 하는데 도움을 줌</li>
<li>하지만 특정 포인트 이상으로 모델 사이즈가 커지는건 GPU&#x2F;TPU 메모리 크기의 제한, 학습시간이 길어짐, 예상치 못한 model degradation등으로 인해 문제가 될 수 있음</li>
<li>이런 문제를 해결하기 위해 적은 메모리 사용과 BERT의 학습 속도를 높이기 위한 two parameter-reduction techniques을 제안함</li>
<li>실험적인 증거들은 제안하는 방법이 orginal BERT보다 더 낫다는 걸 보여줌</li>
<li>추가로, self-supervised loss를 추가헀는데 이는 inter-sentence coherence를 modeling하는데 초점을 맞췄음</li>
<li>결과적으로 BERT-large에 비해 적은 parameter를 갖고도 GLUE, RACE, SQuAD benchamrk에서 SOTA를 기록함</li>
</ul>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h4><ul>
<li>Full network pre-training은 langauge representation learning 에서 새로운 breakthroughs를 기록해왔음 (Dai &amp; Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard &amp; Ruder, 2018)</li>
<li>많은 NLP tasks들이 학습 데이터가 제한되어있는데 이런 pre-trained model은 매우 큰 benefit을 주었음</li>
<li>그중 하나는 중국 중고등학교 영어 시험에 대한 reading comprehension task (RACE test)임 (<code>reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017)</code>) (<del>저자가 중국인이라 그런가..이걸 첫번째 예시로 드네</del>)<ul>
<li>논문에서 발표할땐 SOTA 44.1% acc였음</li>
<li>최근 논문에서는 SOTA 83.2% acc</li>
<li>본 논문에서 제안하는 모델은 89.4% acc로 SOTA가 되었고 pretrained language representations은 큰 도움이 되었음</li>
</ul>
</li>
<li>large network이 SOTA를 위해선 매우 중요했지만 이젠 real applications을 위해서 경량화하려는게 흔한 practice가 됨 (<code>It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications</code>)</li>
<li>모델 사이즈가 중요해졌기에, 이런 research question을 던짐 <code>Is having better NLP models as easy as having larger models?</code></li>
<li>이런 질문에 대한 한 가지 장애물은 하드웨어의 메모리 제한임<ul>
<li>SOTA 모델은 보통 millions 또는 billions of params을 가짐</li>
<li>분산학습 할때 param 개수가 많으면 거기에 비례해서 커뮤니케이션 오버헤드도 더 커지기 때문에 학습 속도도 느려짐</li>
<li>모델의 hidden size를 BERT-large 처럼 단순히 늘려버리면 성능도 안좋아지는것 또한 발견함 (Figure 1, Table 1 참고)</li>
<li>이러한 문제들은 모델 병렬화와 메모리 관리로 어느정도 해결가능하지만 communication overhead와 model degradation 문제는 해결할 수 없음</li>
</ul>
</li>
</ul>
<p><img src="/img/markdown-img-paste-20191111112603203.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<p><img src="/img/markdown-img-paste-20191111113004913.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<ul>
<li>본 논문에서는 이러한 문제 해결을 위한 A Lite BERT (ALBERT) 구조를 디자인했음 (훨씬 적은 param을 사용)</li>
<li>ALBERT는 two parameter reduction techniques을 사용함<ul>
<li>factorized embedding parameterization<ul>
<li>large vocabulary embedding matrix를 두개의 작은 matrices로 decomposing함</li>
<li><code>we separate the size of the hidden layers from the size of vocabulary embedding.</code></li>
<li>이렇게 분리하면, vocabulary embeddings의 parameter size를 크게 늘리지 않아도 hidden size를 더 쉽게 늘릴 수 있음</li>
</ul>
</li>
<li>cross-layer parameter sharing<ul>
<li>layer 간에 param을 share함으로써  network의 depth가 깊어져도 parameter가 증가하는걸 막을 수 있음</li>
</ul>
</li>
<li>위의 두가지 techniques으로 심각한 performance 손상 없이 param를 줄일 수 있었음</li>
</ul>
</li>
<li>BERT-large와 비슷한 configuration으로 ALBERT는 18배 적은 param을 갖고 학습도 1.7배 빨랐음</li>
<li>param을 줄이는 기술은 Regularization 처럼 동작해서 학습도 안정적으로 할 수 있게 해주고, Generalization에도 도움을 줬음</li>
<li>더 나아가, ALBERT의 성능을 더 높이기 위해, sentence-order prediction (SOP)를 위한 self-supervised loss를 제안함<ul>
<li>inter-sentence coherence에 포커싱을 맞춤</li>
<li>BERT에서 사용된 NSP loss가 효과가 없다는 걸 보이기 위해 사용함 (Yang et al., 2019; Liu et al., 2019)</li>
</ul>
</li>
<li>이러한 방법들로 ALBERT configuration보다 더 scale up이 가능하게 되면서도 동시에 BERT-large 보다는 fewer param이 가능하게 되었음. 하지만 성능은 더 좋음.</li>
<li>benchmark<ul>
<li>the RACE accuracy to 89.4%</li>
<li>the GLUE benchmark to 89.4</li>
<li>the F1 score of SQuAD 2.0 to 92.2</li>
</ul>
</li>
</ul>
<h4 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h4><h5 id="2-1-Scaling-Up-Representation-Learning-For-Natural-Language"><a href="#2-1-Scaling-Up-Representation-Learning-For-Natural-Language" class="headerlink" title="2.1 Scaling Up Representation Learning For Natural Language"></a>2.1 Scaling Up Representation Learning For Natural Language</h5><ul>
<li>natural langauge에서 representation을 학습하는 것이 매우 유용하다는건 다양하게 보여져 왔음 (e.g. word2vec) (Mikolov et al., 2013; Le &amp; Mikolov, 2014; Dai &amp; Le, 2015; Pe- ters et al., 2018; Devlin et al., 2019; Radford et al., 2018; 2019)</li>
<li>지난 2년간 가장 큰 변화는 pre-training word embeddings이나 contextualized embedding 패러다임에서 full-network pre-training 을 통해 task-specific fine-tuning하는 패러다임으로 넘어갔다는 것임</li>
<li>이러한 연구의 연장선에서, 모델의 크기가 크면 성능이 향상된다는 것들도 보여져왔음</li>
<li>하지만 기존 연구들은 hidden size를 1024로 셋팅한데 까지만 연구했고 본 논문에서는 2048로 늘려보았지만 성능이 더 악화되었음</li>
<li>그러므로 representation learning에서 scaling up 하는 것은 단순히 모델 사이즈를 증가시키는것 만큼 쉬운 것은 아님</li>
<li>추가로 computational constraints 때문에 실험하기도 어려움<ul>
<li>Chen et al. (2016) 연구에서는 gradient checkpointing이라는 기법으로 memory requirements를 줄임</li>
<li>Gomez et al. (2017) 연구에서는 next layer로부터 이전 layer의 activation을 reconstruction 하는 방법을 제안해서 intermeidate activations을 저장하지 않아도 되게함</li>
<li>두 방법 모두 메모리 사용은 감소시켰지만 속도가 느림</li>
<li>제안하는 방법은 속도도 빠르게하고 paramter-reduction으로 메모리 사용도 줄임</li>
</ul>
</li>
</ul>
<h5 id="2-2-Cross-Layer-Parameter-Sharing"><a href="#2-2-Cross-Layer-Parameter-Sharing" class="headerlink" title="2.2 Cross-Layer Parameter Sharing"></a>2.2 Cross-Layer Parameter Sharing</h5><ul>
<li>Transformer 구조 자체는 standard encoder-decoder task를 다뤘지 pretraining&#x2F;finetuniong setting을 염두해둔건 아님</li>
<li>기존의 연구들은 corss-layer parameter sharing (Universal Transformer, UT)이 LM과 subject-verb agreement에 도움이 됨을 보여줌 (Dehghani et al. (2018))</li>
<li>최근 Bai et al. (2019) 가 제안한 Deep Equilibrium Model (DQE) 모델에서는 input embedding과 output embedding이 특정 layer에서 equilibrium point에 도달할 수 있다는걸 보여줌 (<del>무슨 말이죠..</del>)</li>
<li>본 논문에서 발견한 것은 이와 다름 (<code>Our observations show that our embeddings are oscillating rather than converging</code>)</li>
</ul>
<h5 id="2-3-Sentence-Ordering-Objectives"><a href="#2-3-Sentence-Ordering-Objectives" class="headerlink" title="2.3 Sentence Ordering Objectives"></a>2.3 Sentence Ordering Objectives</h5><ul>
<li>ALBERT는 pretraining loss중 하나로 두개의 연속된 text 세그먼트의 순서를 맞추는걸 사용함</li>
<li>담화의 coherence와 coheision은 이전에도 많은 연구가 이뤄졌었음 (<code>Coherence and cohesion in discourse have been widely studied and many phenomena have been identified that connect neighboring text segments (Hobbs, 1979; Halliday &amp; Hasan, 1976; Grosz et al., 1995)</code>)</li>
<li>Skip-thought (Kiros et al., 2015) and FastSent (Hill et al., 2016) 처럼 sentence embeddings들은 문장 주변의 단어들을 예측하도록 sentence를 인코딩하는 방식을 사용했음</li>
<li>다른 objectives로는 단지 주변이 아닌 미래의 문장들을 예측하거나 discourse markers를 예측하는 방법으로 sentence embedding을 학습하기도 했음 (<code>Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019)</code>)</li>
<li>본 논문에서 사용하는 방법은 Jernite et al. (2017)에서 제안된 <code>sentence ordering objective</code>와 비슷함</li>
<li>대부분의 above work와는 달리 <code>본 논문에서 사용하는 loss는 textual segments 단위임. sentence가 아니라.</code></li>
<li>BERT에서 사용된 NSP와 비교해서 SOP이 더 challenging pretarining task라는걸 확인함</li>
<li>본 논문과 거의 동시에 Wang et al. (2019) 논문도 SOP를 다뤘는데 차이점은 본논문에서는 binary classificaiton인데 저기서는 three-way classification임 (<del>어떤 종류로 클래스를 나눴을까</del>)</li>
</ul>
<h4 id="3-The-Elements-of-ALBERT"><a href="#3-The-Elements-of-ALBERT" class="headerlink" title="3. The Elements of ALBERT"></a>3. The Elements of ALBERT</h4><h5 id="3-1-Model-Architecture-Choices"><a href="#3-1-Model-Architecture-Choices" class="headerlink" title="3.1 Model Architecture Choices"></a>3.1 Model Architecture Choices</h5><ul>
<li>ALBERT의 backbone은 BERT와 비슷함 (transformer encoder with GELU nonlinearities) </li>
<li>구조에 있어 3가지의 contribution은 다음과 같음</li>
</ul>
<h6 id="Factorized-embedding-parameterization"><a href="#Factorized-embedding-parameterization" class="headerlink" title="Factorized embedding parameterization"></a>Factorized embedding parameterization</h6><ul>
<li>BERT, XLNet, RoBERTa 등에서 WordPiece embedding size $ E $는 hidden layer size $ H $ 와 묶여있다. i.e., $ E \equiv H $ (왜냐하면, attention 자체가 dimension 자체를 줄이는게 아니라 weighted sum 개념이기 때문에 input dim -&gt; output dim이 되기 때문)</li>
<li>이러한 선택은 modeling 과 practical reasons에 있어서 suboptimal임<ul>
<li>modeling 관점:<ul>
<li>WordPiece embeddings은 context-independent representations를 학습하는 것임</li>
<li>반면, hidden-layer embeddings은 context-dependent representation을 학습하는 것임</li>
<li>BERT-like representation의 power는 context-dependent representation에서 옴</li>
<li>WordPiece embedding size $ E $를 hidden layer size $ H $와 분리하는 것은 더 효율적인 모델 파라미터 사용으로 이어질 수 있음 ($ H \gg E $)</li>
</ul>
</li>
<li>practical 관점:<ul>
<li>NLP에서 vocab size $ V $ 는 굉장히 큼</li>
<li>만약 $ E \equiv H $ 라면 $ H $ 가 증가하면 size of embedding matrix도 증가하게됨 ($ V \times E $)</li>
<li>결과적으로 매우 쉽게 billions of params을 갖게됨</li>
<li>하지만 실제 업데이트되는 건 적음 (update sparsely)</li>
</ul>
</li>
</ul>
</li>
<li>그러므로 본 논문에서는 factorization을 사용해서 embedding params을 two smaller matrices로 분리함</li>
<li>one-hot을 바로 hidden space로 보내는게 아니라 embedding space로 보낸 후에 hidden space로 projection함 (<del>사실 너무나 당연한거 아닌가..확실히 transformer의 naive한 초기셋팅이 안고쳐지다가 여기에 와서야 고쳐지는군</del>)</li>
</ul>
<p>$$<br>O(V \times H) \text { to } O(V \times E+E \times H)<br>$$</p>
<ul>
<li>이러한 parameter reduction은 $ H \gg E $ 일때 매우 중요함</li>
</ul>
<h6 id="Cross-layer-parameter-sharing"><a href="#Cross-layer-parameter-sharing" class="headerlink" title="Cross-layer parameter sharing"></a>Cross-layer parameter sharing</h6><ul>
<li>다른 여러 방법들이 있었지만 ALBERT에서는 layer간 모든 parameter를 공유함</li>
<li>본 논문에서의 관찰과는 다른 결과를 보였던 선행연구들<ul>
<li>Universal Transformer (UT)와 Deep Equilibrium Models (DQE)에서도 비슷한 전략이 있었지만 UT 논문에서는 UT가 vanilla Transformer보다 높은 성능을 보여줌 (본논문에서 실험한 결과와는 다름)</li>
<li>DQE도 equilibrium point가 있다고 했지만 본 논문의 실험에서는 embeddings이 수렴하기보단 oscillating 하게 보였음</li>
</ul>
</li>
<li>layer를 올라갈 수록 ALBERT가 BERT보다 더 smooth하게 값이 바뀌는걸 볼 수 있는데 이건 weighty-sharing이 network parameters의 stabilizing에 효과가 있음을 보여줌</li>
<li>DQE와는 다르게 절대 0으로 수렴하는 현상이 보여지지 않는걸로 봐서 ALBERT의 solution space가 DQE와 매우 다르다는걸 알 수 있음</li>
</ul>
<p><img src="/img/markdown-img-paste-2019111115453932.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h6 id="Inter-sentence-coherence-loss"><a href="#Inter-sentence-coherence-loss" class="headerlink" title="Inter-sentence coherence loss"></a>Inter-sentence coherence loss</h6><ul>
<li>MLM loss에 추가해서 BERT에서는 NSP loss를 사용했음</li>
<li>하지만 그간의 연구들 (Yang et al., 2019; Liu et al., 2019)은 NSP가 unreliable하다 판단했고, 지워버렸음 (실제로 downstream task performance 결과도 그랬음)</li>
<li>본 논문에서는 NSP가 효과가 없는 것이 MLM에 비해 어렵지 않은 태스크기 때문이라고 추측함</li>
<li>NSP는 topic prediction이나 coherence prediction task의 결합으로 볼 수 있음 (topic으로 볼수 있는 또 다른 이유는 문장을 랜덤으로 뽑을때 다른 문서에서 뽑음)</li>
<li>하지만 topic prediction은 coherence prediction에 비해 비교적으로 학습하기 쉽고 MLM loss로 학습해서 배우는 것과 비교적으로 더 오버랩됨</li>
<li>langauge understanding 관점에서 inter-sentence modeling은 중요하기 때문에 을 유지시키기로 함</li>
<li>대신 coherence에 기반을 둔 loss를 제안하기로 했고 sentence-order prediction (SOP) loss 를 사용하기로 함</li>
<li>topic prediction을 피하고, 대신 inter-sentence coherence에 집중함</li>
<li>positive examples은 BERT와 같이 같은 문서에서 연속된 2개의 segments를 사용</li>
<li>negative examples은 동일한 two consecutive segments를 순서만 바꿔서 사용함 (<del>동일한걸 쓰면 의미가 있나.. 약간 난이도를 낮춘건가..segment embedding에 대한 값이 매우 중요하게 학습되는 결과만 낳을거 같은데</del>)</li>
<li>이러한 task는 모델이 discourse-level coherence properties에 대해 finer-grained distinctions을 학습하게 함</li>
<li><code>NSP는 SOP를 거의 하나도 못풀지만, SOP는 NSP를 reasonable한 수준에서 해결 할 수 있는 것으로 나타남</code></li>
<li>결과적으로 ALBERT는 multi-sentence encoding task에 대한 downstream task performance를 개선할 수 있었음</li>
</ul>
<h5 id="3-2-Model-Setup"><a href="#3-2-Model-Setup" class="headerlink" title="3.2 Model Setup"></a>3.2 Model Setup</h5><ul>
<li>BERT와 ALBERT의 모델을 hyperparameter setting으로 비교해봄</li>
</ul>
<p><img src="/img/markdown-img-paste-20191111175447714.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<ul>
<li>ALBERT-large는 BERT-large에 비해 18배 적은 params을 가짐 (18M vs 334M)</li>
<li>Hidden size를 2048까지 하면 BERT-xlarge는 1.27 billion params을 가짐</li>
<li>반면에 ALBERT는 60M밖에 안함</li>
<li>ALBERT-xxlarge 모델의 경우 12-layer network에 대한 결과를 기록함 24-layer network은 결과는 비슷한데 computationally more expensive해서 기록 안함</li>
</ul>
<h4 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4. Experimental Results"></a>4. Experimental Results</h4><h5 id="4-1-Experimental-Setup"><a href="#4-1-Experimental-Setup" class="headerlink" title="4.1 Experimental Setup"></a>4.1 Experimental Setup</h5><ul>
<li>BERT와 비교하기 위해 비슷하게 셋팅함</li>
<li>Data: (for pretraining, 16GB) <ul>
<li>BookCORPUS</li>
<li>English Wikipedia</li>
</ul>
</li>
<li>Format:<ul>
<li>$ [\mathrm{CLS}] x_{1}[\mathrm{SEP}] x_{2}[\mathrm{SEP}], \text { where } x_{1}&#x3D;x_{1,1}, x_{1,2} \cdots \text { and } x_{2}&#x3D;x_{1,1}, x_{1,2} \cdots $</li>
<li>length: 512</li>
</ul>
</li>
<li>Noise:<ul>
<li>randomly generate input (10%)</li>
</ul>
</li>
<li>Vocab:<ul>
<li>30000 (tokenized using SentencePiece)</li>
</ul>
</li>
<li>MLM:<ul>
<li>n-gram masking (Joshi et al., 2019) 사용</li>
<li>each n-gram mask의 length는 랜덤하게 선택되고, 길이 n에 대한 확률은 다음과 같음</li>
<li>$ p(n)&#x3D;\frac{1 &#x2F; n}{\sum_{k&#x3D;1}^{N} 1 &#x2F; k} $ (<del>길이가 길면 선택될 확률이 낮다?!</del>)</li>
<li>maximum length of n-gram: 3 (이렇게하면 “White House correspondents” 과 같은 단어도 잡을 수 있음)</li>
</ul>
</li>
<li>Training:<ul>
<li>Batch size: 4096</li>
<li>Optim: LAMB </li>
<li>Lr: 0.00176 (You et al., 2019)</li>
<li>Steps: 125,000</li>
<li>Machine: Cloud TPU V3 (# of TPUs used for training ranged from 64 to 1024)</li>
</ul>
</li>
</ul>
<h5 id="4-2-Evaluation-Benchmarks"><a href="#4-2-Evaluation-Benchmarks" class="headerlink" title="4.2 Evaluation Benchmarks"></a>4.2 Evaluation Benchmarks</h5><h6 id="4-2-1-Intrinsic-Evaluation"><a href="#4-2-1-Intrinsic-Evaluation" class="headerlink" title="4.2.1 Intrinsic Evaluation"></a>4.2.1 Intrinsic Evaluation</h6><ul>
<li>학습 과정을 모니터링 하기 위해 SQuAD와 RACE로부터 dev set 만들어서 테스트함 (<del>Deview때 네이버가 보여준거랑 똑같네</del>)</li>
<li>MLM과 sentence classification task의 accuracy를 모두 확인함</li>
<li>모델이 잘 학습되서 수렴하는지만 확인하려고함</li>
</ul>
<h6 id="4-2-2-Downstream-Evaluation"><a href="#4-2-2-Downstream-Evaluation" class="headerlink" title="4.2.2 Downstream Evaluation"></a>4.2.2 Downstream Evaluation</h6><ul>
<li>GLUE (the General Language Understanding Evaluation)</li>
<li>SQuAD (the Standford Question Answering Dataset)</li>
<li>RACE (the ReAding Comprehension from Examinations dataset)</li>
<li>dev set에 대해서 early stopping 해서 학습시킴</li>
</ul>
<h5 id="4-3-Overall-Comparison-Between-BERT-and-ALBERT"><a href="#4-3-Overall-Comparison-Between-BERT-and-ALBERT" class="headerlink" title="4.3 Overall Comparison Between BERT and ALBERT"></a>4.3 Overall Comparison Between BERT and ALBERT</h5><ul>
<li>with only around 70% of BERT-large’s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large</li>
<li>measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%)</li>
<li>We also observe that BERT-xlarge gets significantly worse results than BERT-base on all metrics. This indicates that a model like BERT-xlarge is more difficult to train than those that have smaller parameter sizes.</li>
<li>Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models.</li>
</ul>
<p><img src="/img/markdown-img-paste-20191112115816716.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h5 id="4-4-Factorized-Embedding-Parameterization"><a href="#4-4-Factorized-Embedding-Parameterization" class="headerlink" title="4.4 Factorized Embedding Parameterization"></a>4.4 Factorized Embedding Parameterization</h5><ul>
<li>the effect of changing the vocabulary embedding size E using an ALBERT-base configuration setting</li>
<li>Under the non-shared condition (BERT-style):<ul>
<li>larger embedding sizes give better performance, but not by much.</li>
</ul>
</li>
<li>Under the all-shared condition (ALBERT-style):<ul>
<li>an embedding of size 128 appears to be the best</li>
</ul>
</li>
</ul>
<p><img src="/img/markdown-img-paste-20191112120120393.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h5 id="4-5-Cross-Layer-Parameter-Sharing"><a href="#4-5-Cross-Layer-Parameter-Sharing" class="headerlink" title="4.5 Cross-Layer Parameter Sharing"></a>4.5 Cross-Layer Parameter Sharing</h5><ul>
<li>experiments for various cross-layer parameter-sharing strategies, using an ALBERT-base configuration (Table 2) with two embedding sizes (E &#x3D; 768 and E &#x3D; 128)</li>
<li>compare <ul>
<li>all-shared strategy (ALBERT-style)<ul>
<li>hurts performance under both conditions</li>
<li>but it is less severe for E &#x3D; 128 (- 1.5 on Avg) compared to E &#x3D; 768 (-2.5 on Avg)</li>
</ul>
</li>
<li>the not-shared strategy (BERT-style)</li>
<li>intermediate strategies in which <ul>
<li>only the attention parameters are shared (but not the FNN ones) <ul>
<li>sharing the attention parameters results in no drop when E &#x3D; 128 (+0.1 on Avg), and a slight drop when E &#x3D; 768 (-0.7 on Avg)</li>
</ul>
</li>
<li>only the FFN parameters are shared (but not the attention ones)<ul>
<li>most of the performance drop appears to come from sharing the FFN-layer parameters</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>other strategy<ul>
<li>divide the L layers into N groups of size M , and each size-M group shares parameters<ul>
<li>the smaller the group size M is, the better the performance we get. However, decreasing group size M also dramatically increase the number of overall parameters. We choose all-shared strategy as our default choice</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/img/markdown-img-paste-20191112120723931.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h5 id="4-6-Sentence-Order-Prediction-SOP"><a href="#4-6-Sentence-Order-Prediction-SOP" class="headerlink" title="4.6 Sentence Order Prediction (SOP)"></a>4.6 Sentence Order Prediction (SOP)</h5><ul>
<li>compare head-to-head three experimental conditions for the additional inter-sentence loss<ul>
<li>none (XLNet- and RoBERTa-style)</li>
<li>NSP (BERT-style)</li>
<li>SOP (ALBERT-style) (base config)</li>
</ul>
</li>
<li>the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the “None” condition)<ul>
<li>NSP ends up modeling only topic shift<br><img src="/img/markdown-img-paste-20191112121058671.png">{: height&#x3D;”50%” width&#x3D;”50%”}</li>
</ul>
</li>
</ul>
<h5 id="4-7-Effect-of-Network-Depth-and-Width"><a href="#4-7-Effect-of-Network-Depth-and-Width" class="headerlink" title="4.7 Effect of Network Depth and Width"></a>4.7 Effect of Network Depth and Width</h5><ul>
<li>check how depth (number of layers) and width (hidden size) affect the performance of ALBERT</li>
<li>the performance of an ALBERT-large configuration<ul>
<li>Layer 개수로 실험: 12 layer 이상부터는 큰 성능차이 없음<br><img src="/img/markdown-img-paste-2019111213351073.png">{: height&#x3D;”50%” width&#x3D;”50%”}</li>
<li>Hidden size로 실험: 4096 정도가 괜찮았음 (<del>왜 3-layer로 한거지..</del>)<br><img src="/img/markdown-img-paste-20191112134323631.png">{: height&#x3D;”50%” width&#x3D;”50%”}</li>
</ul>
</li>
</ul>
<h5 id="4-8-What-if-we-train-for-the-same-amount-of-time"><a href="#4-8-What-if-we-train-for-the-same-amount-of-time" class="headerlink" title="4.8 What if we train for the same amount of time?"></a>4.8 What if we train for the same amount of time?</h5><ul>
<li>Table 3에서는 BERT-large 가 ALBERT-xxlarge 보다 3.17배 빠름</li>
<li>보통 학습을 오래할수록 성능도 좋아짐</li>
<li>데이터의 epoch을 맞춰서 실험하기보다 절대 시간을 맞춰서 실험해보기로 함 (comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours))</li>
<li>BERT로 34시간동안 400k step돌린 것과 ALBERT로 32시간동안 125k 돌린것이 얼추 시간이 비슷하니 성능을 비교해보기로함 (the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training))</li>
<li>결과는 ALBERT-xxlarge가 BERT-large보다 확실히 좋음 (ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%)</li>
</ul>
<p><img src="/img/markdown-img-paste-20191112135024847.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h5 id="4-9-Do-very-wide-ALBERT-models-need-to-be-deep-er-too"><a href="#4-9-Do-very-wide-ALBERT-models-need-to-be-deep-er-too" class="headerlink" title="4.9 Do very wide ALBERT models need to be deep(er) too?"></a>4.9 Do very wide ALBERT models need to be deep(er) too?</h5><ul>
<li>Hidden size가 크면 네트워크도 더 깊게 쌓아야되는지 실험함</li>
<li>H&#x3D;1024인 경우에 12-layer와 24-layer차이가 크지 않았음 (sec 4.7)</li>
<li>H&#x3D;4096인 경우는 또 다를수 있으니 테스트해봤는데 결과는 큰 차이가 없게 나옴</li>
</ul>
<p><img src="/img/markdown-img-paste-20191112135257813.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h5 id="4-10-Additional-training-data-and-dropout-effects"><a href="#4-10-Additional-training-data-and-dropout-effects" class="headerlink" title="4.10 Additional training data and dropout effects"></a>4.10 Additional training data and dropout effects</h5><ul>
<li>pretraining data는 Wikipedia와 BookCORPUS만 썼는데, XLNet이나 RoBERTa에서 썼던것 처럼 추가 데이터를 쓰면 어떤 효과가 있는지 알아보고자함</li>
<li>additional data가 있을때 <ul>
<li>확실히 MLM이 좋아짐</li>
<li>downstream task도 좋아졌지만 SQuAD는 나빠짐</li>
<li>SQuAD는 Wikipedia-based라서 out-of-domain training material의 영향을 받았을거라 추측</li>
</ul>
</li>
<li>1M steps을 학습시켜도 모델이 training data에 overfit이 안되서 dropout을 없애버림</li>
<li>dropout 없애니 MLM이 더 잘됨</li>
<li>As a result, we decide to remove dropout to further increase our model capacity. (<del>왜 드랍아웃 없앤게 model capacity를 올리는거지.. 약간 다른개념 아닌가.. 학습이 더 잘되서 저렇게 말한건가 -&gt; 학습 잘되면 오버핏도 잘되고 &#x3D;&#x3D; model capa 높으면 오버핏도 잘되고 &#x3D;&#x3D; 드랍아웃 없으면 오버핏도 잘되고</del>)</li>
</ul>
<p><img src="/img/markdown-img-paste-20191112135823400.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<ul>
<li>dropout을 빼니 downstream 결과도 더 좋아짐<ul>
<li>실험적으로 (Szegedy et al., 2017) 그리고 이론적으로 (Li et al., 2019) batchnorm과 dropout을 CNN에 쓰면 성능이 더 떨어진다는 결과도 있음 (combination of batch normalization and dropout in Convolutional Neural Networks may have harmful results)</li>
<li>예측하기로는 transformer 구조에서 dropout이 performance를 떨어뜨리는거 같은데 다른 transformer based model에서도 검증이 필요할 것 같다고함<br><img src="/img/markdown-img-paste-20191112140551726.png">{: height&#x3D;”50%” width&#x3D;”50%”}</li>
</ul>
</li>
</ul>
<h5 id="4-11-Current-state-of-te-art-on-NLU-tasks"><a href="#4-11-Current-state-of-te-art-on-NLU-tasks" class="headerlink" title="4.11 Current state-of-te-art on NLU tasks"></a>4.11 Current state-of-te-art on NLU tasks</h5><ul>
<li>dev set 성능 기준으로 선택함</li>
<li>The single-model ALBERT:<ul>
<li>the best-performing settings discussed: an ALBERT-xxlarge configuration (Table 2) <strong>using combined MLM and SOP losses, and no dropout</strong></li>
</ul>
</li>
<li>the final ensemble model ALBERT:<ul>
<li>the number of checkpoints considered for this selection range <strong>from 6 to 17</strong>, depending on the task</li>
</ul>
</li>
<li>single model과 ensemble 모두 SOTA 기록<ul>
<li>GLUE score of 89.4</li>
<li>SQuAD 2.0 test F1 score of 92.2</li>
<li>RACE test accuracy of 89.4<ul>
<li>RACE test의 경우 크게 좋아짐 (jump of +17.4% absolute points over BERT (Devlin et al., 2019), +7.6% over XLNet (Yang et al., 2019), +6.2% over RoBERTa (Liu et al., 2019), and 5.3% over DCMI+ (Zhang et al., 2019))</li>
</ul>
</li>
</ul>
</li>
<li>single model의 기존 SOTA의 ensemble보다 좋다고 어필<br><img src="/img/markdown-img-paste-20191112142949229.png">{: height&#x3D;”50%” width&#x3D;”50%”}</li>
</ul>
<p><img src="/img/markdown-img-paste-20191112143016109.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h4 id="5-Discussion"><a href="#5-Discussion" class="headerlink" title="5. Discussion"></a>5. Discussion</h4><ul>
<li>ALBERT-xxlarge 모델이 BERT-large보다 params이 적고 성능도 더 좋지만 computationally more expensive함</li>
<li>앞으로는 spare attetnion (Child et al., 2019)나 block attention (Shen et al., 2018)등으로 ALBERT의 infernece speed를 높이는게 중요해보임</li>
<li>representation을 위해 더 나은 LM 학습 방법이 필요해보임</li>
<li>langauge representation에서는 SOP가 훨씬 더 유용한걸 확인함</li>
<li>더 다양한 self-supervised training losses가 있을 것임</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</p><p><a href="https://eagle705.github.io/2019-11-11-Albert/">https://eagle705.github.io/2019-11-11-Albert/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Joosung Yoon</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2019-11-11</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-08-28</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/nlp/">nlp</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019-11-28-DistilBERT/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019-10-14-MassivelyMultilingualSentenceEmbeddigns/"><span class="level-item">Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://eagle705.github.io/2019-11-11-Albert/';
            this.page.identifier = '2019-11-11-Albert/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eagle705-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/eagle705-logo.png" alt="Joosung Yoon"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joosung Yoon</p><p class="is-size-6 is-block">Machine Learning Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>There and Back Again</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">28</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/eagle705" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/eagle705"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hisdevelopers"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JSYoon53859120"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/joosung-yoon/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/cslog/"><span class="level-start"><span class="level-item">cslog</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/photo/"><span class="level-start"><span class="level-item">photo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">12월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">11월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">10월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">9월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">8월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">7월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">5월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">4월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">11월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">6월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">5월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">4월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/drone/"><span class="tag">drone</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/grpc/"><span class="tag">grpc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">17</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp-kb/"><span class="tag">nlp, kb</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"><span class="tag">생각정리</span><span class="tag">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Author"><span class="level-left"><span class="level-item">1</span><span class="level-item">Author</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Who-is-an-Author"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Who is an Author?</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#느낀점"><span class="level-left"><span class="level-item">1.1.1</span><span class="level-item">느낀점</span></span></a></li><li><a class="level is-mobile" href="#Abstract"><span class="level-left"><span class="level-item">1.1.2</span><span class="level-item">Abstract</span></span></a></li><li><a class="level is-mobile" href="#1-Introduction"><span class="level-left"><span class="level-item">1.1.3</span><span class="level-item">1. Introduction</span></span></a></li><li><a class="level is-mobile" href="#2-Related-work"><span class="level-left"><span class="level-item">1.1.4</span><span class="level-item">2. Related work</span></span></a></li><li><a class="level is-mobile" href="#3-The-Elements-of-ALBERT"><span class="level-left"><span class="level-item">1.1.5</span><span class="level-item">3. The Elements of ALBERT</span></span></a></li><li><a class="level is-mobile" href="#4-Experimental-Results"><span class="level-left"><span class="level-item">1.1.6</span><span class="level-item">4. Experimental Results</span></span></a></li><li><a class="level is-mobile" href="#5-Discussion"><span class="level-left"><span class="level-item">1.1.7</span><span class="level-item">5. Discussion</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-10-22T03:00:00.000Z">2020-10-22</time></p><p class="title"><a href="/2020-10-22-DocumentExpansionByQueryPrediction/">Document Expansion by Query Prediction</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-06-18T03:00:00.000Z">2020-06-18</time></p><p class="title"><a href="/2020-06-18-bert_based_lexical_substitution/">BERT-based Lexical Substitution</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-05-27T03:00:00.000Z">2020-05-27</time></p><p class="title"><a href="/2020-05-27-elastic_search/">Elastic Search 정리</a></p><p class="categories"><a href="/categories/cslog/">cslog</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-05-26T03:00:00.000Z">2020-05-26</time></p><p class="title"><a href="/2020-05-26-Deeper%20Text%20Understanding%20for%20IR%20with%20Contextual%20Neural%20Language%20Modeling/">Deeper Text Understanding for IR with Contextual Neural Language Modeling&quot;</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-10T03:00:00.000Z">2020-02-10</time></p><p class="title"><a href="/2020-02-10-Practical-Linux-Unix/">Linux, Unix 정리</a></p><p class="categories"><a href="/categories/cslog/">cslog</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Joosung Yoon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>