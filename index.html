<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Luke&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Luke&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Luke&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Luke&#039;s Blog"><meta property="og:url" content="https://eagle705.github.io/"><meta property="og:site_name" content="Luke&#039;s Blog"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://eagle705.github.io/img/og_image.png"><meta property="article:author" content="Joosung Yoon"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://eagle705.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://eagle705.github.io"},"headline":"Luke's Blog","image":["https://eagle705.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Joosung Yoon"},"publisher":{"@type":"Organization","name":"Luke's Blog","logo":{"@type":"ImageObject","url":"https://eagle705.github.io/img/eagle705-logo.png"}},"description":null}</script><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-110980734-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-110980734-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-2655870716902046" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="ê²€ìƒ‰" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-03-23T08:14:37.000Z" title="3/23/2023, 5:14:37â€¯PM">2023-03-23</time>&nbsp;ê²Œì‹œ ë¨</span><span class="level-item"><time dateTime="2023-03-23T08:16:49.012Z" title="3/23/2023, 5:16:49â€¯PM">2023-03-23</time>&nbsp;ì—…ë°ì´íŠ¸ ë¨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">27ë¶„ì•ˆì— ì½ê¸° (ì•½ 4078 ë‹¨ì–´)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Alpaca/">Alpaca (A Strong Instruction-Following Model)</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca: A Strong, Replicable Instruction-Following Model</a></p>
<h2 id="Hello-Alpaca"><a href="#Hello-Alpaca" class="headerlink" title="Hello, Alpaca?"></a>Hello, Alpaca?</h2><p>ìµœê·¼ LLaMaì´ì–´ì„œ ì•„ì£¼ í•«í•œ ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ Alpacaë¼ëŠ” ëª¨ë¸ì¸ë°ìš”. ì˜¤ëŠ˜ì€ Stanfordì—ì„œ ê³µê°œí•œ ì˜¤í”ˆì†ŒìŠ¤ì¸ Alpacaì— ëŒ€í•´ì„œ ê°„ë‹¨íˆ ì†Œê°œí•´ë³´ë ¤í•©ë‹ˆë‹¤.<br>AlpacaëŠ” ì§€ë‚œë²ˆì— í¬ìŠ¤íŒ…ëœ LLaMaë¼ëŠ” ì–¸ì–´ëª¨ë¸ì„ <a target="_blank" rel="noopener" href="https://devocean.sk.com/blog/techBoardDetail.do?ID=164601">Stanford ë°•ì‚¬ê³¼ì • í•™ìƒë“¤</a>ì´ ì‚¬ìš©ìì˜ ëª…ë ¹ì–´ì— ì–¸ì–´ëª¨ë¸ì´ ì˜ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ Instruction-following ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì…ë‹ˆë‹¤.<br>ì–¸ì–´ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ í’€ê¸° ë•Œë¬¸ì— ì¼ë°˜ì ì¸ ì‚¬ìš©ìì˜ ëª…ë ¹ì–´ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ê¸°ê°€ ì–´ë ¤ìš´ë°ìš”. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ChatGPT ê°™ì€ ëª¨ë¸ì´ ë‹µë³€ì„ ì˜í•˜ëŠ” ê²ƒì€ ì‚¬ìš©ìì˜ ì˜ë„ì— ë§ê²Œ ëª¨ë¸ì„ Instruction-following ë°ì´í„°ë¡œ íŠœë‹ (Alignment) í–ˆê¸° ë•Œë¬¸ì´ë¼ê³ ë„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê²°êµ­ ì‚¬ìš©ìê°€ ì–¸ì–´ëª¨ë¸ì„ ì˜ í™œìš©í•˜ê¸° ìœ„í•´ì„œëŠ” Instruction tuningì€ ê¼­ ê±°ì³ì•¼í•˜ëŠ” ê´€ë¬¸ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br>LLaMaë¥¼ íŠœë‹í•œ ëª¨ë¸ì´ë‹ˆ ì•„ë§ˆ ë¼ë§ˆì™€ ë¹„ìŠ·í•œ ìƒê¹€ìƒˆ ê°€ì§„ ì•ŒíŒŒì¹´ë¼ê³  ì´ë¦„ì„ ì§€ì€ê²Œ ì•„ë‹Œê°€ ì‹¶ë„¤ìš”ğŸ¤”</p>
<p><img src="https://devocean.sk.com/editorImg/2023/3/23/e61a09ac00cbff86421a28127c228355177ca171f85f2757aee6d21df3c5fdbd" alt="image.png"></p>
<p>AlpacaëŠ” ë…¼ë¬¸ì´ ë”°ë¡œ ë°œí‘œë˜ì§„ ì•Šì•˜ì§€ë§Œ, ì–´ë–¤ ë°ì´í„°ë¡œ ì–´ë–»ê²Œ í•™ìŠµì„ í–ˆëŠ”ì§€ ì½”ë“œì™€ í•¨ê»˜ ê³µê°œê°€ ë˜ì–´ìˆì–´ì„œ í˜„ì¬ì‹œì ì—ì„œë„ LLaMaì™€ ê°™ì´ ë§ì€ ë³€í˜• ë° ì–´í”Œë¦¬ì¼€ì´ì…˜ì´ ë‚˜ì˜¤ê³  ìˆëŠ”ë°ìš”. ì§€ê¸ˆë¶€í„° í•œë²ˆ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.</p>
<h2 id="Alpacaë¥¼-ì™œ-ë§Œë“¤ì—ˆì„ê¹Œ"><a href="#Alpacaë¥¼-ì™œ-ë§Œë“¤ì—ˆì„ê¹Œ" class="headerlink" title="Alpacaë¥¼ ì™œ ë§Œë“¤ì—ˆì„ê¹Œ?"></a>Alpacaë¥¼ ì™œ ë§Œë“¤ì—ˆì„ê¹Œ?</h2><p>Stanford í•™ìƒë“¤ì€ ChatGPT, Claude, Bing Chatë“± ë‹¤ì–‘í•œ ëª¨ë¸ì´ ì´ë¯¸ í›Œë¥­í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆì§€ë§Œ ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì•„ì§ì€ ë¶€ì¡±í•œ ì ì´ ìˆë‹¤ê³  ì§€ì í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ì˜ëª»ëœ ì •ë³´ë¥¼ ìƒì„±í•˜ê±°ë‚˜, ì‚¬íšŒì ì¸ í¸ê²¬ ë° ë¶ˆí¸í•œ ë§ë“¤ì„ ìƒì„±í•˜ëŠ” ê²ƒì´ì£ . ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•™ê³„ì™€ì˜ í˜‘ì—…ì´ í•„ìš”í•˜ì§€ë§Œ OpenAIì˜ <code>text-davinci-003</code>ê³¼ ê°™ì€ ëª¨ë¸ì€ ì ‘ê·¼í•˜ê¸° í˜ë“  closed-source modelì´ê¸° ë•Œë¬¸ì— ì—°êµ¬ì— ì–´ë ¤ì›€ì´ ìˆë‹¤ê³  ë§í•©ë‹ˆë‹¤ğŸ¥²<br>ë§ˆì¹¨ Metaì—ì„œ LLaMaë¥¼ ê³µê°œí–ˆê³ , ê¸°ì¡´ì— ì•Œë ¤ì§„ ì—°êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ í›¨ì”¬ ì €ë ´í•œ ë¹„ìš©ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡, ë°ì´í„° ë° ëª¨ë¸ í•™ìŠµ ë°©ë²•ì„ ì¬í˜„ ê°€ëŠ¥í•˜ë„ë¡ ê³µê°œí•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.<br>ê²°ê³¼ì ìœ¼ë¡œ, AlpacaëŠ” text-davinci-003(175B)ë³´ë‹¤ í›¨ì”¬ ì‘ì€ 7B ëª¨ë¸ì´ì§€ë§Œ ìœ ì‚¬í•˜ê²Œ ë™ì‘í•œë‹¤ê³  í•©ë‹ˆë‹¤.<br><a target="_blank" rel="noopener" href="https://alpaca-ai.ngrok.io/">Gradio ê¸°ë°˜ ë°ëª¨ í˜ì´ì§€</a>ë„ ê³µê°œí–ˆëŠ”ë°, ì ‘ì†ì€ ê°€ë” ì•ˆë˜ëŠ” ê²ƒ ê°™ë„¤ìš”ğŸ¤”<br><img src="https://devocean.sk.com/editorImg/2023/3/23/c0b4da8b95c774dfc242df286f8ddde97cb4aef73ed9ade7233f3d6d1d3fe741" alt="image.png"><br>AlpacaëŠ” academic researchì— í•œí•´ì„œë§Œ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ê³  ìƒì—…ì  ì‚¬ìš©ì€ ê¸ˆì§€í•˜ê³  ìˆëŠ”ë°ìš”.<br>ì´ìœ ëŠ” LLaMaì˜ ë¼ì´ì„¼ìŠ¤ê°€ <a target="_blank" rel="noopener" href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform">non-commercial ë¼ì´ì„¼ìŠ¤</a>ë¼ëŠ”ì  ê·¸ë¦¬ê³  <a target="_blank" rel="noopener" href="https://openai.com/policies/terms-of-use">OpenAIì˜ tet-davinci-003ì—ì„œ ì–»ì–´ë‚¸ ë°ì´í„°ë¥¼ í™œìš©í–ˆë‹¤ëŠ” ì </a>ë“±ì„ ì´ìœ ë¡œ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.</p></div><a class="article-more button is-small is-size-7" href="/Alpaca/#more">ìì„¸íˆ ë³´ê¸°</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-02-20T13:07:50.000Z" title="2/20/2023, 10:07:50â€¯PM">2023-02-20</time>&nbsp;ê²Œì‹œ ë¨</span><span class="level-item"><time dateTime="2023-03-04T14:28:25.788Z" title="3/4/2023, 11:28:25â€¯PM">2023-03-04</time>&nbsp;ì—…ë°ì´íŠ¸ ë¨</span><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a></span><span class="level-item">21ë¶„ì•ˆì— ì½ê¸° (ì•½ 3187 ë‹¨ì–´)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/">SentencePieceë¥¼ í™œìš©í•œ íš¨ê³¼ì ì¸ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ë§Œë“¤ê¸°</a></h1><div class="content"><h1 id="ì†Œê°œ"><a href="#ì†Œê°œ" class="headerlink" title="ì†Œê°œ"></a>ì†Œê°œ</h1><p>ìì—°ì–´ ë¬¸ì¥ì„ ì»´í“¨í„°ê°€ ì‰½ê²Œ ì´í•´í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì³ì•¼í•©ë‹ˆë‹¤.<br>ê·¸ ì¤‘ í•˜ë‚˜ë¡œ ë¬¸ì¥ì„ í† í° ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ì²˜ë¦¬í•˜ëŠ” í† í¬ë‚˜ì´ì§• ê¸°ë²•ì´ ìˆìŠµë‹ˆë‹¤.<br>ì˜¤ëŠ˜ì€ SentencePieceë¥¼ í™œìš©í•˜ì—¬ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í† í¬ë‚˜ì´ì§• í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">SentencePiece</a>ëŠ” Googleì—ì„œ <a target="_blank" rel="noopener" href="https://aclanthology.org/D18-2012/">2018ë…„ë„ì— ê³µê°œí•œ</a> ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ë‹¤ì–‘í•œ ìì—°ì–´ì²˜ë¦¬ íƒœìŠ¤í¬ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.<br>ìµœê·¼ì—ëŠ” <a target="_blank" rel="noopener" href="https://github.com/huggingface/tokenizers">Huggingfaceì—ì„œ ê³µê°œí•œ Tokenizers</a>ë„ ìì£¼ ì‚¬ìš©ë˜ê³  ìˆì§€ë§Œ ì˜¤ëŠ˜ì€ Sentencepieceì— ëŒ€í•œ ë‚´ìš©ì„ ì£¼ë¡œ ë‹¤ë£¨ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.</p>
<h3 id="í…ìŠ¤íŠ¸-ì „ì²˜ë¦¬-ê³¼ì •-ì˜ˆì‹œ"><a href="#í…ìŠ¤íŠ¸-ì „ì²˜ë¦¬-ê³¼ì •-ì˜ˆì‹œ" class="headerlink" title="í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê³¼ì • ì˜ˆì‹œ"></a>í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê³¼ì • ì˜ˆì‹œ</h3><table>
<thead>
<tr>
<th>ë°ì´í„° ì „ì²˜ë¦¬</th>
<th>ì˜ˆì‹œ</th>
</tr>
</thead>
<tbody><tr>
<td>í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì´í•´í•˜ëŠ” ë²¡í„°ë¡œ ë°”ê¾¸ë ¤ë©´ ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì¹˜ê²Œ ë©ë‹ˆë‹¤. ì˜¤ëŠ˜ ë‹¤ë£¨ëŠ” ì£¼ì œëŠ” ì´ì¤‘ì—ì„œë„ ê°€ì¥ ì•ë‹¨ì— ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ìª¼ê°œëŠ”(Split) ì „ì²˜ë¦¬ ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì€ <code>&quot;íˆì–´ë¡œ ë¬´ë¹„ ì¤‘ ê°€ì¥ ì–´ë‘¡ì§€ë§Œ ê°€ì¥ ì°¸ì‹ í–ˆë‹¤.&quot;</code> ë¼ëŠ” ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ë¬¸ì¥ì€ word, character, í˜•íƒœì†Œ, subword (char, byte)ë“± ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ìª¼ê°œì„œ ì²˜ë¦¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</td>
<td><img src="https://user-images.githubusercontent.com/7252598/220215155-57aa7c35-ee6a-4aaf-8b54-d302a093186c.png" alt="image"></td>
</tr>
</tbody></table>
<h1 id="ì„¤ì¹˜ë°©ë²•"><a href="#ì„¤ì¹˜ë°©ë²•" class="headerlink" title="ì„¤ì¹˜ë°©ë²•"></a>ì„¤ì¹˜ë°©ë²•</h1><ul>
<li>pyenvë¥¼ í†µí•´ sentencepieceë¥¼ ì„¤ì¹˜í•  í™˜ê²½ì„ êµ¬ì„±í•©ë‹ˆë‹¤</li>
<li>ì‘ì„±ì‹œì  ê¸°ì¤€ ë¹„êµì  ìµœì‹ ì¸ 3.11.x ë²„ì „ì„ ì„¤ì¹˜í•´ì¤ë‹ˆë‹¤</li>
<li>ì„¤ì¹˜ëŠ” pyenv ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤ (MacOS í™˜ê²½ì…ë‹ˆë‹¤)</li>
</ul></div><a class="article-more button is-small is-size-7" href="/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/#more">ìì„¸íˆ ë³´ê¸°</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-02-16T08:24:12.000Z" title="2/16/2023, 5:24:12â€¯PM">2023-02-16</time>&nbsp;ê²Œì‹œ ë¨</span><span class="level-item"><time dateTime="2023-02-16T08:25:22.220Z" title="2/16/2023, 5:25:22â€¯PM">2023-02-16</time>&nbsp;ì—…ë°ì´íŠ¸ ë¨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">20ë¶„ì•ˆì— ì½ê¸° (ì•½ 3031 ë‹¨ì–´)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Toolformer/">Toolformer: Language Models Can Teach Themselves to Use Tools</a></h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>ë¯¸ë¦¬ í•™ìŠµë°ì´í„°ë¥¼ API ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•´ë†“ë˜, ìƒì„±í• ë•ŒëŠ” lossì— ë„ì›€ì´ ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ êµ¬ì„±í•´ë†“ê³ , ì‹¤ì œ ì¸í¼ëŸ°ìŠ¤í• ë•Œ APIì½œê³¼ ê´€ë ¨ëœ í† í°ì´ ë‚˜ì˜¤ë©´ ì ì‹œ ë””ì½”ë”© ì¤‘ì§€ í›„ APIì½œí•˜ê³  ê²°ê³¼ ë°›ì•„ì˜¨ë‹¤ìŒì— ë‹¤ì‹œ ì´ì–´ì„œí•˜ëŠ” ë°©ì‹!</li>
<li>APIì¢…ë¥˜ê°€ ë§ì§„ ì•Šì•„ì„œ, ì™„ì „ ë²”ìš©ì ì¸ í‰ê°€ë¼ í•˜ê¸°ì—” ì• ë§¤í•˜ê³  ì•½ê°„ ë¬´ê±°ìš´ê²ƒ ê°™ê¸°ë„í•˜ë‚˜(í•™ìŠµê³¼ì •ì´), ì‹¤ì œ ì‚¬ìš©í• ë• í¸í• ìˆ˜ë„</li>
<li>ê³µê°œëœ ë ˆí¬ëŠ” ì—†ì§€ë§Œ, lucidrainsê°€ ë§Œë“¤ê¸° ì‹œë„ (<a target="_blank" rel="noopener" href="https://github.com/lucidrains/toolformer-pytorch">https://github.com/lucidrains/toolformer-pytorch</a>)</li>
<li>paper: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/10753629/Toolformer-.Language.Models.Can.Teach.Themselves.to.Use.Tools.pdf">Toolformer Language Models Can Teach Themselves to Use Tools.pdf</a></li>
<li>ìì„¸í•œ ë‚´ìš©ì€ slide ì°¸ê³ : <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/10753635/Toolformer.pdf">Toolformer.pdf</a></li>
</ul>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><ul>
<li>Timo Schick Jane Dwivedi-Yu Roberto DessÃ¬â€  Roberta Raileanu Maria Lomeli Luke Zettlemoyer Nicola Cancedda Thomas Scialom<ul>
<li><code>Meta AI Research</code> â€ Universitat Pompeu Fabra</li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>They also, <strong>paradoxically</strong>, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel.</li>
<li>In this paper, we show that <strong>LMs can teach themselves</strong> to use <code>external tools via simple APIs</code> and achieve the best of both worlds.</li>
<li>We introduce <code>Toolformer</code>, a model <code>trained to decide which APIs to call</code>, <code>when to call</code> them, <code>what arguments to pass</code>, and <code>how to best incorporate the results into future token prediction</code>.<ul>
<li>ì´ê²Œ ì´ ë…¼ë¬¸ì˜ í•µì‹¬ì´ë„¤, ì–´ë–¤ APIë¥¼ ì½œí• ì§€, ì–¸ì œ ì½œí• ì§€, ì–´ë–¤ argsë¥¼ ë„£ì„ì§€ ì–´ë–»ê²Œ future token ì˜ˆì¸¡ì— ì“¸ê±´ì§€ë¥¼ ê³ ë¥´ëŠ” ê²ƒ!</li>
</ul>
</li>
<li>This is done in a self-supervised way, <code>requiring nothing more than a handful of demonstrations for each API</code></li>
<li>We incorporate a range of tools, including a calculator, a Q&amp;A system, a search engine, a translation system, and a calendar.</li>
<li>Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1></div><a class="article-more button is-small is-size-7" href="/Toolformer/#more">ìì„¸íˆ ë³´ê¸°</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-02-13T04:18:48.000Z" title="2/13/2023, 1:18:48â€¯PM">2023-02-13</time>&nbsp;ê²Œì‹œ ë¨</span><span class="level-item"><time dateTime="2023-02-13T04:19:27.901Z" title="2/13/2023, 1:19:27â€¯PM">2023-02-13</time>&nbsp;ì—…ë°ì´íŠ¸ ë¨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">20ë¶„ì•ˆì— ì½ê¸° (ì•½ 3026 ë‹¨ì–´)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/SELF-INSTRUCT%20Aligning%20Language%20Model%20with%20Self%20Generated%20Instructions/">SELF-INSTRUCT Aligning Language Model with Self Generated Instructions</a></h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>code: <a target="_blank" rel="noopener" href="https://github.com/yizhongw/self-instruct">https://github.com/yizhongw/self-instruct</a></li>
<li>slide: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/10718545/SELF-INSTRUCT.pdf">SELF-INSTRUCT.pdf</a></li>
<li>ì—¬ê¸° ìˆëŠ” Instructionì€ NLP taskìª½ì´ë¼ê¸°ë³´ë‹¤ InstructGPTì—ì„œì˜ promptì¤‘ì— ëª…ë ¹ê´€ë ¨ í‘œí˜„ì„ ì˜ë¯¸í•˜ëŠ” ë“¯</li>
<li>Instructionë¿ë§Œ ì•„ë‹ˆë¼ Instanceë„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— challengeí•˜ê³ , LLMì— ë‚´ì¬ë˜ì–´ìˆëŠ” ëŠ¥ë ¥ì„ êº¼ë‚´ë˜ êº¼ë‚¸ ì»¨í…ì¸ ë„ LLMì•ˆì— ìˆëŠ”ê±°ë¼ì„œ, humanì˜ ê°œì…ì´ ì˜ì•ˆë“¤ì–´ê°„ self-Instruct+Instanceë¼ê³  í•  ìˆ˜ ìˆì„ë“¯<ul>
<li>Because of SELF-INSTRUCTâ€™s dependence on the inductive biases extracted from LMs</li>
</ul>
</li>
<li>InstructGPT_001 ì •ë„ì˜ ëª¨ë¸ì„ íœ´ë¨¼ë¦¬ì†ŒìŠ¤ ì ê²Œí•´ì„œ ë§Œë“œëŠ” ë°©ë²•</li>
<li><a target="_blank" rel="noopener" href="https://github.com/yizhongw/self-instruct/blob/main/data/seed_tasks.jsonl">175ê°œ ì‹œë“œ í…œí”Œë¦¿</a></li>
</ul>
<table>
<thead>
<tr>
<th>ì˜ˆì‹œ1</th>
<th>ì˜ˆì‹œ2</th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://user-images.githubusercontent.com/7252598/217453282-8fc30591-2633-473a-b157-e08abb47596d.png" alt="image"></td>
<td><img src="https://user-images.githubusercontent.com/7252598/217465744-c1fcb396-cbc1-468b-86e4-d72d5d005e9d.png" alt="ë‘ë²ˆì§¸ì˜ˆì‹œ"></td>
</tr>
</tbody></table>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><ul>
<li>Yizhong Wangâ™£ Yeganeh Kordiâ™¢ Swaroop Mishraâ™¡ Alisa Liuâ™£ Noah A. Smithâ™£+ Daniel Khashabiâ™  Hannaneh Hajishirziâ™£+ </li>
<li>â™£University of Washington â™¢Tehran Polytechnic â™¡Arizona State University â™ Johns Hopkins University +Allen Institute for AI</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>Large â€œinstruction-tunedâ€ language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks.<ul>
<li>Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model.</li>
</ul>
</li>
<li>We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations.</li>
<li>Our pipeline <ul>
<li>generates <ul>
<li>instruction, </li>
<li>input, and </li>
<li>output samples</li>
</ul>
</li>
<li>from a language model, then prunes them before using them to finetune the original model.</li>
</ul>
</li>
<li>Applying our method to vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on SUPERNATURALINSTRUCTIONS, on par with the performance of InstructGPT001</li>
<li>we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with SELF-INSTRUCT outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT001</li>
<li>SELF-INSTRUCT provides an almost annotation-free method for aligning pretrained language models with instructions, and we <a target="_blank" rel="noopener" href="https://github.com/yizhongw/self-instruct">release our large synthetic dataset to facilitate future studies on instruction tuning</a>.</li>
</ul></div><a class="article-more button is-small is-size-7" href="/SELF-INSTRUCT%20Aligning%20Language%20Model%20with%20Self%20Generated%20Instructions/#more">ìì„¸íˆ ë³´ê¸°</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-02-06T04:15:12.000Z" title="2/6/2023, 1:15:12â€¯PM">2023-02-06</time>&nbsp;ê²Œì‹œ ë¨</span><span class="level-item"><time dateTime="2023-02-06T04:15:59.816Z" title="2/6/2023, 1:15:59â€¯PM">2023-02-06</time>&nbsp;ì—…ë°ì´íŠ¸ ë¨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">14ë¶„ì•ˆì— ì½ê¸° (ì•½ 2138 ë‹¨ì–´)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/(FLAN)%20Finetuned%20Language%20Models%20Are%20Zero-Shot%20Learners/">(FLAN) Finetuned Language Models Are Zero-Shot Learners</a></h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>2022ë…„ 2ì›” êµ¬ê¸€ ë¦¬ì„œì¹˜ìª½ ë…¼ë¬¸</li>
<li>github: <a target="_blank" rel="noopener" href="https://github.com/google-research/flan">https://github.com/google-research/flan</a></li>
<li>ì°¸ê³ í•  ë°ì´í„° mixture code: <a target="_blank" rel="noopener" href="https://github.com/google-research/FLAN/blob/main/flan/v2/mixtures.py">https://github.com/google-research/FLAN/blob/main/flan/v2/mixtures.py</a></li>
<li>ë…¼ë¬¸ pdf:<br><a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/10659673/FLAN.FINETUNED.LANGUAGE.MODELS.ARE.ZERO-SHOT.LEARNERS.pdf">(FLAN)FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS.pdf</a></li>
</ul>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><ul>
<li>Jason Weiâˆ—, Maarten Bosmaâˆ—, Vincent Y. Zhaoâˆ—, Kelvin Guuâˆ—, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le<ul>
<li><strong>Google Research</strong></li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>explores a simple method for improving the zero-shot learning abilities of language models.</li>
<li>instruction tuningâ€”finetuning language models on a collection of datasets described via instructionsâ€”substantially improves zero-shot performance on unseen tasks.</li>
<li><strong>137B</strong> parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types.<br><img src="https://user-images.githubusercontent.com/7252598/216854220-97b06f92-1c9b-4bac-996d-f4de2bf9c5fd.png" alt="image"></li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1></div><a class="article-more button is-small is-size-7" href="/(FLAN)%20Finetuned%20Language%20Models%20Are%20Zero-Shot%20Learners/#more">ìì„¸íˆ ë³´ê¸°</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-02-03T07:44:54.000Z" title="2/3/2023, 4:44:54â€¯PM">2023-02-03</time>&nbsp;ê²Œì‹œ ë¨</span><span class="level-item"><time dateTime="2023-02-03T07:45:54.565Z" title="2/3/2023, 4:45:54â€¯PM">2023-02-03</time>&nbsp;ì—…ë°ì´íŠ¸ ë¨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">14ë¶„ì•ˆì— ì½ê¸° (ì•½ 2025 ë‹¨ì–´)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/(T0)%20Multitask%20Prompted%20Training%20Enables%20Zero-Shot%20Task%20Generalization/">(T0) Multitask Prompted Training Enables Zero-Shot Task Generalization</a></h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>Zero-Shotì˜ ê°€ëŠ¥ì„± ì—´ì–´ì¤Œ</li>
<li>T5ì˜ ë§ˆë¬´ë¦¬ ë…¼ë¬¸ê²©</li>
<li>All trained models are available at <a target="_blank" rel="noopener" href="https://github.com/bigscience-workshop/t-zero">https://github.com/bigscience-workshop/t-zero</a></li>
<li>all prompts are available at <a target="_blank" rel="noopener" href="https://github.com/bigscience-workshop/promptsource">https://github.com/bigscience-workshop/promptsource</a>.</li>
<li>ë…¼ë¬¸ pdf íŒŒì¼: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/10576665/T0.MULTITASK.PROMPTED.TRAINING.ENABLES.ZERO-SHOT.TASK.GENERALIZATION.pdf">(T0)MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK GENERALIZATION.pdf</a></li>
</ul>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><ul>
<li>V Sanh (Hugging Face) ì €ìˆ  Â· 2021</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>Q) Can zero-shot generalization instead be directly induced by explicit multitask learning?</li>
<li>we develop a system for easily mapping any natural language tasks into a human-readable prompted form. <ul>
<li>convert a large set of supervised datasets, each with multiple prompts with diverse wording</li>
<li>fine-tune a pre-trained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multi-task mixture covering a wide variety of tasks</li>
<li>The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16Ã— its size.</li>
</ul>
</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1></div><a class="article-more button is-small is-size-7" href="/(T0)%20Multitask%20Prompted%20Training%20Enables%20Zero-Shot%20Task%20Generalization/#more">ìì„¸íˆ ë³´ê¸°</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-01-20T14:21:17.000Z" title="1/20/2023, 11:21:17â€¯PM">2023-01-20</time>&nbsp;ê²Œì‹œ ë¨</span><span class="level-item"><time dateTime="2023-01-20T14:24:23.531Z" title="1/20/2023, 11:24:23â€¯PM">2023-01-20</time>&nbsp;ì—…ë°ì´íŠ¸ ë¨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">33ë¶„ì•ˆì— ì½ê¸° (ì•½ 5011 ë‹¨ì–´)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/InstructGPT/">(InstructGPT) Training language models to follow instructions with human feedback</a></h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>ChatGPTë¥¼ ê°€ê¸° ìœ„í•œ ê¸°ì´ˆë…¼ë¬¸</li>
<li>paper: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/10467006/Training.language.models.to.follow.instructions.with.human.feedback.pdf">Training language models to follow instructions with human feedback.pdf</a></li>
<li>ê²°êµ­ real-worldì˜ promptsê°€ ì—„ì²­ ì¤‘ìš”í•˜ê³ , ê·¸ê±¸ labelersë¥¼ í†µí•´ ì˜ demonstrationì„ ì ì–´ë†”ì•¼ë˜ê³ , output rankingsì„ í†µí•´ RMì„ ì˜ êµ½ê³  plm objë¥¼ íŠ¹ì • ë¹„ìœ¨ë¡œ ìœ ì§€ì‹œí‚¤ë©´ì„œ PPOë¥¼ ì˜ êµ½ëŠ”ê²Œ í•µì‹¬..!</li>
</ul>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Ouyang,+L">Long Ouyang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Wu,+J">Jeff Wu</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Jiang,+X">Xu Jiang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Almeida,+D">Diogo Almeida</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Wainwright,+C+L">Carroll L. Wainwright</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Mishkin,+P">Pamela Mishkin</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Zhang,+C">Chong Zhang</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Agarwal,+S">Sandhini Agarwal</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Slama,+K">Katarina Slama</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Ray,+A">Alex Ray</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Schulman,+J">John Schulman</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Hilton,+J">Jacob Hilton</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Kelton,+F">Fraser Kelton</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Miller,+L">Luke Miller</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Simens,+M">Maddie Simens</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Askell,+A">Amanda Askell</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Welinder,+P">Peter Welinder</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Christiano,+P">Paul Christiano</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Leike,+J">Jan Leike</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Lowe,+R">Ryan Lowe</a><ul>
<li>OpenAI</li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>Making language models bigger does not inherently make them better at following a userâ€™s intent<ul>
<li>ëª¨ë¸ í¬ê¸°ë§Œ í‚¤ìš°ëŠ”ê²Œ ìœ ì €ì˜ ì˜ë„ë¥¼ ë”°ë¼ê°€ëŠ” ê´€ì ì—ì„œëŠ” ë” ë‚«ê²Œí•´ì£¼ì§„ ì•ŠìŒ</li>
</ul>
</li>
<li>large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user<ul>
<li>ë¯¿ê¸° ì–´ë µê±°ë‚˜ toxicí•˜ê±°ë‚˜ í•˜ëŠ” ë¬¸ì¥ë„ ìƒì„±í•´ë‚´ê¸° ë•Œë¬¸</li>
</ul>
</li>
<li>ì´ëŸ° ëª¨ë¸ë“¤ì€ <strong>not aligned</strong> with their users!</li>
<li>show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.<ul>
<li>ì´ ë…¼ë¬¸ì—ì„œ LMì„ ìœ ì €ì˜ ì˜ë„ì— ë§ê²Œ íŒŒì¸íŠœë‹í•˜ê³  human feedbackì„ ì¤˜ì„œ aligní–ˆì„ë•Œ íš¨ê³¼ë¥¼ ë³´ì—¬ì¤„ ê²ƒì„</li>
</ul>
</li>
<li>Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning.<ul>
<li>GPT-3 íŠœë‹í•  ëª©ì ìœ¼ë¡œ labeler demonstrations ë°ì´í„°ì…‹ ìˆ˜ì§‘</li>
</ul>
</li>
<li>collect a dataset of rankings of model outputs<ul>
<li>ëª¨ë¸ ì•„ì›ƒí’‹ì— ëŒ€í•œ ë­í‚¹ìœ¼ë¡œ ìˆ˜ì§‘í•¨</li>
</ul>
</li>
<li>which we use to further fine-tune this supervised model using reinforcement learning from human feedback<ul>
<li>RLHFë¡œ íŒŒì¸íŠœë‹í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•  ê²ƒ</li>
</ul>
</li>
<li>call the resulting models InstructGPT<ul>
<li>í•™ìŠµí•œ ëª¨ë¸ì„ InstructGPTë¡œ ë¶€ë¥¼ ê²ƒì„</li>
</ul>
</li>
<li>In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.<ul>
<li>Human í‰ê°€ì—ì„œ ë³´ë©´ 1.3B InstructGPTê°€ 175 GPT-3ë³´ë‹¤ ê²°ê³¼ê°€ ì¢‹ìŒ</li>
</ul>
</li>
<li>InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets<ul>
<li>truthfulnessë‚˜ toxic ê´€ì ì—ì„œë„ ê½¤ ê°œì„ ì´ ë³´ì˜€ìŒ</li>
</ul>
</li>
<li>RLHFê¸°ë°˜ finetuningì´ aligning LMs with human intents ê´€ì ì—ì„œ ê½¤ promising directionì„ì„ ë³´ì„</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1></div><a class="article-more button is-small is-size-7" href="/InstructGPT/#more">ìì„¸íˆ ë³´ê¸°</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-01-09T07:34:18.000Z" title="1/9/2023, 4:34:18â€¯PM">2023-01-09</time>&nbsp;ê²Œì‹œ ë¨</span><span class="level-item"><time dateTime="2023-01-09T07:34:37.397Z" title="1/9/2023, 4:34:37â€¯PM">2023-01-09</time>&nbsp;ì—…ë°ì´íŠ¸ ë¨</span><span class="level-item">11ë¶„ì•ˆì— ì½ê¸° (ì•½ 1598 ë‹¨ì–´)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Chinchilla-Training-Compute-Optimal-Large-Language-Models/">(Chinchilla) Training Compute-Optimal Large Language Models</a></h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>paper file: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/10371283/Training.Compute-Optimal.Large.Language.Models.pdf">Training Compute-Optimal Large Language Models.pdf</a></li>
<li>ìµœì  ëª¨ë¸ í¬ê¸°ì™€ ë°ì´í„° í¬ê¸°, FLOPsë¥¼ ì•Œê¸°ìœ„í•œ í•¨ìˆ˜ë¥¼ estimateí–ˆë˜ ë…¼ë¬¸</li>
<li>ë°ì´í„° ìŠ¤ì¼€ì¼ë§ë„ ëª¨ë¸ìŠ¤ì¼€ì¼ë§ë§Œí¼ ì¤‘ìš”í•˜ë‹¤!</li>
</ul>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><ul>
<li>Jordan Hoffmannâ˜…, Sebastian Borgeaudâ˜…, Arthur Menschâ˜…, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifreâ˜… (â˜…Equal contributions)<ul>
<li>DeepMind</li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>investigate the optimal model size and number of tokens for training a transformer language model</li>
<li>By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, find that for compute-optimal training, <strong>the model size and the number of training tokens should be scaled equally</strong><ul>
<li>ëª¨ë¸ ì‚¬ì´ì¦ˆì™€ í•™ìŠµ í† í°ì˜ ìŠ¤ì¼€ì¼ì€ ë¹„ë¡€í•¨</li>
</ul>
</li>
<li>for every <strong>doubling</strong> of model size the number of training tokens should also be <strong>doubled</strong></li>
<li>test this hypothesis by training a predicted compute-optimal model, <code>Chinchilla</code>, that uses the <code>same compute budget as Gopher</code> but with <code>70B parameters and 4Ã— more more data</code></li>
<li>Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.<ul>
<li>ì¹œì¹ ë¼ê°€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼íƒœìŠ¤í¬ì—ì„œ ë‹¤ ì´ê²¼ë‹¤?</li>
</ul>
</li>
<li>Chinchilla reaches a state-of-the-art average accuracy of <code>67.5% on the MMLU benchmark</code>, greater than a 7% improvement over Gopher</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1></div><a class="article-more button is-small is-size-7" href="/Chinchilla-Training-Compute-Optimal-Large-Language-Models/#more">ìì„¸íˆ ë³´ê¸°</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-12-12T06:45:59.000Z" title="12/12/2022, 3:45:59â€¯PM">2022-12-12</time>&nbsp;ê²Œì‹œ ë¨</span><span class="level-item"><time dateTime="2022-12-12T06:46:28.958Z" title="12/12/2022, 3:46:28â€¯PM">2022-12-12</time>&nbsp;ì—…ë°ì´íŠ¸ ë¨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">18ë¶„ì•ˆì— ì½ê¸° (ì•½ 2743 ë‹¨ì–´)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Robust-Conversational-Agents-against-Imperceptible-Toxicity-Triggers/">Robust Conversational Agents against Imperceptible Toxicity Triggers</a></h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>Github: <a target="_blank" rel="noopener" href="https://github.com/Ninarehm/Robust-Agents">https://github.com/Ninarehm/Robust-Agents</a></li>
<li>ë°œí‘œìë£Œ: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/10205206/pdf_2_Robust.Conversational.Agents.against.Imperceptible.Toxicity.Triggers.pdf">Robust Conversational Agents against Imperceptible Toxicity Triggers.pdf</a></li>
</ul>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><ul>
<li>Ninareh Mehrabi1, Ahmad Beirami2âˆ—, Fred Morstatter1, Aram Galstyan1 </li>
<li>1University of Southern California - Information Sciences Institute 2Meta AI</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>ìµœê·¼ NLP ì—°êµ¬ëŠ” ë‹¤ì–‘í•œ toxicity detectionì— ê°œì„ ì´ ìˆì—ˆìŒ<ul>
<li>toxicity detection models with the intention of identifying and mitigating toxic language from existing systems.</li>
</ul>
</li>
<li>ê¸°ì¡´ ì—°êµ¬ê°€ ë§ê¸´í•˜ë‚˜ adversarial attacksê³¼ defenseì— ëŒ€í•œ ì—°êµ¬ëŠ” ë¶€ì¡±í–ˆìŒ<ul>
<li>adversarial attacks that force the system to generate toxic language and the defense against them</li>
</ul>
</li>
<li>ê¸°ì¡´ì˜ ì—°êµ¬ëŠ” ëŒ€ë¶€ë¶„ ì‚¬ëŒì´ attack ìš© ë¬¸ì¥ì„ ìƒì„±í•´ì™”ìŒ, ë¹„ìš©ì´ ë¹„ì‹¸ê³  í™•ì¥ê°€ëŠ¥í•˜ì§€ ì•ŠìŒ</li>
<li>ë°˜ë©´ì— ìë™í™”í•´ì„œ ë§Œë“  attackì¸ ê²½ìš° attack vectorê°€ human-like languageì™€ ë§ì§€ ì•ŠìŒ, ì´ëŠ” LM lossë¡œ detectingì´ ê°€ëŠ¥í•¨<ul>
<li>Existing work to generate such attacks is either based on human-generated attacks which is costly and not scalable or, in case of automatic attacks, the attack vector does not conform to human-like language, which can be detected using a language model loss</li>
</ul>
</li>
<li>ë³¸ ì—°êµ¬ì—ì„œëŠ” conversational agentsë¥¼ ëˆˆì— ë„ì§€ ì•Šê²Œ ê³µê²©(ì•ì„œ ìë™í™”í•œ ê³µê²©ê³¼ ë‹¬ë¦¬ ì¸ì‹ë˜ì§€ ëª»í•˜ê²Œ) í•˜ëŠ” ë°©ë²•ì„ coherency, relevancy, fluency ê´€ì ì—ì„œ ì œì•ˆí•¨<ul>
<li>propose attacks against conversational agents that are imperceptible, i.e., they fit the conversation in terms of coherency, relevancy, and fluency, while they are effective and scalable, i.e., they can automatically trigger the system into generating toxic language</li>
</ul>
</li>
<li>ë³¸ ì—°êµ¬ì—ì„œëŠ” ì œì•ˆí•œ attackì— ëŒ€í•œ defense mechanismë„ ì œì•ˆí•¨. ê³µê²©ì„ ì™„í™”ì‹œí‚¬ë¿ë§Œ ì•„ë‹ˆë¼ conversational flowë„ ìœ ì§€ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì•ˆí•¨<ul>
<li>propose a defense mechanism against such attacks which not only mitigates the attack but also attempts to maintain the conversational flow</li>
</ul>
</li>
<li>ê²°ë¡ ì ìœ¼ë¡œ ê³µê²©ì´ ì˜ë“¤ì–´ì™€ë„ ì˜ ë§‰ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì— ëŒ€í•´ automaitc and human evaluationsí–ˆê³  íš¨ê³¼ì ì„ì„ ë³´ì˜€ìŒ<ul>
<li>our defense is effective at avoiding toxic language generation even against imperceptible toxicity triggers while the generated language fits the conversation in terms of coherency and relevancy</li>
</ul>
</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1></div><a class="article-more button is-small is-size-7" href="/Robust-Conversational-Agents-against-Imperceptible-Toxicity-Triggers/#more">ìì„¸íˆ ë³´ê¸°</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-11-02T06:27:15.000Z" title="11/2/2022, 3:27:15â€¯PM">2022-11-02</time>&nbsp;ê²Œì‹œ ë¨</span><span class="level-item"><time dateTime="2022-11-02T06:29:32.403Z" title="11/2/2022, 3:29:32â€¯PM">2022-11-02</time>&nbsp;ì—…ë°ì´íŠ¸ ë¨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">30ë¶„ì•ˆì— ì½ê¸° (ì•½ 4456 ë‹¨ì–´)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/SOCIAL-CHEMISTRY-101/">SOCIAL CHEMISTRY 101 - Learning to Reason about Social and Moral Norms</a></h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>web site: <a target="_blank" rel="noopener" href="https://maxwellforbes.com/social-chemistry/">https://maxwellforbes.com/social-chemistry/</a></li>
<li>paper: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/9916874/SOCIAL.CHEMISTRY.101-.Learning.to.Reason.about.Social.and.Moral.Norms.pdf">SOCIAL CHEMISTRY 101- Learning to Reason about Social and Moral Norms.pdf</a></li>
<li>ë°ì´í„°ì…‹ ìˆ˜ì§‘ ìˆœì„œ<ul>
<li>ìƒí™© ìˆ˜ì§‘ í•„ìš” (ì—¬ê¸°ì„  ë ˆë”§, ê³ ë¯¼ìƒë‹´, ì°¬ë°˜ì¢Œë“±ì´ ìˆì„ë²•í•œ ìƒí™©ì„¤ì •ëœê³³ì—ì„œ ê°€ì ¸ì˜´) </li>
<li>â†’ ìƒí™©ë§ˆë‹¤ RoTë¥¼ 1~5ê°œ ìƒì„± </li>
<li>â†’  RoTë§ˆë‹¤ multiple annotation ì‘ì„± (ê°™ì€ RoTë¥¼ ë‹¤ë¥´ê²Œ ë³¼ ìˆ˜ ìˆìœ¼ë‹ˆ..) </li>
<li>â†’ RoT Breakdown, Action Breakdown (Actionë„ í•´ì•¼ë˜ë‚˜.. RoTì•ˆì— í¬í•¨ëœê±°ì•„ë‹Œê°€.. attribute ìì²´ëŠ” ë‹¤ë¥´ê¸´í•œë°? ì •ë‹µì€! RoTëŠ” ë„ë•ì ì¸ ê´€ì ì—ì„œ ë”°ì§€ê³ , Actionì€ ë²•ì´ë‚˜ ë¬¸í™”ì ì¸ ê´€ì ì—ì„œ ë”°ì§) </li>
<li>â†’ ì¶”ê°€ì ì¸ íƒœê¹…ë„ ì§„í–‰ ë¶ˆë¶„ëª…í•œ ìƒí™©ì´ê±°ë‚˜, 19ê¸ˆìƒí™©ì´ê±°ë‚˜, ë„ˆë¬´ë¶ˆí¸í•œì£¼ì œ(í•™ëŒ€ë‚˜,â€¦); ì´ëŸ° ê²½ìš°ëŠ” ìŠ¤í‚µê°€ëŠ¥</li>
</ul>
</li>
<li>RoTê°€ì´ë“œë¼ì¸ë“±ì€ ë¹„ìŠ·í•˜ê²Œ ê°€ì ¸ê°€ë©´ ë  ê²ƒìœ¼ë¡œ ë³´ì„</li>
<li>137ëª…ì´ íƒœê¹…í•¨</li>
<li>split our dataset into 80&#x2F;10&#x2F;10% train&#x2F;dev&#x2F;test partitions by situation</li>
<li>ë°ì´í„° ì˜ˆì‹œ</li>
</ul>
<table>
<thead>
<tr>
<th>area</th>
<th>m</th>
<th>split</th>
<th>rot-agree</th>
<th>rot-categorization</th>
<th>rot-moral-foundations</th>
<th>rot-char-targeting</th>
<th>rot-bad</th>
<th>rot-judgment</th>
<th>action</th>
<th>action-agency</th>
<th>action-moral-judgment</th>
<th>action-agree</th>
<th>action-legal</th>
<th>action-pressure</th>
<th>action-char-involved</th>
<th>action-hypothetical</th>
<th>situation</th>
<th>situation-short-id</th>
<th>rot</th>
<th>rot-id</th>
<th>rot-worker-id</th>
<th>breakdown-worker-id</th>
<th>n-characters</th>
<th>characters</th>
</tr>
</thead>
<tbody><tr>
<td>amitheasshole</td>
<td>1</td>
<td>train</td>
<td>4</td>
<td>advice</td>
<td>loyalty-betrayal</td>
<td>char-1</td>
<td>0</td>
<td>itâ€™s bad</td>
<td>doing something that causes other people to lose trust in you.</td>
<td>agency</td>
<td>-1</td>
<td>3</td>
<td>legal</td>
<td>-2</td>
<td>char-1</td>
<td>hypothetical</td>
<td>losing trust in my friend</td>
<td>reddit&#x2F;amitheasshole&#x2F;aypvmz</td>
<td>Itâ€™s bad to do something that causes other people to lose trust in you.</td>
<td>rot&#x2F;reddit&#x2F;amitheasshole&#x2F;aypvmz&#x2F;3K5TEWLKGYQFYAIY0H6JQMIY5MEIVM&#x2F;127&#x2F;2</td>
<td>127</td>
<td>0</td>
<td>2</td>
<td>narrator|my friend</td>
</tr>
<tr>
<td>amitheasshole</td>
<td>1</td>
<td>dev</td>
<td>3</td>
<td>social-norms</td>
<td>loyalty-betrayal</td>
<td>char-0</td>
<td>0</td>
<td>expected</td>
<td>people participating in the big events in their friendsâ€™ lives if asked.</td>
<td>agency</td>
<td>0</td>
<td>3</td>
<td>legal</td>
<td>2</td>
<td>char-0</td>
<td>explicit-no</td>
<td>saying no to being a bridesmaid at a friendâ€™s wedding</td>
<td>reddit&#x2F;amitheasshole&#x2F;9tzn0z</td>
<td>People are expected to participate in the big events in their friendsâ€™ lives if asked.</td>
<td>rot&#x2F;reddit&#x2F;amitheasshole&#x2F;9tzn0z&#x2F;3EG49X351XRR9FSSYVYCH4PEC656XX&#x2F;89&#x2F;1</td>
<td>89</td>
<td>39</td>
<td>3</td>
<td>narrator|a bridesmaid|a friend</td>
</tr>
<tr>
<td>amitheasshole</td>
<td>1</td>
<td>test</td>
<td>3</td>
<td>social-norms</td>
<td>care-harm|loyalty-betrayal</td>
<td>char-1</td>
<td>0</td>
<td>Partners should</td>
<td>Listening to each otherâ€™s issues.</td>
<td>agency</td>
<td>2</td>
<td>3</td>
<td>legal</td>
<td>2</td>
<td>char-1</td>
<td>probable</td>
<td>telling my boyfriend I am bored and unhappy at my job</td>
<td>reddit&#x2F;amitheasshole&#x2F;a1311q</td>
<td>Partners should listen to each otherâ€™s issues.</td>
<td>rot&#x2F;reddit&#x2F;amitheasshole&#x2F;a1311q&#x2F;3JV9LGBJWWT6CZ369HK2AIBAUGUGOV&#x2F;111&#x2F;2</td>
<td>111</td>
<td>145</td>
<td>2</td>
<td>narrator|my boyfriend</td>
</tr>
</tbody></table>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><p>Maxwell Forbesâ€ â€¡ Jena D. Hwangâ€¡ Vered Shwartzâ€ â€¡ Maarten Sapâ€  Yejin Choiâ€ â€¡<br>â€ Paul G. Allen School of Computer Science &amp; Engineering, University of Washington â€¡Allen Institute for AI</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>introduce SOCIAL-CHEM- 101, a large-scale corpus that catalogs <code>292k rules-of-thumb</code> such as <code>â€œIt is rude to run a blender at 5amâ€</code> as the basic conceptual units.</li>
<li>Each <code>rule-of-thumb</code> is further broken down with <code>12 different dimensions</code> of peopleâ€™s judgments, including <code>social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality</code><ul>
<li>which together amount to over <code>4.5 million annotations</code> of categorical labels and free-text descriptions.</li>
</ul>
</li>
<li>NEURAL NORM TRANSFORMER, learns and generalizes SOCIAL-CHEM-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb</li>
</ul></div><a class="article-more button is-small is-size-7" href="/SOCIAL-CHEMISTRY-101/#more">ìì„¸íˆ ë³´ê¸°</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">ì´ì „</a></div><div class="pagination-next"><a href="/page/2/">ë‹¤ìŒ</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/6/">6</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/eagle705-logo.png" alt="Joosung Yoon"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joosung Yoon</p><p class="is-size-6 is-block">Machine Learning Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>There and Back Again</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">í¬ìŠ¤íŠ¸</p><a href="/archives"><p class="title">51</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">ì¹´í…Œê³ ë¦¬</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">íƒœê·¸</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/eagle705" target="_blank" rel="noopener">íŒ”ë¡œìš°</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/eagle705"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hisdevelopers"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JSYoon53859120"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/joosung-yoon/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">ì¹´í…Œê³ ë¦¬</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cslog/"><span class="level-start"><span class="level-item">cslog</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/categories/photo/"><span class="level-start"><span class="level-item">photo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">ì•„ì¹´ì´ë¸Œ</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">3ì›” 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">2ì›” 2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">1ì›” 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">12ì›” 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">11ì›” 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">10ì›” 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">9ì›” 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">8ì›” 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7ì›” 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6ì›” 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">5ì›” 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1ì›” 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">12ì›” 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11ì›” 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10ì›” 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6ì›” 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5ì›” 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2ì›” 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">12ì›” 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">11ì›” 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">10ì›” 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">9ì›” 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">8ì›” 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">7ì›” 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">5ì›” 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">4ì›” 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">11ì›” 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">6ì›” 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">5ì›” 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">4ì›” 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">íƒœê·¸</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/drone/"><span class="tag">drone</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/grpc/"><span class="tag">grpc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">40</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp-kb/"><span class="tag">nlp, kb</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"><span class="tag">ìƒê°ì •ë¦¬</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">ê´‘ê³ </h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-2655870716902046" data-ad-slot="2764253071" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">ìµœê·¼ ê¸€</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-23T08:14:37.000Z">2023-03-23</time></p><p class="title"><a href="/Alpaca/">Alpaca (A Strong Instruction-Following Model)</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-20T13:07:50.000Z">2023-02-20</time></p><p class="title"><a href="/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/">SentencePieceë¥¼ í™œìš©í•œ íš¨ê³¼ì ì¸ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ë§Œë“¤ê¸°</a></p><p class="categories"><a href="/categories/ML/">ML</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-16T08:24:12.000Z">2023-02-16</time></p><p class="title"><a href="/Toolformer/">Toolformer: Language Models Can Teach Themselves to Use Tools</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-13T04:18:48.000Z">2023-02-13</time></p><p class="title"><a href="/SELF-INSTRUCT%20Aligning%20Language%20Model%20with%20Self%20Generated%20Instructions/">SELF-INSTRUCT Aligning Language Model with Self Generated Instructions</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-06T04:15:12.000Z">2023-02-06</time></p><p class="title"><a href="/(FLAN)%20Finetuned%20Language%20Models%20Are%20Zero-Shot%20Learners/">(FLAN) Finetuned Language Models Are Zero-Shot Learners</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 Joosung Yoon</span>Â Â Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>Â &amp;Â <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="ë§¨ ìœ„ë¡œ" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "ì´ ì›¹ ì‚¬ì´íŠ¸ëŠ” ê·€í•˜ì˜ ê²½í—˜ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ Cookieë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
          dismiss: "ë¬´ì‹œ",
          allow: "í—ˆìš©",
          deny: "ê±°ë¶€",
          link: "ë” ì•Œì•„ë³´ê¸°",
          policy: "Cookie ì •ì±…",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="ì…ë ¥ í•˜ì„¸ìš”..."></div><a class="searchbox-close" href="javascript:;">Ã—</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"ì…ë ¥ í•˜ì„¸ìš”...","untitled":"(ì œëª© ì—†ìŒ)","posts":"í¬ìŠ¤íŠ¸","pages":"í˜ì´ì§€","categories":"ì¹´í…Œê³ ë¦¬","tags":"íƒœê·¸"});
        });</script></body></html>