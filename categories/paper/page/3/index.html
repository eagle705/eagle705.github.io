<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>카테고리: paper - Luke&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Luke&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Luke&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Luke&#039;s Blog"><meta property="og:url" content="https://eagle705.github.io/"><meta property="og:site_name" content="Luke&#039;s Blog"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://eagle705.github.io/img/og_image.png"><meta property="article:author" content="Joosung Yoon"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://eagle705.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://eagle705.github.io"},"headline":"Luke's Blog","image":["https://eagle705.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Joosung Yoon"},"publisher":{"@type":"Organization","name":"Luke's Blog","logo":{"@type":"ImageObject","url":"https://eagle705.github.io/img/eagle705-logo.png"}},"description":null}</script><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-110980734-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-110980734-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-2655870716902046" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">카테고리</a></li><li class="is-active"><a href="#" aria-current="page">paper</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-11-11T03:00:00.000Z" title="2019. 11. 11. 오후 12:00:00">2019-11-11</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:52:46.434Z" title="2022. 8. 28. 오전 12:52:46">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">33분안에 읽기 (약 5015 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019-11-11-Albert/">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></h1><div class="content"><h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자:<ul>
<li>Zhenzhong Lan, Sebastian Goodman, Piyush Sharma Radu Soricut (<strong>Google Research</strong>)</li>
<li>Mingda Chen,  Kevin Gimpel  (<strong>Toyota Technological Institute at Chicago</strong>)</li>
</ul>
</li>
</ul>
<h3 id="Who-is-an-Author"><a href="#Who-is-an-Author" class="headerlink" title="Who is an Author?"></a>Who is an Author?</h3><ul>
<li>원래는 CV를 위주로 하던 친구인데 이번에 NLP꺼도 해본듯 (CVPR도 들고 있고..)</li>
<li>논문 인용수도 꽤 됨</li>
<li>Google VR팀에서도 인턴했었음<br><img src="/img/markdown-img-paste-2019111214553733.png">{: height&#x3D;”50%” width&#x3D;”50%”}<br><a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~lanzhzh/">http://www.cs.cmu.edu/~lanzhzh/</a></li>
</ul>
<h4 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h4><ul>
<li>간단한 아이디어인데 실험을 엄청 많이 해놔서 paper를 만든느낌</li>
<li>실험이 의미는 있지만 직관적으로 예측가능한 결과임</li>
<li>간단한 아이디어도 사실 예전부터 적용되어야 했음 (weight sharing, decomposition)</li>
<li>transformer 논문이 처음에 pretraining용이 아니다보니 당시 그 논문에서 빼먹었지만 당연히 앞으론 적용되었어야할 아이디어가 2년이 지나서야 적용된 느낌</li>
<li>SOP가 NSP보단 Good이다</li>
<li>SOP 할때 문장 단위가 아니라 textual segments로 한거 괜찮았음 (SEP도 그러면 segment단위로 넣겠네)</li>
<li>MLM 을 n-gram masking 한건 좀 신기하네 나쁘지 않음</li>
<li>transformer에서 dropout을 없애는게 pretraining할 때 진짜 좋은지는 좀 더 검증해봐야할 듯</li>
<li>이 논문은 모델 그림이 없다(?)</li>
</ul>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4></div><a class="article-more button is-small is-size-7" href="/2019-11-11-Albert/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-14T03:00:00.000Z" title="2019. 10. 14. 오후 12:00:00">2019-10-14</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:52:49.837Z" title="2022. 8. 28. 오전 12:52:49">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">25분안에 읽기 (약 3760 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019-10-14-MassivelyMultilingualSentenceEmbeddigns/">Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</a></h1><div class="content"><h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자:<ul>
<li>Mikel Artetxe (<strong>University of the Basque Country (UPV&#x2F;EHU)</strong>)</li>
<li>Holger Schwenk (<strong>Facebook AI Research</strong>)</li>
</ul>
</li>
</ul>
<h3 id="Who-is-an-Author"><a href="#Who-is-an-Author" class="headerlink" title="Who is an Author?"></a>Who is an Author?</h3><p>Mikel Artetxe 라는 친구인데 주로 번역쪽 태스크를 많이 한 것 같고 조경현 교수님하고도 co-author 이력이 있음. 페북에서 인턴할때 쓴 논문임.</p>
<p><img src="/img/markdown-img-paste-20191014145614237.png" alt="author">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h4 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h4><ul>
<li>결국 이 논문도 parallel corpus가 필요하다고함. 이걸 통해 multilingual sentence embedding을 얻는 것임</li>
<li>Translation이 되게 학습시켜서 encoder를 훈련함</li>
<li>대신에 그 양이 좀 적어도 다양한 언어에 대해서 얻을 수 있게 하는 것</li>
<li>영어로만 transfer learning 시켰는데도 다른언어도 적용된다는 점은 의미있음</li>
<li>encoder가 BPE를 통해 language independent하게 모델링했다는게 좀 의미가 있긴한데 한편으로는 universal한 구조다보니 좀 개별언어에 대해서 성능이 최적화되진 않겠다는 생각(<del>이지만 논문에선 결과가 괜찮음</del>)</li>
<li>language ID로 decoder에 언어정보를 주는건 꽤 괜찮은 아이디어였다고 생각</li>
<li>parallel corpus alignment하는거 어떻게하니.. 고생이 눈에 훤함 (꼭 다할 필요가 없다고 했지만서도)</li>
<li>이번 논문은 약간 Scaling 으로 승부한 케이스인것 같음 (제목 자체가 그렇지만)</li>
<li>Scaling을 키워서 실험할 줄 아는것도 결국 연구자의 역량..이라면 인프라가 중요하고 인프라가 중요하다면 configuration 잘하는건 기본이고, <del>실험비가 많거나 회사가 좋아야(?) 너무 스케일 싸움으로 가는것 같은 논문을 보면 왠지 모르게 아쉽고 씁쓸하다(?)</del></li>
<li>보통 transfer랑 one-shot, few-shot 등의 용어가 나오는데 fine-tune 안한다고해서 zero-shot이라고 한듯</li>
<li><code>Language-Agnostic</code> 라는 용어: 언어에 구애받지 않는다라는 뜻</li>
<li>BERT 등 최신 논문과도 비교했지만(<del>1년이 지났으니 최신이라고 이제 할수있을지..</del>) 본 논문의 기법 자체는 좀 옛날 기법이라는 생각이 듬</li>
<li><del>논문의 설명이 잘나와있으나 몇가지 좀 생략되어있음 (은근 불친절한)</del></li>
</ul></div><a class="article-more button is-small is-size-7" href="/2019-10-14-MassivelyMultilingualSentenceEmbeddigns/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-11T03:00:00.000Z" title="2019. 10. 11. 오후 12:00:00">2019-10-11</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:53:01.316Z" title="2022. 8. 28. 오전 12:53:01">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">10분안에 읽기 (약 1447 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019-10-11-UniversalLMFinetuneforTextClf/">Universal Language Model Fine-tuning for Text Classification (ULMFiT)</a></h1><div class="content"><h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자:Jeremy Howard, Sebastian Ruder (<strong>fast.ai</strong> University of San Francisco)</li>
</ul>
<h3 id="Who-is-an-Author"><a href="#Who-is-an-Author" class="headerlink" title="Who is an Author?"></a>Who is an Author?</h3><ul>
<li>Google Scholar에 안나와서..</li>
<li><a target="_blank" rel="noopener" href="https://twitter.com/jeremyphoward">Author’s Twitter</a></li>
</ul>
<h4 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h4><ul>
<li>pretrained model을 범용적으로 쓰려고 시도하려는 시기의 초기 논문인것 같다</li>
<li>저자가 어필을 되게 많이 하는 듯</li>
<li>각 레이어마다 feature가 다르니 다르게 finetune시켜줘야한다는 아이디어가 검증하긴 좀 어렵지만 직관적으론 꽤 설득력있었음. 한편으론 꼭 그래야되나 싶긴하면서도 나쁘지 않았던?</li>
<li>warm up등 테크닉이 여기서부터 점점 변형되면서 제안되는 듯</li>
</ul>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4></div><a class="article-more button is-small is-size-7" href="/2019-10-11-UniversalLMFinetuneforTextClf/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-08T03:00:00.000Z" title="2019. 10. 8. 오후 12:00:00">2019-10-08</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:53:04.246Z" title="2022. 8. 28. 오전 12:53:04">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">12분안에 읽기 (약 1731 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019-10-08-SAN_for_NLI/">Stochastic Answer Networks for Natural Language Inference (SAN)</a></h1><div class="content"><h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자:Xiaodong Liu†, Kevin Duh and Jianfeng Gao (<strong>Microsoft Research, Johns Hopkins University</strong>)</li>
</ul>
<h3 id="Who-is-an-Author"><a href="#Who-is-an-Author" class="headerlink" title="Who is an Author?"></a>Who is an Author?</h3><p>Xiaodong Liu 라는 친구인데 꽤 꾸준히 연구활동을 하는 친구인것 같다.</p>
<p><img src="/img/markdown-img-paste-20191007162823137.png" alt="author">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h4 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h4><ul>
<li>turn의 정보를 반영하기에 attention은 필수</li>
<li>하지만 5턴 이상 반영하는건 쉬운게 아님(여기서도 10개까지 했지만 5~6개가 best라고 했음)</li>
<li>multi turn을 위한 architecture를 pretrained model를 feature extractor로 써서 결합해서 쓰는게 앞으로의 연구 트렌드가 될 듯</li>
</ul></div><a class="article-more button is-small is-size-7" href="/2019-10-08-SAN_for_NLI/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-08-14T03:00:00.000Z" title="2019. 8. 14. 오후 12:00:00">2019-08-14</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:53:13.173Z" title="2022. 8. 28. 오전 12:53:13">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">16분안에 읽기 (약 2336 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019-08-14-GPT1/">Improving Language Understanding by Generative Pre-Training (GPT)</a></h1><div class="content"><h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자:Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever (<strong>Open AI</strong>, <del>Open AI다! 부럽(?)다</del>)</li>
</ul>
<h3 id="Who-is-an-Author"><a href="#Who-is-an-Author" class="headerlink" title="Who is an Author?"></a>Who is an Author?</h3><p>Alec Radford라는 친군데, GPT논문 인용수가 젤 많겠지 했는데 오히려 Vision쪽에서 한 Generative model 인용수가 넘사임.. 원래 유명한 친구였음</p>
<p><img src="/img/markdown-img-paste-20190814104818828.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h4 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h4><ul>
<li>작은 변화가 큰 성능의 변화를 가져다줌<ul>
<li>Add auxiliary objective</li>
<li>pre-training LM</li>
</ul>
</li>
</ul></div><a class="article-more button is-small is-size-7" href="/2019-08-14-GPT1/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-05-16T03:00:00.000Z" title="2019. 5. 16. 오후 12:00:00">2019-05-16</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:53:22.543Z" title="2022. 8. 28. 오전 12:53:22">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">16분안에 읽기 (약 2437 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019-05-16-SentencePiece/">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a></h1><div class="content"><h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자:Taku Kudo, John Richardson (Google, Inc)</li>
<li>EMNLP 2018</li>
<li>Official Repo: <a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">https://github.com/google/sentencepiece</a></li>
<li>Recommended Tutorial: <a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb">https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb</a></li>
</ul>
<h3 id="Who-is-an-Author"><a href="#Who-is-an-Author" class="headerlink" title="Who is an Author?"></a>Who is an Author?</h3><p><img src="/img/markdown-img-paste-20190516010154436.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h4 id="장점"><a href="#장점" class="headerlink" title="장점"></a>장점</h4><ul>
<li>언어에 상관없이 적용 가능</li>
<li>OOV 대처 가능</li>
<li>적은 vocab size로 높은 성능기록</li>
<li>빠름</li>
</ul>
<h4 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h4></div><a class="article-more button is-small is-size-7" href="/2019-05-16-SentencePiece/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-05-09T03:00:00.000Z" title="2019. 5. 9. 오후 12:00:00">2019-05-09</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:53:27.015Z" title="2022. 8. 28. 오전 12:53:27">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">30분안에 읽기 (약 4430 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019-05-09-BERT/">BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding</a></h1><div class="content"><h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (<strong>Google AI Language</strong>, <del>Google AI니 말다했지</del>)</li>
</ul>
<h3 id="Who-is-an-Author"><a href="#Who-is-an-Author" class="headerlink" title="Who is an Author?"></a>Who is an Author?</h3><p>Jacob Devlin is a Senior Research Scientist at Google. At Google, his primary research interest is developing fast, powerful, and scalable deep learning models for information retrieval, question answering, and other language understanding tasks. From 2014 to 2017, he worked as a Principle Research Scientist at Microsoft Research, where he led Microsoft Translate’s transition from phrase-based translation to neural machine translation (NMT). He also developed state-of-the-art on-device models for mobile NMT. Mr. Devlin was the recipient of the ACL 2014 Best Long Paper award and the NAACL 2012 Best Short Paper award. He received his Master’s in Computer Science from the University of Maryland in 2009, advised by Dr. Bonnie Dorr.</p>
<p><img src="/img/markdown-img-paste-20190509155558988.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h4 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h4><ul>
<li>Masking 기반의 Language Model과 context 추출을 위한 문장 연관성 (NSP) Task를 동시에 학습시켜서 Rich representation을 얻는다는 아이디어가 참신했음. 두마리 토끼를 한번에..!</li>
<li>Bidirectional feature가 상당히 중요함</li>
<li>pre-train 중요함</li>
<li>NSP도 매우 중요함</li>
<li>여기서도 Loss Masking이 중요함</li>
<li>CLS Loss와 LM Loss를 따로 떼서 계산해야함</li>
<li>gelu, masking scheme 썼을때와 안썼을때 성능차이가 꽤 남</li>
<li>segment embedding 처리하는게 은근 귀찮음, 전처리 할때 아예 생성해버리는게 편하긴함</li>
<li>CLS acc 올리기보다 LM acc 올리는게 더 쉬움</li>
</ul></div><a class="article-more button is-small is-size-7" href="/2019-05-09-BERT/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-05-01T03:00:00.000Z" title="2019. 5. 1. 오후 12:00:00">2019-05-01</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:53:34.980Z" title="2022. 8. 28. 오전 12:53:34">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">29분안에 읽기 (약 4423 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019-05-01-Attention_is_All_you_need/">Attention Is All You Need</a></h1><div class="content"><p>이번엔 오늘날의 NLP계의 표준이 된 Transformer를 제안한 논문인 <code>Attenion Is All You Need</code>에 대해서 리뷰해보고자 한다. 대략적인 내용은 이미 알고 있었지만, 디테일한 부분도 살펴보고자 한다.</p>
<h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자: Ashish Vaswani, 외 7명 (Google Brain)</li>
<li><del>구글브레인..wow</del></li>
<li>NIPS 2017 accepted</li>
</ul>
<h3 id="Who-is-an-Author"><a href="#Who-is-an-Author" class="headerlink" title="Who is an Author?"></a>Who is an Author?</h3><p><img src="/img/markdown-img-paste-20190501115722701.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h4 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h4><ul>
<li>Multi-Head-Attention을 빠르게 구현하기 위해 Matrix 연산으로 처리하되, Embedding Tensor를 쪼갠 후 합치는게 아닌 reshape &amp; transpose operator로 shape을 변경 후 한꺼번에 행렬곱으로 계산해서 다시 reshape 함으로써 병렬처리가 가능하게끔 구현하는게 인상적이었음</li>
<li>행렬곱할 때 weight 와 곱하는건 weight variable 생성 후 MatMul 하는게 아니라 그냥 다 Dense로 처리하는게 구현 팁이구나 느꼈음</li>
<li>요약: 쪼갠다음에 weight 선언 후 매트릭스 곱? No! -&gt; 쪼갠 다음에 Dense! -&gt; 쪼개면 for loop 때문에 병렬처리 안되잖아! -&gt; 다 계산후에 쪼개자!</li>
<li>Attention만 구현하면 얼추 끝날 줄 알았는데 Masking 지분이 70~80%였음<ul>
<li>Masking은 logical 연산 (boolean)으로 padding 체크해서 하는게 일반적임</li>
<li>Masking은 input에도 해주고 loss에도 해줌</li>
<li>마스킹 적용할땐 broadcasting 기법을 써서 하는게 일반적임</li>
<li>아래의 두 경우 모두 가능함<ul>
<li>ex) (40, 5, 10, 10) + (40, 1, 1, 10) &#x3D;&#x3D; (batch, head, seq, seq)</li>
<li>ex) (40, 5, 10, 10) + (40, 1, 10, 10) &#x3D;&#x3D; (batch, head, seq, seq)</li>
</ul>
</li>
</ul>
</li>
</ul></div><a class="article-more button is-small is-size-7" href="/2019-05-01-Attention_is_All_you_need/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-04-12T03:00:00.000Z" title="2019. 4. 12. 오후 12:00:00">2019-04-12</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:53:37.562Z" title="2022. 8. 28. 오전 12:53:37">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">18분안에 읽기 (약 2730 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019-04-12-Neural_Machine_Translation_in_Linear_Time/">Neural Machine Translation in Linear Time</a></h1><div class="content"><h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자:Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, Koray Kavukcuoglu (Google Deepmind, London UK)   </li>
<li>딥마인드 (<del>말 다했다</del>)</li>
</ul>
<h3 id="Who-is-an-Author"><a href="#Who-is-an-Author" class="headerlink" title="Who is an Author?"></a>Who is an Author?</h3><p><img src="https://eagle705.github.io/img/markdown-img-paste-20190417203106628.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul>
<li>이 당시엔 Novel architecture였다 (2016년, 후에 <code>Attention is all you need</code> 논문에서도 인용함)</li>
<li><code>ByteNet</code>이라고 부름</li>
</ul>
<p><img src="https://eagle705.github.io/img/markdown-img-paste-20190412152730173.png">{: height&#x3D;”50%” width&#x3D;”50%”}</p></div><a class="article-more button is-small is-size-7" href="/2019-04-12-Neural_Machine_Translation_in_Linear_Time/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2018-11-06T03:00:00.000Z" title="2018. 11. 6. 오후 12:00:00">2018-11-06</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-27T15:53:44.382Z" title="2022. 8. 28. 오전 12:53:44">2022-08-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">5분안에 읽기 (약 772 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2018-11-06-ELMo/">Deep contextualized word representations(ELMo)</a></h1><div class="content"><p>요즘 Transformer가 나오면서 NLP관련 모델의 성능이 크게 증가했다.<del>요즘 시대에 그냥 CNN이나 LSTM쓰면 옛날 사람 취급받을 것만 같은..</del> 또 하나의 breakthrough라고도 할 수 있을 것 같다. Word Representation쪽에서도 비슷한 도약이 있었는데, 그 시작이 ELMo다. 처음엔 그냥 성능을 약간 올려주는 모델인가보다 하고 넘어갔지만, 다양한 연구에서 활용되는 것을 보면서 이를 기반으로 현재는 Bert와 같은 모델도 나오게 되었다. 이젠 안볼수없는 개념이 되었기에, 논문을 통해 다시 한번 정리해보고자 한다.</p>
<h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,Christopher Clark, Kenton Lee∗, Luke Zettlemoyer</li>
</ul>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul>
<li>Syntax &amp; semantics 정보를 잡아냄</li>
<li>언어적 문맥(linguistic contexts)도 고려함</li>
<li>word vectors는 internal states에 대한 deep bidirectional language model(biLM)를 통해 유도됨</li>
<li>이러한 representation은 기존 모델에도 잘 붙여서 쓸수 있고 성능도 많이 올랐음(6개의 도전적인 NLP task에서 SOTA!! 기록; QA, SA 등등)</li>
</ul>
<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><ul>
<li>word2vec등이 NLP에서 Key component였지만 양질의 표현을 얻기는 도전적인 문제가 있었음</li>
<li>syntax &amp; semantics와 linguistic contexts를 모델링해주는게 이상적이라 할 수 있음</li>
<li>이 관점으로 새로운 representation을 제안 하겠음</li>
<li><strong>Vector를 LM과 함께 학습된 BiLSTM으로부터 얻어낼 것임</strong></li>
<li>이러한 이유로, ELMo(Embeddings from Language Models) representation이라 칭할 것임</li>
<li></li>
<li>internal states를 조합해주는건 매우 풍부한 word representation을 줌</li>
<li>higher-level LSTM states는 context-dependent aspects of word meaning을 잘 캡쳐함</li>
<li>lower-level states는 aspects of syntax(e.g. POS)를 잘함</li>
</ul></div><a class="article-more button is-small is-size-7" href="/2018-11-06-ELMo/#more">자세히 보기</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/paper/page/2/">이전</a></div><div class="pagination-next is-invisible is-hidden-mobile"><a href="/categories/paper/page/4/">다음</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/paper/">1</a></li><li><a class="pagination-link" href="/categories/paper/page/2/">2</a></li><li><a class="pagination-link is-current" href="/categories/paper/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/eagle705-logo.png" alt="Joosung Yoon"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joosung Yoon</p><p class="is-size-6 is-block">Machine Learning Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>There and Back Again</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">44</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/eagle705" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/eagle705"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hisdevelopers"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JSYoon53859120"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/joosung-yoon/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/cslog/"><span class="level-start"><span class="level-item">cslog</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">30</span></span></a></li><li><a class="level is-mobile" href="/categories/photo/"><span class="level-start"><span class="level-item">photo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">1월 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">12월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">11월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">10월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">9월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">8월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">5월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">12월 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">12월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">11월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">10월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">9월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">8월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">7월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">5월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">4월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">11월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">6월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">5월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">4월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/drone/"><span class="tag">drone</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/grpc/"><span class="tag">grpc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">33</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp-kb/"><span class="tag">nlp, kb</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"><span class="tag">생각정리</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">광고</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-2655870716902046" data-ad-slot="2764253071" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-01-09T07:34:18.000Z">2023-01-09</time></p><p class="title"><a href="/Chinchilla-Training-Compute-Optimal-Large-Language-Models/">(Chinchilla) Training Compute-Optimal Large Language Models</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-12-12T06:45:59.000Z">2022-12-12</time></p><p class="title"><a href="/Robust-Conversational-Agents-against-Imperceptible-Toxicity-Triggers/">Robust Conversational Agents against Imperceptible Toxicity Triggers</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-11-02T06:27:15.000Z">2022-11-02</time></p><p class="title"><a href="/SOCIAL-CHEMISTRY-101/">SOCIAL CHEMISTRY 101 - Learning to Reason about Social and Moral Norms</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-05T07:16:45.000Z">2022-10-05</time></p><p class="title"><a href="/A-Contrastive-Framework-for-Neural-Text-Generation-NeurIPS-2022/">A Contrastive Framework for Neural Text Generation (NeurIPS 2022)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-09-20T05:18:10.000Z">2022-09-20</time></p><p class="title"><a href="/Learning-rate-warmup-scheduling/">Learning rate &amp; warmup step &amp; LR scheduling</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 Joosung Yoon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>