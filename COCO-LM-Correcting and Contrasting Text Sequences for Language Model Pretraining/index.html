<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining - Luke&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Luke&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Luke&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="논문파일 COCO-LM- Correcting and Contrasting Text Sequences for Language Model Pretraining.pdf  Ref github: https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;microsoft&amp;#x2F;COCO-LM 발표슬라이드: COCO_LM_220622.pdf  Author 저자: Yu Meng1∗, Chenyan X"><meta property="og:type" content="blog"><meta property="og:title" content="COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining"><meta property="og:url" content="https://eagle705.github.io/COCO-LM-Correcting%20and%20Contrasting%20Text%20Sequences%20for%20Language%20Model%20Pretraining/"><meta property="og:site_name" content="Luke&#039;s Blog"><meta property="og:description" content="논문파일 COCO-LM- Correcting and Contrasting Text Sequences for Language Model Pretraining.pdf  Ref github: https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;microsoft&amp;#x2F;COCO-LM 발표슬라이드: COCO_LM_220622.pdf  Author 저자: Yu Meng1∗, Chenyan X"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174862541-c622119c-fbbc-451e-a28e-b6d3533614d3.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174528476-33742ff9-6eef-492e-b2b9-74ee65587b2f.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174524483-f3da6248-8a99-4bdd-85c6-4930dbb60997.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174525203-78f1e8e7-6ffa-4d5d-ba80-f8fc0c611b91.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174526168-e668b74e-3d6b-49b6-9970-ceece3139ccb.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174528476-33742ff9-6eef-492e-b2b9-74ee65587b2f.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174529064-894885ca-524a-459a-a049-b93b15fccf2a.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174529359-12e490de-4b9a-46dd-9131-9d1bbf9dd9d3.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174543773-6995f2c6-0bfd-4b2c-9957-b0a1d804bee8.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174944631-ade96c02-96ff-49c9-ba28-747fabbb7e1f.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174546688-70025097-e106-4803-ab3c-36cd2f5a1776.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174546704-3e4a823f-4c06-4b4f-b822-0956357d9d75.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174547162-477cf8aa-626b-461c-968f-f94393212643.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174549284-2af5c4d4-8171-426e-af66-9949de2f4505.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174555297-094088d9-5de3-48db-b6b4-d04d8c052889.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174947724-8442f682-af96-4c76-84ac-001cfa92413c.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174556264-c75922d7-8196-4f05-97eb-3483e2b314e5.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174615775-2632203a-4f23-4178-872d-d82e7a618274.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174616974-fa24a5ee-7123-4b4a-a005-12943b75ffa4.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/174631039-71b4977c-02ef-498c-b577-cfd973ef0fc5.png"><meta property="article:published_time" content="2022-06-20T03:00:00.000Z"><meta property="article:modified_time" content="2022-08-30T04:33:46.299Z"><meta property="article:author" content="Joosung Yoon"><meta property="article:tag" content="nlp"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://user-images.githubusercontent.com/7252598/174862541-c622119c-fbbc-451e-a28e-b6d3533614d3.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://eagle705.github.io/COCO-LM-Correcting%20and%20Contrasting%20Text%20Sequences%20for%20Language%20Model%20Pretraining/"},"headline":"COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining","image":["https://user-images.githubusercontent.com/7252598/174862541-c622119c-fbbc-451e-a28e-b6d3533614d3.png","https://user-images.githubusercontent.com/7252598/174528476-33742ff9-6eef-492e-b2b9-74ee65587b2f.png","https://user-images.githubusercontent.com/7252598/174524483-f3da6248-8a99-4bdd-85c6-4930dbb60997.png","https://user-images.githubusercontent.com/7252598/174525203-78f1e8e7-6ffa-4d5d-ba80-f8fc0c611b91.png","https://user-images.githubusercontent.com/7252598/174526168-e668b74e-3d6b-49b6-9970-ceece3139ccb.png","https://user-images.githubusercontent.com/7252598/174528476-33742ff9-6eef-492e-b2b9-74ee65587b2f.png","https://user-images.githubusercontent.com/7252598/174529064-894885ca-524a-459a-a049-b93b15fccf2a.png","https://user-images.githubusercontent.com/7252598/174529359-12e490de-4b9a-46dd-9131-9d1bbf9dd9d3.png","https://user-images.githubusercontent.com/7252598/174543773-6995f2c6-0bfd-4b2c-9957-b0a1d804bee8.png","https://user-images.githubusercontent.com/7252598/174944631-ade96c02-96ff-49c9-ba28-747fabbb7e1f.png","https://user-images.githubusercontent.com/7252598/174546688-70025097-e106-4803-ab3c-36cd2f5a1776.png","https://user-images.githubusercontent.com/7252598/174546704-3e4a823f-4c06-4b4f-b822-0956357d9d75.png","https://user-images.githubusercontent.com/7252598/174547162-477cf8aa-626b-461c-968f-f94393212643.png","https://user-images.githubusercontent.com/7252598/174549284-2af5c4d4-8171-426e-af66-9949de2f4505.png","https://user-images.githubusercontent.com/7252598/174555297-094088d9-5de3-48db-b6b4-d04d8c052889.png","https://user-images.githubusercontent.com/7252598/174947724-8442f682-af96-4c76-84ac-001cfa92413c.png","https://user-images.githubusercontent.com/7252598/174556264-c75922d7-8196-4f05-97eb-3483e2b314e5.png","https://user-images.githubusercontent.com/7252598/174615775-2632203a-4f23-4178-872d-d82e7a618274.png","https://user-images.githubusercontent.com/7252598/174616974-fa24a5ee-7123-4b4a-a005-12943b75ffa4.png","https://user-images.githubusercontent.com/7252598/174631039-71b4977c-02ef-498c-b577-cfd973ef0fc5.png"],"datePublished":"2022-06-20T03:00:00.000Z","dateModified":"2022-08-30T04:33:46.299Z","author":{"@type":"Person","name":"Joosung Yoon"},"publisher":{"@type":"Organization","name":"Luke's Blog","logo":{"@type":"ImageObject","url":"https://eagle705.github.io/img/eagle705-logo.png"}},"description":"논문파일 COCO-LM- Correcting and Contrasting Text Sequences for Language Model Pretraining.pdf  Ref github: https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;COCO-LM 발표슬라이드: COCO_LM_220622.pdf  Author 저자: Yu Meng1∗, Chenyan X"}</script><link rel="canonical" href="https://eagle705.github.io/COCO-LM-Correcting%20and%20Contrasting%20Text%20Sequences%20for%20Language%20Model%20Pretraining/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-110980734-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-110980734-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-2655870716902046" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-06-20T03:00:00.000Z" title="6/20/2022, 12:00:00 PM">2022-06-20</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-08-30T04:33:46.299Z" title="8/30/2022, 1:33:46 PM">2022-08-30</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/paper/">paper</a></span><span class="level-item">15분안에 읽기 (약 2211 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining</h1><div class="content"><h2 id="논문파일"><a href="#논문파일" class="headerlink" title="논문파일"></a>논문파일</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/9043435/COCO-LM-.Correcting.and.Contrasting.Text.Sequences.for.Language.Model.Pretraining.pdf">COCO-LM- Correcting and Contrasting Text Sequences for Language Model Pretraining.pdf</a></li>
</ul>
<h2 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h2><ul>
<li>github: <a target="_blank" rel="noopener" href="https://github.com/microsoft/COCO-LM">https://github.com/microsoft/COCO-LM</a></li>
<li>발표슬라이드: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/9043434/COCO_LM_220622.pdf">COCO_LM_220622.pdf</a></li>
</ul>
<h2 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h2><ul>
<li>저자: Yu Meng1∗, Chenyan Xiong2, Payal Bajaj2, Saurabh Tiwary2, Paul Bennett2, Jiawei Han1, Xia Song2<br>1 University of Illinois at Urbana-Champaign 2 Microsoft<ul>
<li>NeurIPS 2021 논문<br><img src="https://user-images.githubusercontent.com/7252598/174862541-c622119c-fbbc-451e-a28e-b6d3533614d3.png" alt="image"></li>
</ul>
</li>
</ul>
<h2 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h2><ul>
<li>ELECTRA 개선 버전 논문<ul>
<li>논문 나온 순서는 대략 ELECTRA - COCO-LM - SimCSE 가 됨</li>
<li>RTD를 copy mechanism에 녹여서 multi-task learning으로 All-token MLM 사용</li>
</ul>
</li>
<li>sentence similarity를 PLM self-supervised learning 안에 추가함<ul>
<li>present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences<br><img src="https://user-images.githubusercontent.com/7252598/174528476-33742ff9-6eef-492e-b2b9-74ee65587b2f.png" alt="image"></li>
</ul>
</li>
</ul>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><ul>
<li>논문 순서는 대략 ELECTRA - COCO-LM - SimCSE 가 됨</li>
<li>약간 <a target="_blank" rel="noopener" href="https://blog.pingpong.us/electra-review/">ELECTRA 논문</a>의 연장선, 혹은 변형<ul>
<li>ELECTRA의 contribution -&gt; 15% 계산량 -&gt; 100%로 늘려서 효율 높임<ul>
<li>input token copy mechanism 실험</li>
</ul>
</li>
<li>[MASK] token 대신 generator가 생성한 토큰을 써서 [MASK] 토큰이 일으키는 pre-train&#x2F;fine-tune discrepancy 제거</li>
<li>Replaced Token Detection (RTD) 태스크 제안</li>
</ul>
</li>
</ul>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul>
<li>present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences</li>
<li><code>Following ELECTRA-style pretraining</code>, COCO-LM employs an auxiliary language model to corrupt text sequences, upon which it constructs two new tasks for pretraining the main model<ul>
<li>The first token-level task, Corrective Language Modeling, is to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics.</li>
<li>The second sequence-level task, Sequence Contrastive Learning, is to align text sequences originated from the same source input while ensuring uniformity in the representation space</li>
<li>achieves the MNLI accuracy of ELECTRA with <code>50% of its pretraining GPU hours</code>. With the same pretraining steps of standard base&#x2F;large-sized models, COCO-LM <code>outperforms the previous best models by 1+ GLUE average points</code>.</li>
</ul>
</li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>(무슨말이지..) ELECTRA, that uses an auxiliary language model (“generator”) to replace tokens in input texts and pretrains the main Transformer (“discriminator”) to detect replaced tokens. This improves the pretraining efficiency and effectiveness, but pretraining via binary classification hinders the model’s usage on applications requiring language modeling capability (e.g., prompt-based learning [15, 28, 46]). It could further distort the representation space as the Transformers are pretrained to output the same “non-replacement” label for all actual tokens.</li>
<li>present a new self-supervised learning approach, <code>COCO-LM</code>, that pretrains Lan guage Models by <code>COrrecting and COntrasting corrupted text sequences</code></li>
<li>Following ELECTRA-style pretraining, COCO-LM employs an auxiliary model to corrupt the input texts, upon which it introduces two new pretraining tasks for the main Transformer, one at token level and one at sequence level. </li>
<li>The token-level task, corrective language modeling (CLM), pretrains the main Transformer to detect and correct the tokens in the corrupted sequences. It uses a multi-task setup to combine the benefits of replaced token detection and language modeling. </li>
<li>The sequence-level task, sequence contrastive learning (SCL), pretrains the model to align text sequences originated from the same source sequence and enforce uniformity of the representation space</li>
<li>GLUE [54] and SQuAD [41] benchmarks, COCO-LM not only outperforms state-of-the-art pretraining approaches in effectiveness, but also significantly improves the pretraining efficiency</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul>
<li>Empirically, MLM is still among the most effective tasks to pretrain encoders</li>
<li>Instead of randomly altering texts, ELECTRA [7] uses a smaller auxiliary Transformer pretrained by MLM to replace some tokens in the text sequences using its language modeling probability, and pretrains the main Transformer to detect the replaced tokens. ELECTRA achieves state-of-the-art accuracy in many language tasks [7]. Later, Clark et el. [6] developed ELECTRIC, which pretrains encoders by contrasting original tokens against negatives sampled from a cloze model. ELECTRIC re-enables the language modeling capability but underperforms ELECTRA in downstream tasks.</li>
<li>Our work is also related to contrastive learning which has shown great success in visual representation learning [4, 22, 34]. Its effectiveness of in language is more observed in the fine-tuning stage, for example, in sentence representation [16], dense retrieval [60], and GLUE fine-tuning [19].</li>
</ul>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><ul>
<li>We present the preliminaries of PLMs, their challenges, and the new COCO-LM framework.</li>
</ul>
<h3 id="Preliminary-on-Language-Model-Pretraining"><a href="#Preliminary-on-Language-Model-Pretraining" class="headerlink" title="Preliminary on Language Model Pretraining"></a>Preliminary on Language Model Pretraining</h3><ul>
<li>In this work we focus on pretraining BERT-style bidirectional Transformer encoders</li>
<li>first recap the masked language modeling (MLM) task introduced by BERT [11] and then discuss the pretraining framework of ELECTRA</li>
</ul>
<h4 id="BERT-Pretraining"><a href="#BERT-Pretraining" class="headerlink" title="BERT Pretraining"></a>BERT Pretraining</h4><p><img src="https://user-images.githubusercontent.com/7252598/174524483-f3da6248-8a99-4bdd-85c6-4930dbb60997.png" alt="image"></p>
<h4 id="ELECTRA-Pretraining"><a href="#ELECTRA-Pretraining" class="headerlink" title="ELECTRA Pretraining"></a>ELECTRA Pretraining</h4><p><img src="https://user-images.githubusercontent.com/7252598/174525203-78f1e8e7-6ffa-4d5d-ba80-f8fc0c611b91.png" alt="image"></p>
<h3 id="Challenges-of-ELECTRA-Style-Pretraining"><a href="#Challenges-of-ELECTRA-Style-Pretraining" class="headerlink" title="Challenges of ELECTRA-Style Pretraining"></a>Challenges of ELECTRA-Style Pretraining</h3><ul>
<li>Missing Language Modeling Benefits.<ul>
<li>classification task in ELECTRA is simpler and more stable [61], but raises two challenges.<ul>
<li>first is the <strong>lack of language modeling capability</strong> which is a necessity in some tasks [6]. For example, prompt-based learning requires a language model to generate labels</li>
<li>second is that the binary classification task <strong>may not be sufficient to capture certain word-level semantics</strong> that are critical for token-level tasks</li>
</ul>
</li>
</ul>
</li>
<li>Squeezing Representation Space<ul>
<li>the representations from Transformer-based language models often reside in a narrow cone, where two random sentences have high similarity scores (lack of uniformity)</li>
<li>closely related sentences may have more different representations (lack of alignment)</li>
<li>Figure 1 illustrates such behaviors with random sentence pairs (from pretraining corpus) and semantically similar pairs (those annotated with maximum similarity from STS-B [3]). With RoBERTa, the cosine similarities of most random sentence pairs are near 0.8, bigger than many semantically similar pairs. The representation space from ELECTRA is even more squeezed. Nearly all sentence pairs, both random and similar ones, have around 0.9 cosine similarity. This may not be surprising as ELECTRA is pretrained to predict the same output (“non-replacement”) for all tokens in these sequences. The irregular representation space raises the risk of degeneration [37, 55] and often necessitates sophisticated post-adjustment or fine-tuning to improve the sequence representations [16, 30, 32, 60].<br><img src="https://user-images.githubusercontent.com/7252598/174526168-e668b74e-3d6b-49b6-9970-ceece3139ccb.png" alt="image"></li>
</ul>
</li>
</ul>
<h3 id="COCO-LM-Pretraining"><a href="#COCO-LM-Pretraining" class="headerlink" title="COCO-LM Pretraining"></a>COCO-LM Pretraining</h3><ul>
<li>The auxiliary Transformer is pretrained by masked language modeling (MLM) and generates corrupted sequences. </li>
<li>The main Transformer is pretrained to correct the corruption (CLM) and to contrast the corrupted sequences with the cropped sequences (SCL)<br><img src="https://user-images.githubusercontent.com/7252598/174528476-33742ff9-6eef-492e-b2b9-74ee65587b2f.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/174529064-894885ca-524a-459a-a049-b93b15fccf2a.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/174529359-12e490de-4b9a-46dd-9131-9d1bbf9dd9d3.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/174543773-6995f2c6-0bfd-4b2c-9957-b0a1d804bee8.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/174944631-ade96c02-96ff-49c9-ba28-747fabbb7e1f.png" alt="image"></li>
</ul>
<h4 id="Network-Configurations"><a href="#Network-Configurations" class="headerlink" title="Network Configurations"></a>Network Configurations</h4><ul>
<li>auxiliary model<ul>
<li>Similar to ELECTRA, the auxiliary Transformer is smaller than the main model</li>
<li>We reduce the number of layers to 1&#x2F;3 or 1&#x2F;4 (under base or large model setup, respectively) but keep its hidden dimension the same with the main model, instead of shrinking its hidden dimensions</li>
<li>We disable dropout in it when sampling replacement tokens.</li>
</ul>
</li>
<li>main model<ul>
<li>standard architecture of BERT&#x2F;ELECTRA</li>
</ul>
</li>
</ul>
<h2 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h2><ul>
<li>Pretraining Settings<ul>
<li>base, base++, and large++. Base is the BERTBase training configuration [11]: Pretraining on Wikipedia and BookCorpus [63] (16 GB of texts) for 256 million samples on 512 token sequences</li>
<li>32, 768 uncased BPE vocabulary</li>
</ul>
</li>
<li>Model Architecture<ul>
<li>base&#x2F;base++ model uses the BERT Base architecture [11]: 12 layer Transformer, 768 hidden size, plus <strong>T5 relative position encoding</strong>. </li>
<li>large++ model is the same with BERTLarge, 24 layer and 1024 hidden size, plus T5 relative position encoding</li>
<li>auxiliary network uses the same hidden size but a shallow <strong>4-layer Transformer</strong> in base&#x2F;base++ and a <strong>6-layer</strong> one in large++. When generating XMLM we disable dropout in the auxiliary model</li>
</ul>
</li>
<li>Downstream Tasks<ul>
<li>GLUE [54] and SQuAD 2.0</li>
<li>Standard hyperparameter search in fine-tuning is performed, and the search space can be found in Appendix B.</li>
<li><strong>reported results are the median of five random seeds</strong> on GLUE and SQuAD</li>
</ul>
</li>
</ul>
<h2 id="Evaluation-Results"><a href="#Evaluation-Results" class="headerlink" title="Evaluation Results"></a>Evaluation Results</h2><ul>
<li>COCO-LM outperforms all recent state-of-the-art pretraining models on GLUE average and SQuAD<br><img src="https://user-images.githubusercontent.com/7252598/174546688-70025097-e106-4803-ab3c-36cd2f5a1776.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/174546704-3e4a823f-4c06-4b4f-b822-0956357d9d75.png" alt="image"></li>
</ul>
<h3 id="Efficiency"><a href="#Efficiency" class="headerlink" title="Efficiency"></a>Efficiency</h3><ul>
<li>COCO-LM is more efficient in GPU hours. It outperforms RoBERTa &amp; ELECTRA by 1+ points<br><img src="https://user-images.githubusercontent.com/7252598/174547162-477cf8aa-626b-461c-968f-f94393212643.png" alt="image"></li>
</ul>
<h3 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h3><ul>
<li>base setting on GLUE DEV</li>
<li>예상과는 좀 다른..<br><img src="https://user-images.githubusercontent.com/7252598/174549284-2af5c4d4-8171-426e-af66-9949de2f4505.png" alt="image"></li>
</ul>
<h4 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture."></a>Architecture.</h4><ul>
<li>Removing relative position encoding (Rel-Pos) leads to better numbers on some tasks but significantly hurts MNLI.</li>
</ul>
<h4 id="Pretraining-Signal-Construction"><a href="#Pretraining-Signal-Construction" class="headerlink" title="Pretraining Signal Construction."></a>Pretraining Signal Construction.</h4><ul>
<li>Using randomly replaced tokens to corrupt text sequence hurts significantly. Using a converged auxiliary network to pretrain the main model also hurts. It is better to pretrain the two Transformers together</li>
</ul>
<h4 id="CLM-Setup"><a href="#CLM-Setup" class="headerlink" title="CLM Setup."></a>CLM Setup.</h4><ul>
<li>Disabling the multi-task learning and using All-Token MLM [7] reduces model accuracy.</li>
<li>The copy mechanism is effective. The benefits of the stop gradient operation are more on stability (preventing training divergence).</li>
</ul>
<h3 id="Analyses-of-Contrastive-Learning-with-SCL"><a href="#Analyses-of-Contrastive-Learning-with-SCL" class="headerlink" title="Analyses of Contrastive Learning with SCL"></a>Analyses of Contrastive Learning with SCL</h3><h4 id="Ablation-on-Data-Augmentation"><a href="#Ablation-on-Data-Augmentation" class="headerlink" title="Ablation on Data Augmentation"></a>Ablation on Data Augmentation</h4><p><img src="https://user-images.githubusercontent.com/7252598/174555297-094088d9-5de3-48db-b6b4-d04d8c052889.png" alt="image"></p>
<h4 id="Alignment-and-Uniformity"><a href="#Alignment-and-Uniformity" class="headerlink" title="Alignment and Uniformity"></a>Alignment and Uniformity</h4><ul>
<li>The representation space from COCO-LM is drastically different from those in Figure 1</li>
<li>With COCO-LM, similar pairs are more aligned and random pairs are distributed more uniformly</li>
<li>Their average cosine similarity is 0.925 when pretrained with SCL, while is 0.863 without SCL. This better alignment and uniformity is achieved by COCO-LM with SCL via pretraining</li>
</ul>
<h4 id="Regularizing-the-Representation-Learning-for-Better-Few-Shot-Ability"><a href="#Regularizing-the-Representation-Learning-for-Better-Few-Shot-Ability" class="headerlink" title="Regularizing the Representation Learning for Better Few-Shot Ability."></a>Regularizing the Representation Learning for Better Few-Shot Ability.</h4><ul>
<li>SCL is necessary to regularize the representation space and to reduce the risk of degeneration<br><img src="https://user-images.githubusercontent.com/7252598/174947724-8442f682-af96-4c76-84ac-001cfa92413c.png" alt="image"></li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/174556264-c75922d7-8196-4f05-97eb-3483e2b314e5.png" alt="image"></p>
<h3 id="Analyses-of-Language-Modeling-with-CLM"><a href="#Analyses-of-Language-Modeling-with-CLM" class="headerlink" title="Analyses of Language Modeling with CLM"></a>Analyses of Language Modeling with CLM</h3><p><img src="https://user-images.githubusercontent.com/7252598/174615775-2632203a-4f23-4178-872d-d82e7a618274.png" alt="image"></p>
<ul>
<li>CLM과 All-Token MLM 비교</li>
<li>It is quite an unbalanced task<ul>
<li>For the majority of the tokens (Original) the task is simply to copy its input at the same position.</li>
<li>For the replaced tokens (7 − 8% total), however, the model needs to detect the abnormality brought by the auxiliary model and recover the original token</li>
<li>Implicitly training the copy mechanism as part of the hard LM task is not effective: The copy accuracy of All-Token MLM is much lower, and thus the LM head may confuse original tokens with replaced ones<ul>
<li>As shown in Table 3 and ELECTRA [7], pretraining with All-Token MLM performs worse than using the RTD task, though the latter is equivalent to only training the copy mechanism</li>
<li>The multi-task learning of CLM is necessary for the main Transformer to stably learn the language modeling task upon the corrupted text sequence.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Prompt-Based-Fine-Tuning-with-CLM"><a href="#Prompt-Based-Fine-Tuning-with-CLM" class="headerlink" title="Prompt-Based Fine-Tuning with CLM"></a>Prompt-Based Fine-Tuning with CLM</h3><ul>
<li>the prompt-based fine-tuning experiments on MNLI for RoBERTa and COCO-LM under base++ and large++ sizes</li>
<li>COCO-LM’s main Transformer does not even see any<br>[MASK] tokens during pretraining but still performs well on predicting masked tokens for prompt-based learning.</li>
<li>Note that ELECTRA and COCO-LM variants without the CLM task are not applicable: Their main Transformers are not pretrained by language modeling tasks (thus no language modeling capability is learned to generate prompt label words).<br><img src="https://user-images.githubusercontent.com/7252598/174616974-fa24a5ee-7123-4b4a-a005-12943b75ffa4.png" alt="image"></li>
</ul>
<h2 id="Conclusion-and-Future-Work"><a href="#Conclusion-and-Future-Work" class="headerlink" title="Conclusion and Future Work"></a>Conclusion and Future Work</h2><ul>
<li>we present COCO-LM, which pretrains language models using Corrective Language Modeling and Sequence Contrastive Learning upon corrupted text sequences</li>
<li>With standard pre- training data and Transformer architectures, COCO-LM improves the accuracy on the GLUE and SQuAD benchmarks, while also being more efficient in utilizing pretraining computing resources and network parameters</li>
<li><strong>One limitation of this work is that the contrastive pairs are constructed by simple cropping and MLM replacements</strong></li>
<li>To better understand and tailor the training of the auxiliary model to the main model is another important future research direction</li>
</ul>
<h2 id="코드구현"><a href="#코드구현" class="headerlink" title="코드구현"></a>코드구현</h2><ul>
<li>loss 관련 코드 스니펫: <a target="_blank" rel="noopener" href="https://github.com/microsoft/COCO-LM/issues/2#issuecomment-1003639940">https://github.com/microsoft/COCO-LM/issues/2#issuecomment-1003639940</a></li>
<li>scl쪽 (span으로 한번 임베딩뽑고, src로도 한번 뽑고)<br><img src="https://user-images.githubusercontent.com/7252598/174631039-71b4977c-02ef-498c-b577-cfd973ef0fc5.png" alt="image"></li>
<li>위 코드 위치: <a target="_blank" rel="noopener" href="https://github.com/microsoft/COCO-LM/blob/6bb6e5f62d65349657dd51f2f535454a1c50c2e9/fairseq/fairseq/models/cocolm/model.py#L190">https://github.com/microsoft/COCO-LM/blob/6bb6e5f62d65349657dd51f2f535454a1c50c2e9/fairseq/fairseq/models/cocolm/model.py#L190</a></li>
<li>unofficial implementation: <a target="_blank" rel="noopener" href="https://github.com/lucidrains/coco-lm-pytorch/blob/main/coco_lm_pytorch/coco_lm_pytorch.py">https://github.com/lucidrains/coco-lm-pytorch/blob/main/coco_lm_pytorch/coco_lm_pytorch.py</a></li>
<li></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining</p><p><a href="https://eagle705.github.io/COCO-LM-Correcting and Contrasting Text Sequences for Language Model Pretraining/">https://eagle705.github.io/COCO-LM-Correcting and Contrasting Text Sequences for Language Model Pretraining/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Joosung Yoon</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-06-20</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-08-30</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/nlp/">nlp</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=63297c6228f9450019a5f574&amp;product=sop" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/(ALiBi)%20TRAIN%20SHORT,%20TEST%20LONG:%20ATTENTION%20WITH%20LINEAR%20BIASES%20ENABLES%20INPUT%20LENGTH%20EXTRAPOLATION/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/Megatron-LM-Training%20Multi-Billion%20Parameter%20Language%20Models%20Using%20Model%20Parallelism/"><span class="level-item">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://eagle705.github.io/COCO-LM-Correcting%20and%20Contrasting%20Text%20Sequences%20for%20Language%20Model%20Pretraining/';
            this.page.identifier = 'COCO-LM-Correcting and Contrasting Text Sequences for Language Model Pretraining/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eagle705-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/eagle705-logo.png" alt="Joosung Yoon"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joosung Yoon</p><p class="is-size-6 is-block">Machine Learning Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>There and Back Again</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">47</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/eagle705" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/eagle705"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hisdevelopers"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JSYoon53859120"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/joosung-yoon/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/cslog/"><span class="level-start"><span class="level-item">cslog</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile" href="/categories/photo/"><span class="level-start"><span class="level-item">photo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">2월 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">1월 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">12월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">11월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">10월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">9월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">8월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">5월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">12월 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">12월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">11월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">10월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">9월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">8월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">7월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">5월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">4월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">11월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">6월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">5월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">4월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/drone/"><span class="tag">drone</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/grpc/"><span class="tag">grpc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">36</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp-kb/"><span class="tag">nlp, kb</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"><span class="tag">생각정리</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">광고</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-2655870716902046" data-ad-slot="2764253071" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#논문파일"><span class="level-left"><span class="level-item">1</span><span class="level-item">논문파일</span></span></a></li><li><a class="level is-mobile" href="#Ref"><span class="level-left"><span class="level-item">2</span><span class="level-item">Ref</span></span></a></li><li><a class="level is-mobile" href="#Author"><span class="level-left"><span class="level-item">3</span><span class="level-item">Author</span></span></a></li><li><a class="level is-mobile" href="#요약"><span class="level-left"><span class="level-item">4</span><span class="level-item">요약</span></span></a></li><li><a class="level is-mobile" href="#Related-work"><span class="level-left"><span class="level-item">5</span><span class="level-item">Related work</span></span></a></li><li><a class="level is-mobile" href="#Abstract"><span class="level-left"><span class="level-item">6</span><span class="level-item">Abstract</span></span></a></li><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">7</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Related-Work"><span class="level-left"><span class="level-item">8</span><span class="level-item">Related Work</span></span></a></li><li><a class="level is-mobile" href="#Method"><span class="level-left"><span class="level-item">9</span><span class="level-item">Method</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Preliminary-on-Language-Model-Pretraining"><span class="level-left"><span class="level-item">9.1</span><span class="level-item">Preliminary on Language Model Pretraining</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#BERT-Pretraining"><span class="level-left"><span class="level-item">9.1.1</span><span class="level-item">BERT Pretraining</span></span></a></li><li><a class="level is-mobile" href="#ELECTRA-Pretraining"><span class="level-left"><span class="level-item">9.1.2</span><span class="level-item">ELECTRA Pretraining</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Challenges-of-ELECTRA-Style-Pretraining"><span class="level-left"><span class="level-item">9.2</span><span class="level-item">Challenges of ELECTRA-Style Pretraining</span></span></a></li><li><a class="level is-mobile" href="#COCO-LM-Pretraining"><span class="level-left"><span class="level-item">9.3</span><span class="level-item">COCO-LM Pretraining</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Network-Configurations"><span class="level-left"><span class="level-item">9.3.1</span><span class="level-item">Network Configurations</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Experimental-Setup"><span class="level-left"><span class="level-item">10</span><span class="level-item">Experimental Setup</span></span></a></li><li><a class="level is-mobile" href="#Evaluation-Results"><span class="level-left"><span class="level-item">11</span><span class="level-item">Evaluation Results</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Efficiency"><span class="level-left"><span class="level-item">11.1</span><span class="level-item">Efficiency</span></span></a></li><li><a class="level is-mobile" href="#Ablation-Studies"><span class="level-left"><span class="level-item">11.2</span><span class="level-item">Ablation Studies</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Architecture"><span class="level-left"><span class="level-item">11.2.1</span><span class="level-item">Architecture.</span></span></a></li><li><a class="level is-mobile" href="#Pretraining-Signal-Construction"><span class="level-left"><span class="level-item">11.2.2</span><span class="level-item">Pretraining Signal Construction.</span></span></a></li><li><a class="level is-mobile" href="#CLM-Setup"><span class="level-left"><span class="level-item">11.2.3</span><span class="level-item">CLM Setup.</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Analyses-of-Contrastive-Learning-with-SCL"><span class="level-left"><span class="level-item">11.3</span><span class="level-item">Analyses of Contrastive Learning with SCL</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Ablation-on-Data-Augmentation"><span class="level-left"><span class="level-item">11.3.1</span><span class="level-item">Ablation on Data Augmentation</span></span></a></li><li><a class="level is-mobile" href="#Alignment-and-Uniformity"><span class="level-left"><span class="level-item">11.3.2</span><span class="level-item">Alignment and Uniformity</span></span></a></li><li><a class="level is-mobile" href="#Regularizing-the-Representation-Learning-for-Better-Few-Shot-Ability"><span class="level-left"><span class="level-item">11.3.3</span><span class="level-item">Regularizing the Representation Learning for Better Few-Shot Ability.</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Analyses-of-Language-Modeling-with-CLM"><span class="level-left"><span class="level-item">11.4</span><span class="level-item">Analyses of Language Modeling with CLM</span></span></a></li><li><a class="level is-mobile" href="#Prompt-Based-Fine-Tuning-with-CLM"><span class="level-left"><span class="level-item">11.5</span><span class="level-item">Prompt-Based Fine-Tuning with CLM</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Conclusion-and-Future-Work"><span class="level-left"><span class="level-item">12</span><span class="level-item">Conclusion and Future Work</span></span></a></li><li><a class="level is-mobile" href="#코드구현"><span class="level-left"><span class="level-item">13</span><span class="level-item">코드구현</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-06T04:15:12.000Z">2023-02-06</time></p><p class="title"><a href="/(FLAN)%20Finetuned%20Language%20Models%20Are%20Zero-Shot%20Learners/">(FLAN) Finetuned Language Models Are Zero-Shot Learners</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-03T07:44:54.000Z">2023-02-03</time></p><p class="title"><a href="/(T0)%20Multitask%20Prompted%20Training%20Enables%20Zero-Shot%20Task%20Generalization/">(T0) Multitask Prompted Training Enables Zero-Shot Task Generalization</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-01-20T14:21:17.000Z">2023-01-20</time></p><p class="title"><a href="/InstructGPT/">(InstructGPT) Training language models to follow instructions with human feedback</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-01-09T07:34:18.000Z">2023-01-09</time></p><p class="title"><a href="/Chinchilla-Training-Compute-Optimal-Large-Language-Models/">(Chinchilla) Training Compute-Optimal Large Language Models</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-12-12T06:45:59.000Z">2022-12-12</time></p><p class="title"><a href="/Robust-Conversational-Agents-against-Imperceptible-Toxicity-Triggers/">Robust Conversational Agents against Imperceptible Toxicity Triggers</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 Joosung Yoon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>