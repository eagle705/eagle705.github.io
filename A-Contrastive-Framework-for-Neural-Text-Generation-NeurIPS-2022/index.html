<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>A Contrastive Framework for Neural Text Generation (NeurIPS 2022) - Luke&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Luke&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Luke&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="발표자료 논문: A Contrastive Framework for Neural Text Generation.pdf 발표자료: A Contrastive Framework for Neural Text Generation.pdf  느낀점 잘 쓴 논문 같다 간단한 아이디어지만 효과적 decoding 시간이 생각보다 덜 걸려서 신기했음 contrastive 방법론이"><meta property="og:type" content="blog"><meta property="og:title" content="A Contrastive Framework for Neural Text Generation (NeurIPS 2022)"><meta property="og:url" content="https://eagle705.github.io/A-Contrastive-Framework-for-Neural-Text-Generation-NeurIPS-2022/"><meta property="og:site_name" content="Luke&#039;s Blog"><meta property="og:description" content="발표자료 논문: A Contrastive Framework for Neural Text Generation.pdf 발표자료: A Contrastive Framework for Neural Text Generation.pdf  느낀점 잘 쓴 논문 같다 간단한 아이디어지만 효과적 decoding 시간이 생각보다 덜 걸려서 신기했음 contrastive 방법론이"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193591114-fdd96494-0519-4c9c-8c80-74ea1da0a08b.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193846391-2a54a349-7912-402f-8af5-4bbba341affc.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193850567-b262cad8-c9a0-464c-82a3-f8ffc506d85c.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193853504-3ac0bd75-3216-4835-8531-fa64692099d1.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193881433-1ac2dd2c-3630-41a4-b1ee-43c57c218bd5.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193882141-6618ce03-ab60-4fcd-9871-44a4669f70dc.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193882726-017592b8-32a5-48d6-8759-ecb4d51c68f5.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193883487-eba88ed2-1965-4cf2-91e3-c29c9ed68c43.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193883674-18115b21-e85e-47e4-bee2-bc848211fc4e.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193884691-e93678bf-65b2-437c-a617-8e5c7f49bdfa.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193884909-477efd9e-2a80-4e02-959c-49853f72d25a.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193885077-8ab2467a-db03-46ea-ae3b-c1a81cc7f0dd.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/193886445-a88f72fd-9176-44ac-8172-e7de2a78e74f.png"><meta property="article:published_time" content="2022-10-05T07:16:45.000Z"><meta property="article:modified_time" content="2022-10-05T07:17:17.065Z"><meta property="article:author" content="Joosung Yoon"><meta property="article:tag" content="nlp"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://user-images.githubusercontent.com/7252598/193591114-fdd96494-0519-4c9c-8c80-74ea1da0a08b.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://eagle705.github.io/A-Contrastive-Framework-for-Neural-Text-Generation-NeurIPS-2022/"},"headline":"A Contrastive Framework for Neural Text Generation (NeurIPS 2022)","image":["https://user-images.githubusercontent.com/7252598/193591114-fdd96494-0519-4c9c-8c80-74ea1da0a08b.png","https://user-images.githubusercontent.com/7252598/193846391-2a54a349-7912-402f-8af5-4bbba341affc.png","https://user-images.githubusercontent.com/7252598/193850567-b262cad8-c9a0-464c-82a3-f8ffc506d85c.png","https://user-images.githubusercontent.com/7252598/193853504-3ac0bd75-3216-4835-8531-fa64692099d1.png","https://user-images.githubusercontent.com/7252598/193881433-1ac2dd2c-3630-41a4-b1ee-43c57c218bd5.png","https://user-images.githubusercontent.com/7252598/193882141-6618ce03-ab60-4fcd-9871-44a4669f70dc.png","https://user-images.githubusercontent.com/7252598/193882726-017592b8-32a5-48d6-8759-ecb4d51c68f5.png","https://user-images.githubusercontent.com/7252598/193883487-eba88ed2-1965-4cf2-91e3-c29c9ed68c43.png","https://user-images.githubusercontent.com/7252598/193883674-18115b21-e85e-47e4-bee2-bc848211fc4e.png","https://user-images.githubusercontent.com/7252598/193884691-e93678bf-65b2-437c-a617-8e5c7f49bdfa.png","https://user-images.githubusercontent.com/7252598/193884909-477efd9e-2a80-4e02-959c-49853f72d25a.png","https://user-images.githubusercontent.com/7252598/193885077-8ab2467a-db03-46ea-ae3b-c1a81cc7f0dd.png","https://user-images.githubusercontent.com/7252598/193886445-a88f72fd-9176-44ac-8172-e7de2a78e74f.png"],"datePublished":"2022-10-05T07:16:45.000Z","dateModified":"2022-10-05T07:17:17.065Z","author":{"@type":"Person","name":"Joosung Yoon"},"publisher":{"@type":"Organization","name":"Luke's Blog","logo":{"@type":"ImageObject","url":"https://eagle705.github.io/img/eagle705-logo.png"}},"description":"발표자료 논문: A Contrastive Framework for Neural Text Generation.pdf 발표자료: A Contrastive Framework for Neural Text Generation.pdf  느낀점 잘 쓴 논문 같다 간단한 아이디어지만 효과적 decoding 시간이 생각보다 덜 걸려서 신기했음 contrastive 방법론이"}</script><link rel="canonical" href="https://eagle705.github.io/A-Contrastive-Framework-for-Neural-Text-Generation-NeurIPS-2022/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-110980734-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-110980734-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-2655870716902046" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-10-05T07:16:45.000Z" title="10/5/2022, 4:16:45 PM">2022-10-05</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-10-05T07:17:17.065Z" title="10/5/2022, 4:17:17 PM">2022-10-05</time>&nbsp;업데이트 됨</span><span class="level-item">15분안에 읽기 (약 2266 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">A Contrastive Framework for Neural Text Generation (NeurIPS 2022)</h1><div class="content"><h1 id="발표자료"><a href="#발표자료" class="headerlink" title="발표자료"></a>발표자료</h1><ul>
<li>논문: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/9709326/A.Contrastive.Framework.for.Neural.Text.Generation.pdf">A Contrastive Framework for Neural Text Generation.pdf</a></li>
<li>발표자료: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/9713502/A.Contrastive.Framework.for.Neural.Text.Generation.pdf">A Contrastive Framework for Neural Text Generation.pdf</a></li>
</ul>
<h1 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h1><ul>
<li>잘 쓴 논문 같다</li>
<li>간단한 아이디어지만 효과적</li>
<li>decoding 시간이 생각보다 덜 걸려서 신기했음</li>
<li>contrastive 방법론이 결국 비슷한 토큰 안나오게 하겠다인데, simCTG는 MLE 로 보완되지만 디코딩 부분은 아예 비슷한걸 견제하는 식으로 나오는데도 결과가 좋게 나오는게 신기 (물론 기존의 확률아 있어서 보완이 되지만) -&gt; degeneration penalty를 크게 줘도 ppl 결과가 좋길래 신기했음)</li>
</ul>
<h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><p>Yixuan Su* Tian Lan** Yan Wang** Dani Yogatama+ Lingpeng Kong++ Nigel Collier*<br>*Language Technology Lab, University of Cambridge<br>**Tencent AI Lab +DeepMind<br>++Department of Computer Science, The University of Hong Kong</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>문제: maximization-based decoding methods (e.g., beam search) of neural language models often lead to degenerate solutions<ul>
<li>the generated text is unnatural and contains undesirable repetitions</li>
</ul>
</li>
<li>시중에 나온 대안: Existing approaches introduce stochasticity via sampling or modify training objectives to decrease the probabilities of certain tokens (e.g., <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/unlikelihood_training">unlikelihood training</a>)<ul>
<li>대안의 문제: However, they often lead to solutions that lack coherence</li>
</ul>
</li>
<li>본 논문에서 보인 것: an underlying reason for model degeneration is <code>the anisotropic distribution of token representations.</code></li>
<li>present a contrastive solution: <ul>
<li>(i) SimCTG, a contrastive training objective to <code>calibrate</code> the model’s representation space,<ul>
<li>anisotropic 해소하겠다</li>
</ul>
</li>
<li>(ii) a decoding method—contrastive search—to encourage diversity while maintaining coherence in the generated text.<ul>
<li>다양하게 뽑지만 coherence 유지해보겠다</li>
</ul>
</li>
</ul>
</li>
<li>SOTA를 이기는 결과 보여줬음</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul>
<li>the conventional approach of training a language model with maximum likelihood estimation (MLE) and decoding the most likely sequence is often not sufficient<ul>
<li>평범한 접근방법인 MLE 기반의 학습과 디코딩은 대체로 충분하지 않음</li>
<li>degeneration 결과를 보여주기도함<ul>
<li>tend to be dull and contain undesirable repetitions at different levels (e.g., token-, phrase-, and sentence-level)</li>
<li>해결방법중 하나는 less likely vocab에서 샘플링하는 디코딩방법을 사용하는 것 (To alleviate this problem, previous solutions modify the decoding strategy by sampling from less likely vocabularies)</li>
<li>하지만 이런 방법은 의미적으로 안맞거나 반대되기도하는등 부작용이 있음</li>
<li>Another approach addresses the degeneration problem by modifying the model’s output vocabulary distribution with unlikelihood training (unlikelihood training도 비슷한 맥락)</li>
</ul>
</li>
</ul>
</li>
<li>이러한 이유는 token representation distribution의 비대칭 때문이라고 주장해보겠음 (the degeneration of neural language models stems from the anisotropic distribution of token representations, i.e., their representations reside in a narrow subset of the entire space [10, 9, 44].)</li>
<li>Figure 1은 GPT-2의 token representation에 대한 cosine sim matrix임 대부분이 0,95 이상인걸 볼수 있음<ul>
<li>In an ideal setting, the token representations should follow an isotropic distribution, i.e., the token similarity matrix should be sparse and the representations of distinct tokens should be discriminative<br><img src="https://user-images.githubusercontent.com/7252598/193591114-fdd96494-0519-4c9c-8c80-74ea1da0a08b.png" alt="image"></li>
</ul>
</li>
<li>본 논문에서 제안하는 모델<ul>
<li><code>SimCTG</code> (a <strong>sim</strong>ple <strong>c</strong>ontrastive framework for neural <strong>t</strong>ext <strong>g</strong>eneration) that encourages the model to learn discriminative and isotropic token representations.</li>
<li>The Key intuition<ul>
<li>(i) at each decoding step, the output should be selected from the set of most probable candidates predicted by the model to better maintain the semantic coherence between the generated text and the human-written prefix</li>
<li>(ii) the sparseness of the token similarity matrix of the generated text should be preserved to avoid degeneration.</li>
</ul>
</li>
</ul>
</li>
<li>PPL이나 휴먼 평가등에서도 개선된 결과를 보여줌<ul>
<li>the experimental results verify that SimCTG improves the intrinsic qualities of the language model, as evaluated by perplexity and token prediction accuracy (§4.2 and Appendix D). Moreover, we demonstrate that the proposed contrastive search significantly outperforms previous state-of-the-art decoding methods in both human and automatic evaluations</li>
</ul>
</li>
</ul>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><ul>
<li>MLE로 학습하는 LM은 transformer-based model 구조에서 모델의 표현이 anisotropic distribution을 갖게 됨</li>
<li>Deterministic Sampling은 greedy, beam이고 highest probability에 의존해서 degeneration을 야기함</li>
<li>Stochastic Sampling은 top-k, nucleus sampling류임, 가끔 의미적으로 반대되는 단어까지 생성하기도 하는 부작용 있음</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/193846391-2a54a349-7912-402f-8af5-4bbba341affc.png" alt="image"></p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><ul>
<li>how to apply contrastive learning to calibrate the representation space of the language model</li>
<li>introduce our proposed contrastive search decoding algorithm</li>
</ul>
<h2 id="Contrastive-Training"><a href="#Contrastive-Training" class="headerlink" title="Contrastive Training"></a>Contrastive Training</h2><ul>
<li>Our goal is to encourage the language model to learn discriminative and <code>isotropic token representations</code><ul>
<li>cosine sim으로 유사한 토큰들은 더 큰 loss를 받는 구조 -&gt; 붙어있는 토큰을 더 멀리 떨어뜨리게 하는 효과<ul>
<li>Q) 벡터적으로 유사한 토큰표현을 떨어뜨리는 효과가 서로 구분할 수 있는 효과를 주지만, 학습은 잘 되게하는걸까? 이 부분은 MLE가 잘해야하는 구조인듯</li>
</ul>
</li>
<li>ρ 값이 0 이면 적용 안하는거나 마찬가지 -&gt; MLE만 쓰는 구조</li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/193850567-b262cad8-c9a0-464c-82a3-f8ffc506d85c.png" alt="image"></p>
<h2 id="Contrastive-Search"><a href="#Contrastive-Search" class="headerlink" title="Contrastive Search"></a>Contrastive Search</h2><ul>
<li>모델이 예측한 셋 안에서 확률 높은 후보들이되, 이전 문맥과 구분이 될 수 있어야함</li>
<li>토큰 생성에 대한 확률값에 해당 토큰의 hidden states와 이전 토큰들의 hidden states의 유사도중 max값을 뽑아서 penalty term으로 줌 <ul>
<li><code>token들이 많으면 이거 계산시간 오래 걸리지 않을까?</code></li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/193853504-3ac0bd75-3216-4835-8531-fa64692099d1.png" alt="image"></p>
<h1 id="Document-Generation"><a href="#Document-Generation" class="headerlink" title="Document Generation"></a>Document Generation</h1><ul>
<li>Open-ended document generation에 적용</li>
<li>Our proposed approach is architecture-agnostic<ul>
<li>GPT2 (117M)에 Loss_SimCTG 적용해서 파인튜닝하고, contrastive search 이용해서 decoding 해봤음</li>
<li>baseline은 GPT2를 evaluated benchmark에 대해서 finetuning하되 아래 방법으로 함<ul>
<li>[1] MLE GPT2</li>
<li>[2] unlikelihood GPT2</li>
</ul>
</li>
</ul>
</li>
<li>Evaluation Benchmark는 Wikitext-103 데이터</li>
<li>Training은 SimCTG, MLE는 Wikitext-103 (40k training steps)데이터에 대해 파인튜닝했고 UL baseline에 대해서는 38.5K steps를 token-level, 1.5K steps를 sentence-level로 UL 학습함<ul>
<li>bs: 128, max_seq_len: 256, optim: adam, lr: 2e-5</li>
</ul>
</li>
<li>Decoding은 prefix를 32~128 length정도 되는 정보를 주고 시작함<ul>
<li>deterministic method: greedy, beam (10) search</li>
<li>stochastic method: p&#x3D;0.95</li>
<li><code>proposed contrastive search</code>: k and α in Eq. (5) are set as 8 (top_k 8개 보고) and 0.6. (degeneration penalty에 점수를 좀 더 줬음)</li>
</ul>
</li>
</ul>
<h2 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h2><p>평가 기준은 아래의 관점으로 정함</p>
<ul>
<li>(1) language modelling quality<ul>
<li>Perplexity on the test set of Wikitext-103.</li>
<li>Prediction Accuracy (토큰맞추기)</li>
<li>Prediction Repetition (next token의 top-1 예측이 prefix(이전입력)에 있으면 카운팅됨), 낮은게 좋음</li>
</ul>
</li>
<li>(2) generation quality<ul>
<li>Generation Repetition (sentence-level에서 n-grams의 반복을 카운팅) <code>rep-n = 100 × (1.0 − ( |unique n-grams(xˆ )| / |total n-grams(x^)| ))</code></li>
<li>Diversity (n-gram levels에서 repetition을 계산함)</li>
<li>MAUVE (생성한거랑 human-written text와 token distribution closeness를 계산함)</li>
<li>Semantic Coherence (simCSE로 prefix와 generated text의 representation을 구해서 coherence score를 계산함)</li>
<li>Perplexity of Generated Text</li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/193881433-1ac2dd2c-3630-41a4-b1ee-43c57c218bd5.png" alt="image"></p>
<h2 id="Human-Evaluation"><a href="#Human-Evaluation" class="headerlink" title="Human Evaluation"></a>Human Evaluation</h2><ul>
<li>평가 기준 (5점 척도)<ul>
<li>Coherence: Whether the generated text is semantically consistent with the prefix.</li>
<li>Fluency: Whether the generated text is fluent and easy to understand.</li>
<li>Informativeness: Whether the generated text is <strong>diverse</strong> and <strong>contains interesting content</strong>.</li>
</ul>
</li>
<li>평가 데이터<ul>
<li>randomly select 200 prefixes with length of 32 from the test set of Wikitext-103</li>
</ul>
</li>
<li>큰 모델에서 가장 좋은 점수를 얻었기 때문에 GPT3등에 대해서도 future work 해볼 예정</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/193882141-6618ce03-ab60-4fcd-9871-44a4669f70dc.png" alt="image"></p>
<h1 id="Open-domain-Dialogue-Generation"><a href="#Open-domain-Dialogue-Generation" class="headerlink" title="Open-domain Dialogue Generation"></a>Open-domain Dialogue Generation</h1><ul>
<li>Benchmark and Baselines<ul>
<li>영어랑 중국어 진행함</li>
<li>영어: DailyDialog</li>
<li>중국어: LCCC</li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/193882726-017592b8-32a5-48d6-8759-ecb4d51c68f5.png" alt="image"></p>
<h1 id="Further-Analysis"><a href="#Further-Analysis" class="headerlink" title="Further Analysis"></a>Further Analysis</h1><p><img src="https://user-images.githubusercontent.com/7252598/193883487-eba88ed2-1965-4cf2-91e3-c29c9ed68c43.png" alt="image"></p>
<h2 id="Token-Representation-Self-similarity"><a href="#Token-Representation-Self-similarity" class="headerlink" title="Token Representation Self-similarity"></a>Token Representation Self-similarity</h2><ul>
<li>Self-similarity? 토큰끼리 sim의 평균<br><img src="https://user-images.githubusercontent.com/7252598/193883674-18115b21-e85e-47e4-bee2-bc848211fc4e.png" alt="image"></li>
<li>Figure2. 보면 중간 레이어는 self-similarity가 비슷함</li>
<li>output layer에서는 차이가 확 나기 시작함</li>
</ul>
<h2 id="The-Effect-of-Contrastive-Loss-Margin"><a href="#The-Effect-of-Contrastive-Loss-Margin" class="headerlink" title="The Effect of Contrastive Loss Margin"></a>The Effect of Contrastive Loss Margin</h2><ul>
<li>contrastive loss margin ρ (Eq. (2))에 대해서 분석해보면 perplexity on the Wikitext-103 test set기준에서는 0.5값이 가장 적당한 마진임을 알 수 있음</li>
</ul>
<h2 id="Contrastive-Search-versus-Nucleus-Sampling"><a href="#Contrastive-Search-versus-Nucleus-Sampling" class="headerlink" title="Contrastive Search versus Nucleus Sampling"></a>Contrastive Search versus Nucleus Sampling</h2><ul>
<li>두가지 관점에서 분석함 <ul>
<li>(1) generation diversity </li>
<li>(2) perplexity of the generated text (gen-ppl)</li>
</ul>
</li>
</ul>
<h2 id="Decoding-Latency-Comparison"><a href="#Decoding-Latency-Comparison" class="headerlink" title="Decoding Latency Comparison"></a>Decoding Latency Comparison</h2><ul>
<li>생각보다 latency차이가 많이 안남 (신기)</li>
</ul>
<p><code>아래 그림은 simCTG 모델 기준임</code><br><img src="https://user-images.githubusercontent.com/7252598/193884691-e93678bf-65b2-437c-a617-8e5c7f49bdfa.png" alt="image"></p>
<h2 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h2><ul>
<li>실제 예제들<br><img src="https://user-images.githubusercontent.com/7252598/193884909-477efd9e-2a80-4e02-959c-49853f72d25a.png" alt="image"></li>
</ul>
<h2 id="Comparison-of-Token-Similarity-Matrix"><a href="#Comparison-of-Token-Similarity-Matrix" class="headerlink" title="Comparison of Token Similarity Matrix"></a>Comparison of Token Similarity Matrix</h2><ul>
<li>각 기법별 token similarity matrix보면 제안 기법의 align이 잘 되어있는걸 볼 수 있음<br><img src="https://user-images.githubusercontent.com/7252598/193885077-8ab2467a-db03-46ea-ae3b-c1a81cc7f0dd.png" alt="image"></li>
</ul>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul>
<li>Neural LM의 degeneation의 문제는 token representation의 anisotropic distribution 문제임을 보임</li>
<li>SimCTG 제안함, isotropic, discriminative representation space 만들어줌</li>
<li>contrastive search이라는 디코딩 방식도 제안함</li>
<li>automatic and human evaluations에서 모두 가장 좋은 점수 얻고 SOTA보다 높은 점수 기록함</li>
</ul>
<h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><ul>
<li><p>installation</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install simctg --upgrade.</span><br></pre></td></tr></table></figure>
</li>
<li><p>example</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># load the language model</span></span><br><span class="line"><span class="keyword">from</span> simctg.simctggpt <span class="keyword">import</span> SimCTGGPT model_name = r’cambridgeltl/simctg_wikitext103’ model = SimCTGGPT(model_name)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">tokenizer = model.tokenizer</span><br><span class="line"><span class="comment"># prepare input</span></span><br><span class="line">prefix_text = <span class="comment"># The prefix text in Table 4</span></span><br><span class="line"><span class="built_in">print</span> (’Prefix <span class="keyword">is</span>: &#123;&#125;’.<span class="built_in">format</span>(prefix_text))</span><br><span class="line">tokens = tokenizer.tokenize(prefix_text)</span><br><span class="line">input_ids = tokenizer.convert_tokens_to_ids(tokens) input_ids = torch.LongTensor(input_ids).view(<span class="number">1</span>,-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># generate result with contrastive search</span></span><br><span class="line">beam_width, alpha, decoding_len = <span class="number">8</span>, <span class="number">0.6</span>, <span class="number">128</span></span><br><span class="line">output = model.fast_contrastive_search(input_ids=input_ids,</span><br><span class="line">                        beam_width=beam_width, alpha=alpha,</span><br><span class="line">decoding_len=decoding_len) <span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * ’-’)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(output))</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Gen-ppl-Results-Measured-by-Different-Models"><a href="#Gen-ppl-Results-Measured-by-Different-Models" class="headerlink" title="Gen-ppl Results Measured by Different Models"></a>Gen-ppl Results Measured by Different Models</h2><ul>
<li>다른 모델들에 대한 결과를 보면 ppl 자체는 낮은 모델들도 있지만 human-written text와 가장 유사한건 역시 제안하는 모델</li>
<li>ppl이 낮은것 보다 사람이랑 유사한게 제일 좋은 것이다라고 주장 (이런 주장들은 기존에도 쭉 있었고 여기서도 같은 주장제기)<br><img src="https://user-images.githubusercontent.com/7252598/193886445-a88f72fd-9176-44ac-8172-e7de2a78e74f.png" alt="image"></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>A Contrastive Framework for Neural Text Generation (NeurIPS 2022)</p><p><a href="https://eagle705.github.io/A-Contrastive-Framework-for-Neural-Text-Generation-NeurIPS-2022/">https://eagle705.github.io/A-Contrastive-Framework-for-Neural-Text-Generation-NeurIPS-2022/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Joosung Yoon</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-10-05</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-10-05</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/nlp/">nlp</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=63297c6228f9450019a5f574&amp;product=sop" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/SOCIAL-CHEMISTRY-101/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">SOCIAL CHEMISTRY 101 - Learning to Reason about Social and Moral Norms</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/Learning-rate-warmup-scheduling/"><span class="level-item">Learning rate &amp; warmup step &amp; LR scheduling</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://eagle705.github.io/A-Contrastive-Framework-for-Neural-Text-Generation-NeurIPS-2022/';
            this.page.identifier = 'A-Contrastive-Framework-for-Neural-Text-Generation-NeurIPS-2022/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eagle705-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/eagle705-logo.png" alt="Joosung Yoon"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joosung Yoon</p><p class="is-size-6 is-block">Machine Learning Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>There and Back Again</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">54</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/eagle705" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/eagle705"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hisdevelopers"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JSYoon53859120"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/joosung-yoon/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cslog/"><span class="level-start"><span class="level-item">cslog</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/categories/photo/"><span class="level-start"><span class="level-item">photo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">5월 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">3월 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">2월 2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">1월 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">12월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">11월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">10월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">9월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">8월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">5월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">12월 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">12월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">11월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">10월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">9월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">8월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">7월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">5월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">4월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">11월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">6월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">5월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">4월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/drone/"><span class="tag">drone</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/grpc/"><span class="tag">grpc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">43</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp-kb/"><span class="tag">nlp, kb</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"><span class="tag">생각정리</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">광고</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-2655870716902046" data-ad-slot="2764253071" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#발표자료"><span class="level-left"><span class="level-item">1</span><span class="level-item">발표자료</span></span></a></li><li><a class="level is-mobile" href="#느낀점"><span class="level-left"><span class="level-item">2</span><span class="level-item">느낀점</span></span></a></li><li><a class="level is-mobile" href="#Note"><span class="level-left"><span class="level-item">3</span><span class="level-item">Note</span></span></a></li><li><a class="level is-mobile" href="#Author"><span class="level-left"><span class="level-item">4</span><span class="level-item">Author</span></span></a></li><li><a class="level is-mobile" href="#Abstract"><span class="level-left"><span class="level-item">5</span><span class="level-item">Abstract</span></span></a></li><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">6</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Background"><span class="level-left"><span class="level-item">7</span><span class="level-item">Background</span></span></a></li><li><a class="level is-mobile" href="#Methodology"><span class="level-left"><span class="level-item">8</span><span class="level-item">Methodology</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Contrastive-Training"><span class="level-left"><span class="level-item">8.1</span><span class="level-item">Contrastive Training</span></span></a></li><li><a class="level is-mobile" href="#Contrastive-Search"><span class="level-left"><span class="level-item">8.2</span><span class="level-item">Contrastive Search</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Document-Generation"><span class="level-left"><span class="level-item">9</span><span class="level-item">Document Generation</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Evaluation-Metrics"><span class="level-left"><span class="level-item">9.1</span><span class="level-item">Evaluation Metrics</span></span></a></li><li><a class="level is-mobile" href="#Human-Evaluation"><span class="level-left"><span class="level-item">9.2</span><span class="level-item">Human Evaluation</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Open-domain-Dialogue-Generation"><span class="level-left"><span class="level-item">10</span><span class="level-item">Open-domain Dialogue Generation</span></span></a></li><li><a class="level is-mobile" href="#Further-Analysis"><span class="level-left"><span class="level-item">11</span><span class="level-item">Further Analysis</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Token-Representation-Self-similarity"><span class="level-left"><span class="level-item">11.1</span><span class="level-item">Token Representation Self-similarity</span></span></a></li><li><a class="level is-mobile" href="#The-Effect-of-Contrastive-Loss-Margin"><span class="level-left"><span class="level-item">11.2</span><span class="level-item">The Effect of Contrastive Loss Margin</span></span></a></li><li><a class="level is-mobile" href="#Contrastive-Search-versus-Nucleus-Sampling"><span class="level-left"><span class="level-item">11.3</span><span class="level-item">Contrastive Search versus Nucleus Sampling</span></span></a></li><li><a class="level is-mobile" href="#Decoding-Latency-Comparison"><span class="level-left"><span class="level-item">11.4</span><span class="level-item">Decoding Latency Comparison</span></span></a></li><li><a class="level is-mobile" href="#Case-Study"><span class="level-left"><span class="level-item">11.5</span><span class="level-item">Case Study</span></span></a></li><li><a class="level is-mobile" href="#Comparison-of-Token-Similarity-Matrix"><span class="level-left"><span class="level-item">11.6</span><span class="level-item">Comparison of Token Similarity Matrix</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Conclusion"><span class="level-left"><span class="level-item">12</span><span class="level-item">Conclusion</span></span></a></li><li><a class="level is-mobile" href="#Appendix"><span class="level-left"><span class="level-item">13</span><span class="level-item">Appendix</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Usage"><span class="level-left"><span class="level-item">13.1</span><span class="level-item">Usage</span></span></a></li><li><a class="level is-mobile" href="#Gen-ppl-Results-Measured-by-Different-Models"><span class="level-left"><span class="level-item">13.2</span><span class="level-item">Gen-ppl Results Measured by Different Models</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:53.000Z">2023-05-09</time></p><p class="title"><a href="/pythia/">Pythia (A Suite for Analyzing Large Language Models Across Training and Scaling)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:39.000Z">2023-05-09</time></p><p class="title"><a href="/llama/">LLaMA (Open and Efficient Foundation Language Models)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:01.000Z">2023-05-09</time></p><p class="title"><a href="/ia3/">(IA3) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-23T08:14:37.000Z">2023-03-23</time></p><p class="title"><a href="/Alpaca/">Alpaca (A Strong Instruction-Following Model)</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-20T13:07:50.000Z">2023-02-20</time></p><p class="title"><a href="/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/">SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기</a></p><p class="categories"><a href="/categories/ML/">ML</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 Joosung Yoon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>