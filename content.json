{"posts":[{"title":"Docker 입문","text":"Docker를 처음 입문한건 텐서플로우를 윈도우에 설치하면서부터였다. 뭔가 바로 로컬에서 소스를 수정할 수 있는게 아니라서 뭔가 여간 불편해서 그 이후로 잘 안썼었는데.. 이 도커가 사람들은 정말 편리한지 이제 시장에서 배포환경이 거의다 도커가 되어버렸다(배포는 사실 진짜 편하긴하지..). 최근 회사에서도 도커를 쓸일이 생겨버려서.. 이젠 좀 체계적으로 정리를 해야겠다 싶다. 컨테이너 하나가 하나의 프로세스라고 보면 된다 dockerhub는 github같은 오픈된 도커 이미지 저장소다 docker id는 앞부분의 일부만 입력해도 실행이 된다 Docker 상태체크ps -a``` : 실행중인 도커 컨테이너 리스트를 확인12345678910111213141516171819202122232425262728293031323334```docker images``` : 도커 이미지 목록 ```docker login``` : docker hub계정으로 로그인 가능 ```docker restart &lt;도커id&gt;``` : 도커 컨테이너 재실행 ```docker attach &lt;도커id&gt;``` : (실행중인)도커 컨테이너에 접속 (웹서버 같이 백그라운드에서 실행되는 컨테이너에 attach로 접속하면 커맨드를 입력할 수 없고 로그인만 볼 수 있음) ```docker inspect &lt;도커id&gt;``` : 도커 컨테이너의 공유폴더나 기타 옵션들을 다 볼수있음! ### Docker 컨테이너 &amp; 이미지 삭제```docker rm &lt;도커id&gt;``` : 도커 컨테이너를 삭제함```docker rmi &lt;도커id&gt;``` : 도커 이미지를 삭제함 이미지까지 삭제해줘야 나중에 docker image를 업로드할때 같은 이름일경우 오류가 안남### Docker 컨테이너 안에 접속```docker exec &lt;도커id&gt;``` : 컨테이너에 새로운 프로세스를 실행시킬 때 사용함(예를들면 쉘이나.. / 컨테이너가 재시작될때 같이 재시작되진 않는다고 함_체크필요!!) ```docker exec -it &lt;도커id&gt; /bin/bash```: 컨테이너에 접속해서 bash 쉘 실행! (-it 라고 덧붙여 주어야 한다. 이는 STDIN 표준 입출력을 열고 가상 tty (pseudo-TTY) 를 통해 접속하겠다는 의미)### Docker 컨테이너 안에 파일 복사```docker cp /path/foo.txt &lt;도커id&gt;:/path/foo.txt```: 호스트에서 컨테이너로 파일 전송하는 방법 ```docker cp &lt;도커id&gt;:/path/foo.txt /path/foo.txt```:컨테이너에서 호스트로 파일 전송하는 방법 ### Docker 실행```docker run -d -p 8888:8888 -p 6006:6006 dockerhub계정/이미지이름```: 도커 이미지 다운받고 포트포워딩 후 실행(아마 -d가 다운.. -p가 포트포워딩인듯?) ```docker run -p 443:1443 -p 8080:8000 -itd -v &lt;호스트의 로컬디렉토리&gt;:&lt;컨테이너의 디렉토리&gt; --name &lt;생성할컨테이너이름&gt; &lt;이미지이름&gt;```: -v 옵션으로 공유폴더 마운트를 해줄 수 있음! -d는 백그라운드 옵션```docker-compose up -d```### Docker 이미지 업로드```docker commit -m &quot;&lt;메세지&gt;&quot; &lt;도커id&gt; dockerhub계정/이미지이름:태그```: docker 이미지 커밋(컨테이너에서 이미지 생성, 포트포워딩 새로 짜줄때 많이 씀) ```docker commit &lt;도커id&gt; &lt;이미지이름&gt;```: docker 이미지 커밋(컨테이너에서 이미지 생성, 포트포워딩 새로 짜줄때 많이 씀)```docker push dockerhub계정/이미지이름:태그```: dockerhub에 이미지 업로드### Docker 이미지 파일화 및 다시 로딩#### 컨테이너 To 이미지:태그 sudo docker commit oj-postgres oj-postgres:181107sudo docker commit oj-backend oj-backend:181107sudo docker commit oj-redis oj-redis:181107sudo docker commit judge-server judge-server:181107 12#### 이미지:태그 To 파일(tar) sudo docker save oj-redis:181107 &gt; oj-redissudo docker save oj-backend:181107 &gt; oj-backendsudo docker save oj-postgres:181107 &gt; oj-postgressudo docker save judge-server:181107 &gt; judge-server 12345#### 중간팁, docker관련 명령어 검색```history | grep docker```#### 파일 To 이미지 docker load &lt; judge-serverdocker load &lt; oj-backenddocker load &lt; oj-postgresdocker load &lt; oj-redis 12#### 다시 로딩 (docker-compose.yml) 1 version: “3”2 services:34 oj-redis:5 image: oj-redis:180918 &lt;– 이 부분을 images에 있는 이미지:태그 로 변경6 container_name: oj-redis_1809187 restart: always8 volumes:9 - $PWD/data/redis:/data10 ports:11 - “0.0.0.0:6379:6379” 123456789101112131415161718192021```docker-compose up -d```로 새로 이미지 생성참고용!!: https://www.slideshare.net/rkawkxms/docker-container도커 치트시트 https://gist.github.com/nacyot/8366310도커 셋팅 끝판왕: http://raccoonyy.github.io/docker-usages-for-dev-environment-setup/도커 run options: http://pyrasis.com/book/DockerForTheReallyImpatient/Chapter20/28### NVIDIA Docker Settings![docker_image](https://cloud.githubusercontent.com/assets/3028125/12213714/5b208976-b632-11e5-8406-38d379ec46aa.png){: height=&quot;50%&quot; width=&quot;50%&quot;}- 말 나온김에.. 딥러닝 필수품인 NVIDIA docker를 셋팅해보자- nvidia-docker 와 docker 두개를 설치해줘야한다- reference: - https://hiseon.me/linux/ubuntu/install-docker/ - https://github.com/NVIDIA/nvidia-docker - https://github.com/NVIDIA/nvidia-docker/wiki/Installation-(version-2.0) - http://moducon.kr/2018/wp-content/uploads/sites/2/2018/12/leesangsoo_slide.pdf - ftp: https://m.blog.naver.com/PostView.nhn?blogId=alice_k106&amp;logNo=220650722592&amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F##### Install- ubuntu-dirvers devices 명령어로 어떤 Nvidia graphic driver를 설치할지 체크 root@DeepLearning:/home/eagle# ubuntu-drivers devices== /sys/devices/pci0000:00/0000:00:02.0/0000:02:00.0 ==vendor : NVIDIA Corporationmodalias : pci:v000010DEd00001B06sv00001462sd00003609bc03sc00i00driver : nvidia-410 - third-party non-freedriver : nvidia-387 - third-party non-freedriver : nvidia-415 - third-party freedriver : nvidia-430 - third-party free recommendeddriver : nvidia-418 - third-party non-freedriver : nvidia-390 - third-party non-freedriver : nvidia-396 - third-party non-freedriver : xserver-xorg-video-nouveau - distro free builtindriver : nvidia-384 - third-party non-free 12- Ubuntu 16.04/18.04, Debian Jessie/Stretch/Buster Add the package repositories$ distribution=$(. /etc/os-release;echo $ID$VERSION_ID)$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -$ curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list$ sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit$ sudo systemctl restart docker 1 $ sudo apt-get install nvidia-docker2$ sudo pkill -SIGHUP dockerd 12- docker hub에서 이미지를 검색: https://hub.docker.com/r/nvidia/cuda docker pull nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04 12- 잘 동작하는지 nvidia-smi 명령어로 테스트 docker run –gpus all nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04 nvidia-smi 123- 딥러닝 프레임워크 이미지 다운 및 설치 - 텐서플로우 docker가 jupyter가 깔려있어서 이걸로 설치 후 파이토치도 설치하자: https://hub.docker.com/r/tensorflow/tensorflow/tags?page=2 docker run –runtime=nvidia -it -p 8888:8888 -p 6006:6006 tensorflow/tensorflow:latest-gpu-py3-jupyter 123- ftp 사용(5000 포트로 접근하는 경우임! 5000포트라는 건 변경가능!) 및 파일 시스템 공유까지 하고 싶으면 아래와 같이 셋팅 (도커의 경우 /tf 으로 경로를 두면 됨) docker run –runtime=nvidia -it -p 8888:8888 -p 6006:6006 -p 5000:22 -itd -v &lt;호스트의 공유할 디렉토리&gt;:&lt;도커의 공유될 디렉토리&gt; nvidia/cuda:10.0-cudnn7-ubuntu16.04-tf1.4-pytorch1.2 123456789101112131415- 잘 동작하는지 확인```Pythonfrom tensorflow.python.client import device_libdef get_available_gpus(): return [x.name for x in device_lib.list_local_devices()]get_available_gpus()# ['/device:CPU:0',# '/device:XLA_GPU:0',# '/device:XLA_GPU:1',# '/device:XLA_CPU:0',# '/device:GPU:0',# '/device:GPU:1'] ftp setting in docker image -&gt; container를 만들때 포트포워딩을 해준다 만약 못해줬으면 커밋해서 다시 이미지로 만들고 다시 컨테이너로 떠야한다 docker run -i -t --name sftp -p 50000:22 ubuntu ssh 설치 apt-get install ssh ssh 생성 12345678910111213141516171819202122cd ~/ssh-keygen -t rsa -P '' -f ~/.ssh/id_dsa# docker container 안에서 ssh-key 생성 결과Generating public/private rsa key pair.Created directory '/root/.ssh'.Your identification has been saved in /root/.ssh/id_dsa.Your public key has been saved in /root/.ssh/id_dsa.pub.The key fingerprint is:SHA256:woWqfGuErQHO6/FCmDkpY8oD4UsVNqP3lrACrtc0g0M root@f44bce126400The key's randomart image is:+---[RSA 2048]----+| || = . || o + . . ||+.E+ o . ||BBo+= + S ||X@=o*+ . ||X+*Boo ||o*=.o. ||.ooo. |+----[SHA256]-----+ sshd를 위한 폴더 생성 mkdir /var/run/sshd sshd가 컨테이너 시작시 실행되도록 ~/.bashrc 파일에 다음을 추가 12# autorun/usr/sbin/sshd 변경사항 적용 source ~/.bashrc User 추가 adduser eagle705 NLP dependecy settings (konlpy) ref: https://provia.tistory.com/57 방법1 12345apt-get updateapt-get install g++ openjdk-8-jdkpip install konlpy apt-get install curlbash &lt;(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh) 방법2 12345678910111213mkdir mecab_installcd mecab_installwget mecab-0.996-ko-0.9.2.tar.gztar -zxvf mecab-*-ko-*.tar.gzcd mecab-0.996-ko-0.9.2./configuremakemake checkmake installmecab --versionldconfigmecab --version 방법3 1pip install python-mecab-ko 그래도 안될때 12345mecab-config --libs-only-L | sudo tee /etc/ld.so.conf.d/mecab.confldconfiggit clone https://bitbucket.org/eunjeon/mecab-python-0.996.gitpip install ./mecab-python-0.996 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/bin/shOUT_DIR=&quot;${1:-./mecab}&quot;mkdir -v -p $OUT_DIRsudo yum install git cmake make automake wgetwget https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gzwget https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gzmv mecab-0.996-ko-0.9.2.tar.gz &quot;$OUT_DIR/&quot;mv mecab-ko-dic-2.1.1-20180720.tar.gz &quot;$OUT_DIR/&quot;cd &quot;$OUT_DIR&quot;tar zxfv mecab-0.996-ko-0.9.2.tar.gzcd mecab-0.996-ko-0.9.2autoreconf -vi#./configure./configure --with-mecab-config=./mecab-config --with-charset=utf8makemake checksudo make installcd ../sudo ldconfigtar zxfv mecab-ko-dic-2.1.1-20180720.tar.gzcd mecab-ko-dic-2.1.1-20180720autoreconf -vi./configuremakesudo make installcd ../git clone https://bitbucket.org/eunjeon/mecab-python-0.996.gitcd mecab-python-0.996python setup.py buildpython setup.py installmecab-config --libs-only-L | sudo tee /etc/ld.so.conf.d/mecab.confsudo ldconfig#chmod +x /home/eagle/anaconda3/envs/dl_py3/lib/python3.6/site-packages/* 사전추가 1234567wget https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gztar -zxvf mecab-ko-dic-2.1.1-20180720.tar.gzcd mecab-ko-dic-2.1.1-20180720./configuremakemake installmecab -d /usr/local/lib/mecab/dic/mecab-ko-dic 그 외에 설치할 것123456pip install easydictpip install sklearnpip install pandaspip install xlrdpip install easydict(https://worthpreading.tistory.com/56)","link":"/2018-04-17-Docker/"},{"title":"Vue.js 입문","text":"Vue에 대한 몇가지 속성을 빠르게 파악하기 위해 기록을 남겨둔다. 공식적인 문서를 위주로 참고해서 작성할 예정! 예제1123&lt;div id=&quot;app&quot;&gt; {{ message }}&lt;/div&gt; 123456var app = new Vue({ el: '#app', //DOM의 id를 여기서 입력해주고 data: { // DOM안의 템플릿에 key value 형태로 데이터를 뿌려줌 message: '안녕하세요 Vue!' }}) 문자열을 단순히 뿌려줌.자 이제 뿌려주는걸 넘어서 바인딩! 해주는걸 보자. 예제212345&lt;div id=&quot;app-2&quot;&gt; &lt;span v-bind:title=&quot;message&quot;&gt; 내 위에 잠시 마우스를 올리면 동적으로 바인딩 된 title을 볼 수 있습니다! &lt;/span&gt;&lt;/div&gt; 123456var app2 = new Vue({ el: '#app-2', data: { message: '이 페이지는 ' + new Date() + ' 에 로드 되었습니다' }}) 라고 되어있는 키워드가 바인딩의 핵심인데, ```v-bind``` 속성을 **디렉티브** 라고 함. ```v-``` 접두어는 뷰에서 제공하는 특수 속성임을 나타냄.12345678위의 예제는 “이 요소의 ```title``` 속성을 Vue 인스턴스의 message 속성으로 최신 상태를 유지 합니다.” 를 의미함#### 조건문과 반복문```html&lt;div id=&quot;app-3&quot;&gt; &lt;p v-if=&quot;seen&quot;&gt;이제 나를 볼 수 있어요&lt;/p&gt;&lt;/div&gt; 123456var app3 = new Vue({ el: '#app-3', data: { seen: true }}) v-if 라는 속성을 통해 data의 seen 속성이 true면 DOM을 그려줌. 텍스트 속성뿐 아니라 DOM의 구조에서도 데이터를 바인딩 할 수 있다는 뜻임. 1234567&lt;div id=&quot;app-4&quot;&gt; &lt;ol&gt; &lt;li v-for=&quot;todo in todos&quot;&gt; {{ todo.text }} &lt;/li&gt; &lt;/ol&gt;&lt;/div&gt; 12345678910var app4 = new Vue({ el: '#app-4', data: { todos: [ { text: 'JavaScript 배우기' }, { text: 'Vue 배우기' }, { text: '무언가 멋진 것을 만들기' } ] }}) 이제 위와 같은 예제를 통해서 알 수 있는건, v-키워드가 들어간 이후의 내용은 data 에 등장하는 변수라고 생각하면 된다는 것이다. 비록 “”을 통해 문자열처럼 나타내지만 data내의 key, value로 존재하는 변수 나타낸다. 사용자 입력 핸들링사용자가 앱과 상호 작용할 수 있게 하기 위해 우리는 v-on 디렉티브를 사용하여 Vue 인스턴스에 메소드를 호출하는 이벤트 리스너를 첨부 할 수 있음 1234&lt;div id=&quot;app-5&quot;&gt; &lt;p&gt;{{ message }}&lt;/p&gt; &lt;button v-on:click=&quot;reverseMessage&quot;&gt;메시지 뒤집기&lt;/button&gt;&lt;/div&gt; 1234567891011var app5 = new Vue({ el: '#app-5', data: { message: '안녕하세요! Vue.js!' }, methods: { // 이게 추가됨 reverseMessage: function () { this.message = this.message.split('').reverse().join('') } }}) el, data와 같은 레벨에서 methods가 추가 된 것을 확인 할 수 있음. Vue 인스턴스 내의 데이터에 접근할 땐 this 키워드를 사용함. 양방향 바인딩디렉티브를 통해서 양방향으로 바인딩 할 수 있게 한다고함. 이게 아니면 아마.. 위에 처럼 event listenr (```v-on```)를 써서 작업해야하지 않을까?12345```html&lt;div id=&quot;app-6&quot;&gt; &lt;p&gt;{{ message }}&lt;/p&gt; &lt;input v-model=&quot;message&quot;&gt;&lt;/div&gt; 123456var app6 = new Vue({ el: '#app-6', data: { message: '안녕하세요 Vue!' }}) 컴포넌트를 사용한 작성방법UI는 반복적인 부분이 매우 많음. 컴포넌트에 대해서 생각하게 되는건 당연한 수순. 컴포넌트는 대략 “트리” 구조와 비슷함. 구조안에 구조가 있다고 보면 될듯. 컴포넌트는 사실 DOM쪽에 가깝고, 로직은 뷰 앱단에서 처리해야한다고 봄. 컴포넌트 등록 방법! 1234// todo-item 이름을 가진 컴포넌트를 정의합니다Vue.component('todo-item', { template: '&lt;li&gt;할일 항목 하나입니다.&lt;/li&gt;'}) 1234&lt;ol&gt; &lt;!-- todo-item 컴포넌트의 인스턴스 만들기 --&gt; &lt;todo-item&gt;&lt;/todo-item&gt;&lt;/ol&gt; 위의 예제는 모든 할일이 똑같은 컴포넌트를 찍어낸거나 다름없음. 이는 뭔가 부족함.. 실제 앱은 다양한 입력을 대처해야하니까. 이를 위해서 ```prop`` 옵션이 나오는 것도 당연한 수순. 컴포넌트 정의하고~ 거기에 todo 라는 prop 옵션을 넣어준다. 컴포넌트에 대한 Vue app을 로직을 정의 한다. 1234567891011121314151617181920/// DOM 쪽에 가까움Vue.component('todo-item', { // 이제 todo-item 컴포넌트는 &quot;prop&quot; 이라고 하는 // 사용자 정의 속성 같은 것을 입력받을 수 있습니다. // 이 prop은 todo라는 이름으로 정의했습니다. props: ['todo'], template: '&lt;li&gt;{{ todo.text }}&lt;/li&gt;'})/// 로직 쪽에 가까움var app7 = new Vue({ el: '#app-7', data: { groceryList: [ { id: 0, text: 'Vegetables' }, { id: 1, text: 'Cheese' }, { id: 2, text: 'Whatever else humans are supposed to eat' } ] }}) DOM에서 컴포넌트를 생성해주고, todo prop에 Vue app의 변수들을 바인딩해서 넣어준다. 1234567891011121314&lt;div id=&quot;app-7&quot;&gt; &lt;ol&gt; &lt;!-- 이제 각 todo-item 에 todo 객체를 제공합니다. 화면에 나오므로, 각 항목의 컨텐츠는 동적으로 바뀔 수 있습니다. 또한 각 구성 요소에 &quot;키&quot;를 제공해야합니다 (나중에 설명 됨). --&gt; &lt;todo-item v-for=&quot;item in groceryList&quot; v-bind:todo=&quot;item&quot; v-bind:key=&quot;item.id&quot;&gt; &lt;/todo-item&gt; &lt;/ol&gt;&lt;/div&gt; Vue 인스턴스 생성(약간 복잡함~! 마음의 준비를 하자)Vue 인스턴스를 인스턴스화 할 때는 데이터, 템플릿, 마운트할 엘리먼트, 메소드, 라이프사이클 콜백 등의 옵션을 포함 할 수있는 options 객체를 전달 해야함 각 Vue 인스턴스는 data 객체에 있는 모든 속성을 프록시 처리 함 (프록시 처리??) 12345678910111213141516171819// 데이터 객체var data = { a: 1 }// Vue인스턴스에 데이터 객체를 추가합니다.var vm = new Vue({ data: data})// 같은 객체를 참조합니다!vm.a === data.a // =&gt; true// 속성 설정은 원본 데이터에도 영향을 미칩니다.vm.a = 2data.a // =&gt; 2// ... 당연하게도data.a = 3vm.a // =&gt; 3 데이터가 변경되면 화면은 다시 렌더링됨. 유념할 점은, data에 있는 속성들은 인스턴스가 생성될 때 존재한 것들만 반응형이라는 것. 즉, 다음과 같이 새 속성을 추가하면: 1vm.b = 'hi' b가 변경되어도 화면이 갱신되지 않음. 어떤 속성이 나중에 필요하다는 것을 알고 있으며, 빈 값이거나 존재하지 않은 상태로 시작한다면 아래와 같이 초기값을 지정할 필요가 있음. (여기에서 유일한 예외는 Object.freeze()를 사용하는 경우) 1234567data: { newTodoText: '', visitCount: 0, hideCompletedTodos: false, todos: [], error: null} 인스턴스 라이프사이클 훅각 Vue 인스턴스는 데이터 관찰을 설정하고, 템플릿을 컴파일하고, 인스턴스를 DOM에 마운트하고, 데이터가 변경 될 때 DOM을 업데이트해야 할 때 일련의 초기화 단계!!! 를 거침.그 과정에서 사용자 정의 로직을 실행할 수있는 라이프사이클 훅 도 호출됨. 예를 들어, created 훅은 인스턴스가 생성된 후에 호출됨. 예: 12345678910new Vue({ data: { a: 1 }, created: function () { // `this` 는 vm 인스턴스를 가리킵니다. console.log('a is: ' + this.a) }})// =&gt; &quot;a is: 1&quot; 인스턴스 라이프사이클의 여러 단계에서 호출될 다른 훅도 있음. 그 예로 mounted, updated 및 destroyed가 있음. 모든 라이프사이클 훅은 this 컨텍스트가 호출하는 Vue 인스턴스를 가리키며 호출됨. Vue 세계에서 “컨트롤러”의 컨셉이 어디에 있는지 궁금할 수 있음. 답은 컨트롤러가 없다! 컴포넌트의 사용자 지정 로직은 이러한 라이프사이클 훅으로 분할됨. Life cycle Diagram of Vue{: height=”50%” width=”50%”} 템플릿 문법여러가지 문법들이 있지만, 지금 당장에 필요한 것 위주로 일단 기록해둔다. 약어v- 접두사는 템플릿의 Vue 특정 속성을 식별하기 위한 시각적인 신호 역할을 함. 이 기능은 Vue.js를 사용하여 기존의 마크업에 동적인 동작을 적용할 때 유용하지만 일부 자주 사용되는 디렉티브에 대해 너무 장황하다고 느껴질 수 있음. 동시에 Vue.js가 모든 템플릿을 관리하는 SPA를 만들 때 v- 접두어의 필요성이 떨어짐. 따라서 가장 자주 사용되는 두개의 디렉티브인 v-bind와 v-on에 대해 특별한 약어를 제공. v-bind 약어12345&lt;!-- 전체 문법 --&gt;&lt;a v-bind:href=&quot;url&quot;&gt; ... &lt;/a&gt;&lt;!-- 약어 --&gt;&lt;a :href=&quot;url&quot;&gt; ... &lt;/a&gt; v-on 약어12345&lt;!-- 전체 문법 --&gt;&lt;a v-on:click=&quot;doSomething&quot;&gt; ... &lt;/a&gt;&lt;!-- 약어 --&gt;&lt;a @click=&quot;doSomething&quot;&gt; ... &lt;/a&gt; 이들은 일반적인 HTML과 조금 다르게 보일 수 있음. 하지만 :와 @는 속성 이름에 유효한 문자이며 Vue.js를 지원하는 모든 브라우저는 올바르게 구문 분석을 할 수 있음. 또한 최종 렌더링 된 마크업에는 나타나지 않음. 약어는 완전히 선택사항이지만 나중에 익숙해지면 편할 것 계산된 속성(Computed prop)템플릿 내에서 사용하는 표현식은 매우 편리하지만 단순한 연산에만 사용해야 함. 너무 많은 로직을 템플릿에 넣으면 유지보수가 어려움! 123&lt;div id=&quot;example&quot;&gt; {{ message.split('').reverse().join('') }}&lt;/div&gt; 이 시점에서, 템플릿은 더이상 간단하지 않고 장황함(가정). 이 때문에 복잡한 로직의 경우, 반드시 계산된 속성 을 사용해야함.계산된 속성을 사용한 경우, 기본예제11234&lt;div id=&quot;example&quot;&gt; &lt;p&gt;원본 메시지: &quot;{{ message }}&quot;&lt;/p&gt; &lt;p&gt;뒤집히도록 계산된 메시지: &quot;{{ reversedMessage }}&quot;&lt;/p&gt;&lt;/div&gt; 12345678910111213var vm = new Vue({ el: '#example', data: { message: '안녕하세요' }, computed: { // 계산된 getter reversedMessage: function () { // `this` 는 vm 인스턴스를 가리킵니다. return this.message.split('').reverse().join('') } }}) 여기서 우리는 계산된 속성인 reversedMessage를 선언했음. 우리가 제공하는 함수는 vm.reversedMessage속성에 대한 getter 함수(따로 셋팅은 안해줘도 얻어오는)로 사용 됨. 123console.log(vm.reversedMessage) // =&gt; '요세하녕안'vm.message = 'Goodbye'console.log(vm.reversedMessage) // =&gt; 'eybdooG' vm.reversedMessage의 값은 항상 vm.message의 값에 의존함. 일반 속성처럼 템플릿의 계산된 속성에 데이터 바인딩 할 수 있음. Vue는 vm.reversedMessage가 vm.message에 의존하는 것을 알고 있기 때문에 vm.message가 바뀔 때 vm.reversedMessage에 의존하는 바인딩을 모두 업데이트할 것. 그리고 가장 중요한 것은 우리가 선언적으로 의존 관계를 만들었다는 것. 계산된 getter 함수는 사이드 이펙트가 없어 테스트와 추론하기에 쉬워짐. 1 1 1 1","link":"/2018-04-20-Vuejs/"},{"title":"DBpedia 셋팅","text":"본 문서는 챗봇과 Knowlede base를 연동하기 위해 DBpedia를 설치하고 사용하는 내용을 다룬 문서입니다. A. 소개 오픈소스 명: DBpedia 기본적인 OWL, RDF, RDFS, LINED DATA에 대한 설명:http://operatingsystems.tistory.com/entry/Basic-of-Semantic-Web?category=578406http://operatingsystems.tistory.com/entry/Linked-Data-and-RDF?category=578406http://operatingsystems.tistory.com/entry/RDFS?category=578406http://operatingsystems.tistory.com/entry/OWL-Web-Ontology-Language 공식 OWL 관련 문서:http://www.w3c.or.kr/Translation/REC-owl-features-20040210/ Reference: https://joernhees.de/blog/2015/11/23/setting-up-a-linked-data-mirror-from-rdf-dumps-dbpedia-2015-04-freebase-wikidata-linkedgeodata-with-virtuoso-7-2-1-and-docker-optional/ Reference2: http://pacifico.cc/programming/virtuoso-dbpedia-setup Virtuoso vs Neo4j: https://db-engines.com/en/system/Neo4j%3BVirtuoso Docker 기반 DBpedia_virtuoso 설치: https://github.com/harsh9t/Dockerised-DBpedia-Virtuoso-Endpoint-Setup-Guide B. 설치virtuoso 설치12345brew install virtuosovi ~/.zshrc// 수정 아래처럼!export VIRTUOSO_HOME=&quot;/usr/local/Cellar/virtuoso/7.2.4.2&quot;export PATH=$PATH:$VIRTUOSO_HOME/bin virtuoso 실행/bin/virtuoso-t 루트로 실행하면 되는데에러가 발생함 123dyld: Library not loaded: /usr/local/opt/xz/lib/liblzma.5.dylib Referenced from: /usr/local/bin/virtuoso-t Reason: Incompatible library version: virtuoso-t requires version 8.0.0 or later, but liblzma.5.dylib provides version 6.0.0 -&gt; 해결하기 위해서 1brew install xz 해주면 간단히 해결됨 그후 ini파일이 있는 곳에서 virtuosos-t +f 로 실행http://localhost:8890/conductor 에 접속해서 테스트! 종료할 땐, 12ìsqlshutdown(); 기본적인 계정 정보 (id:dba / pw:dba)http://docs.openlinksw.com/virtuoso/defpasschange/ DBpeida 다운로드12345678# see comment above, you could also get another DBpedia version...sudo mkdir -p /usr/local/data/datasets/dbpedia/2016-10cd /usr/local/data/datasets/dbpedia/2016-10wget -r -nc -nH --cut-dirs=1 -np -l1 -A '*.*' -A '*.owl' -R '*unredirected*' http://downloads.dbpedia.org/2016-10/{core/,core-i18n/en,core-i18n/ko,dbpedia_2016-10.owl}brew install pigz pbzip2for i in core/*.*.bz2 core-i18n/*/*.*.bz2 ; do echo $i ; pbzip2 -dc &quot;$i&quot; | pigz - &gt; &quot;${i%bz2}gz&quot; &amp;&amp; rm &quot;$i&quot; ; done// bz2 파일을 virtuoso에서 읽을 수 없음. 압축 안된거거나 gz파일이여야함 아래 폴더 셋팅 잘 해주면 좋음 123456789101112131415161718192021222324252627282930313233343536cd /usr/local/data/datasets/dbpedia/2016-10/mkdir importedGraphscd importedGraphsmkdir dbpedia.orgcd dbpedia.org# ln -s ../../dbpedia*.owl ./ # see below!ln -s ../../core/* ./cd ..mkdir ext.dbpedia.orgcd ext.dbpedia.orgln -s ../../core-i18n/ko/* ./cd ..mkdir pagelinks.dbpedia.orgcd pagelinks.dbpedia.orgln -s ../../core-i18n/ko/page-links_ko.* ./cd ..mkdir topicalconcepts.dbpedia.orgcd topicalconcepts.dbpedia.orgln -s ../../core-i18n/en/topical-concepts_ko.* ./cd ..mkdir ko.dbpedia.orgcd ko.dbpedia.orgln -s ../../core-i18n/ko/article-categories_ko.nt.gz ./cd ..mkdir pagelinks.ko.dbpedia.orgcd pagelinks.ko.dbpedia.orgln -s ../../core-i18n/ko/page-links_ko.nt.gz ./cd .. virtuoso 로 import123456789101112131415161718isqlld_add('/Users/eagle/datasets/dbpedia/2016-10/dbpedia_2016-10.owl', 'http://dbpedia.org/resource/classes#');ld_dir_all('/Users/eagle/datasets/dbpedia/2016-10/importedGraphs/dbpedia.org', '*.*', 'http://dbpedia.org');ld_dir_all('/Users/eagle/datasets/dbpedia/2016-10/importedGraphs/ko.dbpedia.org', '*.*', 'http://ko.dbpedia.org');ld_dir_all('/Users/eagle/datasets/dbpedia/2016-10/importedGraphs/ext.dbpedia.org', '*.*', 'http://ext.dbpedia.org');ld_dir_all('/Users/eagle/datasets/dbpedia/2016-10/importedGraphs/pagelinks.dbpedia.org', '*.*', 'http://pagelinks.dbpedia.org');ld_dir_all('/Users/eagle/datasets/dbpedia/2016-10/importedGraphs/pagelinks.ko.dbpedia.org', '*.*', 'http://pagelinks.ko.dbpedia.org');ld_dir_all('/Users/eagle/datasets/dbpedia/2016-10/importedGraphs/topicalconcepts.dbpedia.org', '*.*', 'http://topicalconcepts.dbpedia.org');또는 아래 명령어로 얻어진 쿼리 이용(이때 현재 디렉토리 위치는 /Users/eagle/datasets/dbpedia/2016-10) for g in * ; do echo &quot;ld_dir_all('$(pwd)/$g', '*.*', 'http://$g');&quot; ; done 명령어 결과 ld_dir_all('/Users/eagle/datasets/dbpedia/2016-10/core', '*.*', 'http://core');ld_dir_all('/Users/eagle/datasets/dbpedia/2016-10/core-i18n', '*.*', 'http://core-i18n');ld_dir_all('/Users/eagle/datasets/dbpedia/2016-10/dbpedia_2016-10.owl', '*.*', 'http://dbpedia_2016-10.owl');ld_dir_all('/Users/eagle/datasets/dbpedia/2016-10/robots.txt', '*.*', 'http://robots.txt'); 저장하기 (위의 쿼리가 실질적으로 실행되는 부분, 시간 꽤 걸림!) 1rdf_loader_run(); 테스트 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263SELECT * WHERE {?s ?p ?q} LIMIT 10또는select * where { ?s &lt;http://ko.dbpedia.org/property/장소&gt; ?o } LIMIT 100 default Grahp IRI: http://core-i18n (첨 만들때 넣어줬던게 Graph IRI인듯, 나중에 Jena와 연동시 ```FROM``` 부분에 들어갈 부분! ) SELECT * WHERE {?s ?p ?q} LIMIT 1000 SELECT * WHERE {?s a &lt;http://dbpedia.org/ontology/SoccerClub&gt; } LIMIT 1000 (a가 예약어임) SELECT * WHERE {?s &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://dbpedia.org/ontology/SoccerClub&gt; } LIMIT 1000 PREFIX dbpedia-owl: &lt;http://dbpedia.org/ontology/&gt;SELECT * WHERE {?s a &lt;http://dbpedia.org/ontology/SoccerClub&gt;.?s dbpedia-owl:abstract ?abstract. } LIMIT 1000 PREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;PREFIX dbpedia-owl: &lt;http://dbpedia.org/ontology/&gt;PREFIX res: &lt;http://ko.dbpedia.org/resource/&gt;SELECT * WHERE {?fc rdfs:label &quot;FC 즈브로요프카 브르노&quot; @ko.?fc dbpedia-owl:abstract ?abstract } LIMIT 1000 PREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;PREFIX dbpedia-owl: &lt;http://dbpedia.org/ontology/&gt;PREFIX res: &lt;http://ko.dbpedia.org/resource/&gt;SELECT * WHERE {?name rdfs:label &quot;컴투스&quot; @ko.?name dbpedia-owl:abstract ?abstract } LIMIT 1000 PREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt;PREFIX dbpedia-owl: &lt;http://dbpedia.org/ontology/&gt;PREFIX res: &lt;http://ko.dbpedia.org/resource/&gt;SELECT * WHERE {?name rdfs:label &quot;컴투스&quot; @ko.?name dbpedia-owl:abstract ?abstract.?name dbpedia-owl:wikiPageWikiLink ?Link. } LIMIT 1000 PREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt; PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; PREFIX dbpedia-owl: &lt;http://dbpedia.org/ontology/&gt; PREFIX res: &lt;http://ko.dbpedia.org/resource/&gt; SELECT * WHERE {?name rdfs:label &quot;컴투스&quot; @ko. ?name dbpedia-owl:wikiPageWikiLink ?Link. ?name dbpedia-owl:abstract ?abstract. } LIMIT 1000 PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;PREFIX dbpedia-owl: &lt;http://dbpedia.org/ontology/&gt;SELECT * FROM &lt;http://core-i18n&gt;WHERE {?name rdfs:label '컴투스' @ko.?name dbpedia-owl:wikiPageWikiLink ?Link.?name dbpedia-owl:abstract ?abstract. } LIMIT 3 ```이 부분이 디비(그래프)를 선택하는 쪽이기 때문에 꼭 필요함!12345(참고: https://www.programcreek.com/java-api-examples/?api=virtuoso.jena.driver.VirtGraph)라이브러리 설치https://github.com/srdc/virt-jena/tree/master/lib/virtuoso/virtjdbc4/4.0 1.2.1 NamespacesIn this document, examples assume the following namespace prefix bindings unless otherwise stated: Prefix IRIrdf: http://www.w3.org/1999/02/22-rdf-syntax-ns#rdfs: http://www.w3.org/2000/01/rdf-schema#xsd: http://www.w3.org/2001/XMLSchema#fn: http://www.w3.org/2005/xpath-functions#","link":"/2018-05-16-DBpedia_setting/"},{"title":"온톨로지와 RDF에 대한 개념설명 및 생성방법","text":"본 Knowlede base를 구축하기 위해 온톨로지 및 RDF에 관한 설명, 문법 그리고 생성방법에 대해 다룬 문서입니다. A. 소개 Ontology란? 왜 Ontology를 만드는가?: 온톨로지는 한 도메인 내에서 정보를 공유하고자 하는 연구자들을 위한 공통된 어휘집(common vocabulary)을 제공한다. 온톨로지를 만들어야 하는 이유중 몇 가지는 사람들 또는 소프트웨어 에이전트 사이에 정보의 구조에 관한 [1] 공통된 이해(understanding)를 공유 [2] 도메인 지식(domain knowledge)을 재사용 [3] 도메인 가설(domain assumptions)을 분명히 [4] 운용 지식(operational knowledge)으로부터 도메인지식(domain knowledge)을 분리 [5]도메인 지식을 분석하기 위함 RDF란? 기본적인 OWL, RDF, RDFS, LINED DATA에 대한 설명:http://www.ezmeta.co.kr/page/?p=248http://operatingsystems.tistory.com/entry/Basic-of-Semantic-Web?category=578406http://operatingsystems.tistory.com/entry/Linked-Data-and-RDF?category=578406http://operatingsystems.tistory.com/entry/RDFS?category=578406http://operatingsystems.tistory.com/entry/OWL-Web-Ontology-Language 공식 OWL 관련 문서:http://www.w3c.or.kr/Translation/REC-owl-features-20040210/ B. Ontology 구축 툴 조사 결과 Stanford에서 나온 Protege라는 툴을 많이 쓰는 것으로 확인됨 Protégé(프로테제)는 온톨로지 에디터임 Visualization plugin도 존재함 (https://www.youtube.com/watch?v=yOeSqu30PPQ) 설치방법: 접속 후 OS에 맞는 버전 다운로드: http://protege.stanford.edu/ 튜토리얼: http://wiblee.tistory.com/entry/Protege-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC-01-%ED%94%84%EB%A1%9C%ED%85%8C%EC%A0%9C-%EA%B0%9C%EC%9A%94-%EC%84%A4%EC%B9%98 그밖: protege 사용법 검색 in Google refs How To build a RDF dataset: https://www.youtube.com/watch?v=leO7__ZonbQ How To Create Classes And Properties: https://www.youtube.com/watch?v=MbauHV2-XYw visualization: https://www.youtube.com/watch?v=bpjMYBc98bk DataType Prop vs ObjectType Prop : https://stackoverflow.com/questions/17724983/how-can-i-recognize-object-properties-vs-datatype-properties Running Simple SPARQL Queries: https://www.youtube.com/watch?v=0zUos1zWB5k importing data plugin: https://protege.stanford.edu/conference/2009/slides/ImportingDataProtegeConference2009.pdf C. RDF 구축 툴 파이썬 라이브러리가 있었음 (https://github.com/RDFLib)","link":"/2018-06-11-Ontology_RDF_summary/"},{"title":"XGBoost 정리","text":"본 문서는 XGBoost에 대한 기본적인 설명과 설치 가이드에 대해서 다룬 문서입니다. A. 소개 오픈소스 명: XGBoost Github URL: https://github.com/dmlc/xgboost Ref: Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016 B. 설치XGBoost는 CPU전용 설치와 GPU전용 설치 두개로 나뉜다.CPU 전용으로 설치한다면, install xgboost``` 를 해버리면 끝이나1234567실제로 사용하려고 하면, Decision Tree보다 느린 속도를 체감하게 되므로 자연스럽게 GPU를 쓰게 된다. GPU 전용으로 설치하려면, 소스로부터 직접 컴파일 해야한다.XGBoost에서는 install guide를 제공해주고 있는데, 현재까지 나온 install guide에는 약간의 문제가 있는데 바로 multi-gpu 관련 문제다. multi-gpu를 사용하기 위해선 GPU간의 communication을 담당해주는 **NCLL**(pronounced &quot;Nickel&quot;) 이라는걸 셋팅해줘야하는데 기본 가이드에선 본 셋팅이 빠져있기 때문이다. 설치 가이드: http://xgboost.readthedocs.io/en/latest/build.html#building-with-gpu-support 교정된 내용 출처: https://github.com/dmlc/xgboost/issues/2915 Ubuntu기준 전체적인 설치 프로세스는 다음과 같다. git clone –recursive https://github.com/dmlc/xgboostgit submodule initgit submodule update cd xgboost; make -j4 mkdir buildcd build 여기서 셋팅이 중요! (공식가이드에선 -DUSE_NCCL=ON가 빠져있음)cmake .. -DUSE_CUDA=ON -DUSE_NCCL=ONmake -j 12파이썬 패키지 설치 cd ..cd python-package; sudo python setup.py installsudo apt-get install python-setuptoolsexport PYTHONPATH=~/xgboost/python-packagecd python-package; python setup.py develop –user 12그러나 여기서 설치된게 conda env에 저장되는건 아니다. xgboost 폴더에서 conda 환경폴더로 복사해주면 끝! 사용자계정:~/xgboost$ cp -R * ~/anaconda3/envs/dl/lib/python3.6/site-packages/xgboost/ 12만약 Jupyter notebook에서 conda env의 커널이 안나온다면 다음 명령어로 해결하자. source activate myenvpython -m ipykernel install –user –name myenv –display-name “Python (myenv)” 123456789101112131415161718192021222324## 사용법간단하다. 파라미터 몇개만 추가해주면 된다.(parameter 설명: http://xgboost.readthedocs.io/en/latest/parameter.html)```python3import xgboost as xgb dtrain = xgb.DMatrix(x_train, label=y_train)dtest = xgb.DMatrix(x_test, label=y_test)param = { 'max_depth': 4, # the maximum depth of each tree 'eta': 0.3, # the training step for each iteration 'silent': 1, # logging mode - quiet 'objective': 'multi:softprob', # error evaluation for multiclass training 'num_class': 2, # the number of classes that exist in this datset# 'gpu_id': 0, # 특정 GPU를 지정하고 싶을때 쓰는 id 'n_gpus' : 2, # 2개 사용하자 'max_bin': 16, # GPU 'tree_method': 'gpu_hist', # GPU method (자세한 설명은 문서로!) 'predictor':'gpu_predictor' # train뿐만아니라 predict할때도 gpu쓸건지 결정} num_round = 35 # the number of training iterationsmodel = xgb.train(param, dtrain, num_round) CPU vs GPU in XGBoost# 실험 데이터 45,000 건에 대한 결과 # Before GPU # CPU times: user 11min 57s, sys: 6min 2s, total: 17min 59s # Wall time: 8min 10s # After GPU # CPU times: user 1min 35s, sys: 8.03 s, total: 1min 43s # Wall time: 10.1 s GPU 2대가 잘 돌아간다~ # +-----------------------------------------------------------------------------+ # | NVIDIA-SMI 390.67 Driver Version: 390.67 | # |-------------------------------+----------------------+----------------------+ # | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | # | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | # |===============================+======================+======================| # | 0 GeForce GTX 108... Off | 00000000:02:00.0 On | N/A | # | 37% 54C P2 205W / 250W | 6352MiB / 11177MiB | 75% Default | # +-------------------------------+----------------------+----------------------+ # | 1 GeForce GTX 108... Off | 00000000:03:00.0 Off | N/A | # | 32% 45C P2 172W / 250W | 811MiB / 11178MiB | 45% Default | # +-------------------------------+----------------------+----------------------+ 123456789101112131415161718### XGBoost rank- XGBoost rank도 랭킹 관련해서는 꽤 강력한 알고리즘인거 같은데 공식 홈페이지도 문서화가 너무 잘 안되어있어서 따로 이렇게 정리해두려함```python# ref: https://www.jianshu.com/p/9caef967ec0a# https://www.slideshare.net/MinsubYim/evaluation-in-ir-system# Dataformat을 보면# Query 상황에 대해 feedback 하기위한 rank 데이터가 필요함# Rank는 중복순위는 안되지만 중복되도 어느정도 그 값을 반영함# Data format# input: (batch, feature_dim) # feature에 벡터뿐 아니라 rank를 넣는건 어떨까?# output: (batch, label)# group은 query당 1개라고 생각하면 될듯# 데이터 셋을 만드려면, 쿼리당 대응되는 순위가 있어야함 (꼭 순위가 아니면 그냥 1등짜리) 123456789101112131415161718192021222324import pandas as pdimport numpy as npfrom xgboost import DMatrix,trainxgb_rank_params1 ={ 'booster' : 'gbtree', 'eta': 0.1, 'gamma' : 1.0 , 'min_child_weight' : 0.1, 'objective' : 'rank:pairwise', 'eval_metric' : 'merror', 'max_depth' : 6, 'num_boost_round':10, 'save_period' : 0 }xgb_rank_params2 = { 'bst:max_depth':2, 'bst:eta':1, 'silent':1, 'objective':'rank:pairwise', 'nthread':4, 'eval_metric':'ndcg'} 123456# generate training datasetn_group=2n_choice=5 feature_dim = 4dtrain=np.random.uniform(0,100,[n_group*n_choice,feature_dim]) # from, to, size(10*4)print(dtrain) # 총 10개고, feature는 4차원 [[98.15224011 98.04314806 55.75168775 49.41552951] [ 0.56621336 93.68536087 33.47794833 62.47306473] [23.30221404 4.19151316 25.7820436 20.25906041] [46.72072013 58.08852829 98.69009012 2.8024138 ] [12.55738795 9.25738782 38.79516659 5.4123012 ] [ 0.48553844 54.86897349 10.16412276 95.7597808 ] [89.66433777 18.72695147 15.36669597 68.28897962] [90.64073532 7.53006587 64.95569435 80.3247172 ] [18.26159288 85.18426467 2.33733382 56.1864659 ] [92.46583392 74.05326436 36.58640261 96.30062398]] 123456# numpy.random.choice(a, size=None, replace=True, p=None)# 각 그룹마다, [0,1,2,4,5] 중에 5개를 중복없이 뽑아서 넣는다. 그리고 한줄로 핀다dtarget=np.array([np.random.choice([0,1,2,4,5],5,False) for i in range(n_group)])print(dtarget)dtarget = dtarget.flatten() print(dtarget) [[2 1 4 4 0] [2 4 2 5 5]] [2 1 4 4 0 2 4 2 5 5] 123456# N_group은 표본의 각 그룹이 연속적이면 앞에서 뒤로 각 그룹에 있는 샘플의 수를 나타내는 데 사용됩니다. # [5,5]은 10개의 샘플 중 첫 번째 5 개가 첫 번째 그룹이고 마지막 세 개가 두 번째 그룹임을 의미합니다.dgroup = np.array([n_choice for i in range(n_group)])print(dgroup)dgroup = dgroup.flatten()print(dgroup) [5 5] [5 5] 123# concate Train data, very import here !xgbTrain = DMatrix(dtrain, label = dtarget)print(xgbTrain) &lt;xgboost.core.DMatrix object at 0x1a0eecd9e8&gt; 12xgbTrain.set_group(dgroup)print(xgbTrain) &lt;xgboost.core.DMatrix object at 0x1a0eecd9e8&gt; 12345# generate eval datadtrain_eval=np.random.uniform(0,100,[n_group*n_choice,feature_dim]) xgbTrain_eval = DMatrix(dtrain_eval, label = dtarget)xgbTrain_eval.set_group(dgroup)evallist = [(xgbTrain,'train'),(xgbTrain_eval, 'eval')] # eval만 있어도 되는듯 12345678910111213# train model# rankModel = train(xgb_rank_params1,xgbTrain,num_boost_round=10)# rankModel = train(xgb_rank_params2,xgbTrain,num_boost_round=20,evals=evallist)xgb_rank_params3 = { 'bst:max_depth':3, 'bst:eta':1, 'silent':1, 'objective':'rank:pairwise', 'nthread':4, 'eval_metric':'ndcg'} rankModel = train(xgb_rank_params3,xgbTrain,num_boost_round=20,evals=evallist) [0] train-ndcg:0.820237 eval-ndcg:0.658025 [1] train-ndcg:1 eval-ndcg:0.929127 [2] train-ndcg:1 eval-ndcg:0.787053 [3] train-ndcg:1 eval-ndcg:0.715353 [4] train-ndcg:1 eval-ndcg:0.755255 [5] train-ndcg:1 eval-ndcg:0.6952 [6] train-ndcg:1 eval-ndcg:0.6952 [7] train-ndcg:1 eval-ndcg:0.6952 [8] train-ndcg:1 eval-ndcg:0.6952 [9] train-ndcg:1 eval-ndcg:0.724963 [10] train-ndcg:1 eval-ndcg:0.724963 [11] train-ndcg:1 eval-ndcg:0.724963 [12] train-ndcg:1 eval-ndcg:0.724963 [13] train-ndcg:1 eval-ndcg:0.6952 [14] train-ndcg:1 eval-ndcg:0.6952 [15] train-ndcg:1 eval-ndcg:0.6952 [16] train-ndcg:1 eval-ndcg:0.6952 [17] train-ndcg:1 eval-ndcg:0.685061 [18] train-ndcg:1 eval-ndcg:0.685061 [19] train-ndcg:1 eval-ndcg:0.685061 1dtarget array([2, 1, 4, 4, 0, 2, 4, 2, 5, 5]) 1print(rankModel.predict(xgbTrain)) [ 0.72241294 -0.65030587 1.4792428 2.1987503 -1.6671811 -1.2401314 0.6501097 -0.49726778 2.8408237 2.3687568 ] 12345678#test datasetdtest=np.random.uniform(0,100,[n_group*n_choice, feature_dim]) dtestgroup=np.array([n_choice for i in range(n_group)]).flatten()xgbTest = DMatrix(dtest)xgbTest.set_group(dgroup)# testprint(rankModel.predict( xgbTest)) [ 1.5810932 -0.70324016 -0.490084 0.646863 -0.6283437 0.29787558 1.371202 1.1917953 0.61845744 0.8794551 ]","link":"/2018-06-12-XGBoost%20%EC%A0%95%EB%A6%AC/"},{"title":"Deep contextualized word representations(ELMo)","text":"요즘 Transformer가 나오면서 NLP관련 모델의 성능이 크게 증가했다.요즘 시대에 그냥 CNN이나 LSTM쓰면 옛날 사람 취급받을 것만 같은.. 또 하나의 breakthrough라고도 할 수 있을 것 같다. Word Representation쪽에서도 비슷한 도약이 있었는데, 그 시작이 ELMo다. 처음엔 그냥 성능을 약간 올려주는 모델인가보다 하고 넘어갔지만, 다양한 연구에서 활용되는 것을 보면서 이를 기반으로 현재는 Bert와 같은 모델도 나오게 되었다. 이젠 안볼수없는 개념이 되었기에, 논문을 통해 다시 한번 정리해보고자 한다. Author 저자:Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,Christopher Clark, Kenton Lee∗, Luke Zettlemoyer Abstract Syntax &amp; semantics 정보를 잡아냄 언어적 문맥(linguistic contexts)도 고려함 word vectors는 internal states에 대한 deep bidirectional language model(biLM)를 통해 유도됨 이러한 representation은 기존 모델에도 잘 붙여서 쓸수 있고 성능도 많이 올랐음(6개의 도전적인 NLP task에서 SOTA!! 기록; QA, SA 등등) Introduction word2vec등이 NLP에서 Key component였지만 양질의 표현을 얻기는 도전적인 문제가 있었음 syntax &amp; semantics와 linguistic contexts를 모델링해주는게 이상적이라 할 수 있음 이 관점으로 새로운 representation을 제안 하겠음 Vector를 LM과 함께 학습된 BiLSTM으로부터 얻어낼 것임 이러한 이유로, ELMo(Embeddings from Language Models) representation이라 칭할 것임 internal states를 조합해주는건 매우 풍부한 word representation을 줌 higher-level LSTM states는 context-dependent aspects of word meaning을 잘 캡쳐함 lower-level states는 aspects of syntax(e.g. POS)를 잘함 Related work 기존 word2vec관련 연구는 single context independent representation을 각 단어에 대해서만 얻었음 subword등을 통해 이를 개선하려는 연구도 있었 본 논문의 연구 또한 subword unit의 도움을 character convolutions을 통해 얻었음 context2vec이라는 context-dependent representation에 대한 연구도 있었음. BiLSTM으로 pivot word 주변의 context를 encoding함. NMT에 쓰인 CoVe라는 임베딩기법도 비슷한 방법임. NMT의 경우 parallel corpora의 한계가 있음. 본 연구에서는 monolingual data로 이러한 문제를 해결하고자함. (30 million sentences) ELMo: Embeddings from Language Models 대부분의 워드 임베딩과 달리, ELMo는 전체 input sentence에 대한 함수라고 할 수 있음 상위 two-layer biLMs에서 (with Character convolutions) 계산됨 bidirectional language models 기존 LM을 LSTM을 통해 context를 적용한 방법으로 모델링하고, 이때 character CNN도 적용 Backward LM의 경우 그냥 거꾸로 한다고 생각하면됨 Forward와 Backward의 log likelihood의 합을 maximize 하도록 수식화 ELMo ELMo는 biLM 모델에 있는 중간레이어의 표현(intermediate layer representations)임 각 token (t_k)마다 L-layer biLM에서는 2L+1개의 representation을 계산함 = L(hidden h) * 2(bidirection) + 1(input x) ELMo는 각 layer의 벡터를 다 써도되고, top만 떼서 써도 되긴하는듯(CoVe처럼)","link":"/2018-11-06-ELMo/"},{"title":"광교호수공원 드론 촬영","text":"[190413] 광교호수공원에서 드론 촬영한거 몇가지 기록 남겨보고자한다. 개인적으로는 너무 만족스러웠다:)","link":"/2019-04-06-drone_1/"},{"title":"Attention Is All You Need","text":"이번엔 오늘날의 NLP계의 표준이 된 Transformer를 제안한 논문인 Attenion Is All You Need에 대해서 리뷰해보고자 한다. 대략적인 내용은 이미 알고 있었지만, 디테일한 부분도 살펴보고자 한다. Author 저자: Ashish Vaswani, 외 7명 (Google Brain) 구글브레인..wow NIPS 2017 accepted Who is an Author?{: height=”50%” width=”50%”} 느낀점 Multi-Head-Attention을 빠르게 구현하기 위해 Matrix 연산으로 처리하되, Embedding Tensor를 쪼갠 후 합치는게 아닌 reshape &amp; transpose operator로 shape을 변경 후 한꺼번에 행렬곱으로 계산해서 다시 reshape 함으로써 병렬처리가 가능하게끔 구현하는게 인상적이었음 행렬곱할 때 weight 와 곱하는건 weight variable 생성 후 MatMul 하는게 아니라 그냥 다 Dense로 처리하는게 구현 팁이구나 느꼈음 요약: 쪼갠다음에 weight 선언 후 매트릭스 곱? No! -&gt; 쪼갠 다음에 Dense! -&gt; 쪼개면 for loop 때문에 병렬처리 안되잖아! -&gt; 다 계산후에 쪼개자! Attention만 구현하면 얼추 끝날 줄 알았는데 Masking 지분이 70~80%였음 Masking은 logical 연산 (boolean)으로 padding 체크해서 하는게 일반적임 Masking은 input에도 해주고 loss에도 해줌 마스킹 적용할땐 broadcasting 기법을 써서 하는게 일반적임 아래의 두 경우 모두 가능함 ex) (40, 5, 10, 10) + (40, 1, 1, 10) == (batch, head, seq, seq) ex) (40, 5, 10, 10) + (40, 1, 10, 10) == (batch, head, seq, seq) Abstract 대부분의 sequence 모델들은 encoder-decoder 프레임워크를 포함해서, 복잡한 RNN이나 CNN이었음 최고의 성능을 내는 모델 역시 encoder-decoder 프레임워크안에서 동작하지만, Attention mechansim으로 모델링했음 본 논문에서는 Transformer라는 오직 Attention mechansim에만 기초한 simple neural architecture를 제안함 RNN이나 CNN따위 없음.. 2개의 machine translation task에 대해서 실험했고, 병렬화도 잘되고 학습시간도 짧으면서 퀄리티도 우월함을 확인함 WMT 2014 English-to-German translation task에서 28.4 BLEU를 기록함 (다른 모델보다 2 BLEU 정도 높음) WMT 2014 English-to-French translation task에서는 41.8 BLEU로 SOTA 기록함 (8 GPUs로 3.5days 걸림) 다른 Task에서도 Generalize 잘됨 1. Introduction RNN, LSTM, GRU 등은 sequence modeling (language modeling, machine translation)에서 SOTA를 기록해왔음 대부분 노력들은 recurrent language model, encoder-decoder 구조 내에서 이뤄졌음 RNN 계열의 모델들은 input, output의 sequence 내에서 position을 따라 계산되는 특징이 있는데, 이는 병렬처리를 막고, 시퀀스 길이가 길어지면 문제가 생기는등의 여러가지 이슈가 있음 Attention mechansim은 sequence 내에서 거리에 상관 없이 dependency modeling이 가능하기 때문에 sequence modeling을 정복할 수 있음 기존엔 RNN 계열에 Attention이 적용되었는데, 본 논문에서 제안하는 Transformer는 recurrence 방식을 피하고, 대신 전적으로 attention mechanism에만 의지해서 input, output 사이의 global dependency를 고려하고자 함 Transformer는 병렬처리도 잘되고, 성능도 SOTA임 (학습시간은 12시간 걸렸음 with 8 P100 GPUs) 2. Background Sequential computation 문제를 해결하기 위해 the Extended Neural GPU, ByteNet, ConvS2S 등 CNN을 hidden representations을 병렬처리로 계산하기 위한 basic building block으로 사용하려는 연구들이 있었음 위의 연구들은 dependency를 고려하려는 관점에서 볼 때, distance에 대해서 linear (ConvS2S), 혹은 logarithm (ByteNet) 한 고려가 가능했음 하지만 position의 거리에 따른 dependency를 학습하는건 더 어렵게 만들었음 (이 부분은 잘 이해가 안가네.. 음 그 이전 모델보다는 더 잘되는거 아닌가? 지적하는 근거가 뭐지..) Transformer에서는 constant 레벨까지 operation 숫자를 줄일 수 있음(N개에 대한 dependency있지만 병렬처리가 가능해서 그런듯..?) Self-Attention (다른 말로, intra-attention)은 한 시퀀스 내에서 서로 다른 position들의 관계 representation을 계산하기 위한 attention mechanism임 Self-attention은 이미 여러 task에서 성공했음 (reading comprehension, abstractive summarization, textual entailment, learning task-independent sentence representations) 3. Model Architecture 대부분 경쟁력있는 sequence transduction model은 encoder-decoder 형태임 encoder는 input sequence of symbol representations (x1,…,xn)을 continuous representations (z1, …, zn)로 맵핑하는 역할을 함 decoder는 주어진 z로 부터 output sequence (y1, …, ym)를 생성함 각 스텝에서 모델은 auto-regressive함(previously generated symbol을 addtional input으로 사용) {: height=”50%” width=”50%”} 3.1. Encoder and Decoder Stacks Encoder 인코더는 N = 6 개의 identical layer의 스택으로 이루어져있음 각 layer는 두 개의 sub-layer로 이루어져있음 첫번째는 multi-head self-attention mechansim 두번째는 position-wise fully connected feed-forward network12345678def sub_layer(self, x, training=False, padding_mask=None): out_1, attention_weight = self.mha(x, K = x, V = x, mask=padding_mask, flag=&quot;encoder_mask&quot;) out_1 = self.dropout1(out_1, training=training) out_2 = self.layer_norm_1(out_1 + x) out_3 = self.position_wise_fc(out_2) out_3 = self.dropout2(out_3, training=training) out_4 = self.layer_norm_2(out_2 + out_3) return out_4, attention_weight 두 개의 레이어에 각각 residual connection &amp; layer normalization을 적용함 각 sub-layer의 output은 LayerNorm(x + Sublayer(x)) 형태임 layerNorm은 tf.keras.layers.LayerNormalization API로 쉽게 구현 가능함 Hidden units들에 대해 Norm을 계산하기 때문에 Batch Norm과 다르다고함 (추가로 공부 필요) where Sublayer(x) is the function implemented by the sub-layer itself1234for i in range(self.layer_num): x, attention_block1, attention_block2 = self.sub_layer(x, encoder_ouput, training, look_ahead_mask, padding_mask) attention_weights['decoder_layer{}_block1'.format(i + 1)] = attention_block1 attention_weights['decoder_layer{}_block2'.format(i + 1)] = attention_block2 residual connection을 하기 위해서 모델에 있는 모든 sub-layer는(embedding layer까지 포함) output의 dimension dmodel = 512 로 셋팅함 Decoder 디코더 또한 N = 6 개의 identical layer의 스택으로 이루어져있음 디코더에는 2개가 아닌 3개의 sub-layer로 구성됨 첫째는 Masked Multi-Head self-Attention 임. 입력 포지션 상에서 이어서 나오는 것들을 마스킹해버려서 position i 를 예측할때 known outputs at position less than i 만 사용 가능하게 함 두번째는 Multi-Head Attention임 얘는 encoder의 output에 적용됨 세번째는 Feed Forward Network임 결국 첫번째 sub-layer가 좀 특이한거고 두번째 sub-layer의 인풋에 encoder의 output이 들어가는 게 차이임1234567891011def sub_layer(self, x, encoder_ouput, training=False, look_ahead_mask=None, padding_mask=None): out_1, attention_weight_lah_mha_in_decoder = self.look_ahead_mha(x, K = x, V = x, mask = look_ahead_mask, flag=&quot;look_ahead_mask&quot;) out_1 = self.dropout1(out_1, training=training) out_2 = self.layer_norm_1(out_1 + x) out_3, attention_weight_pad_mha_in_decoder = self.mha(out_2, K = encoder_ouput, V = encoder_ouput, mask = padding_mask, flag=&quot;padding_mask&quot;) out_3 = self.dropout2(out_3, training=training) out_4 = self.layer_norm_2(out_3 + out_2) out_5 = self.position_wise_fc(out_4) out_6 = self.layer_norm_3(out_4 + out_5) return out_6, attention_weight_lah_mha_in_decoder, attention_weight_pad_mha_in_decoder 3.2. Attention Attention function은 query와 key-value pair를 output에 매핑하는것으로 설명 가능함 여기서 말하는 query, key, value는 모두 vector를 의미함 output은 value에 대한 weighted sum으로 계산되는데, 이 value에 할당되는 이 weight는 query에 대응되는 key의 compatibility function에 의해 계산됨 결과적으로 Query와 Key의 유사도로 weight 결정되고 이걸 적용하겠다는 것임 key와 value는 같은 벡터를 의미함 key는 weight 뽑는 용 value는 weight를 적용할때 실제 곱해지는 용 {: height=”50%” width=”50%”} 3.2.1. Scaled Dot-Product Attention 본 논문에서 쓰는 어텐션을 “Scaled Dot-Product Attention”이라 칭함 input은 queries, keys 라고 보면 되고 key의 dimension은 dk value의 dimension은 dv 임 먼저 query와 모든 key에 대해서 dot product를 계산함 계산한 값에 대해서 각각에 대해 $\\sqrt{d_k}$로 나눠줌 (여기서 Scaled라는 단어가 나온게 아닌가 싶음, 근데 왜 $\\sqrt{d_k}$로 나눠줄까? 다른것들도 어차피 똑같이 나눠주면 softmax에 영향 없을거같은데 음.. $e^x$의 input 스케일에 따라 값이 차이나서 그런건가..) softmax function으로 value에 적용할 weight를 얻음 실제로 쓸 땐, A set of queries에 대한 Attention은 동시에 계산하기 때문에 Matrix 형태로 사용함 {: height=”50%” width=”50%”} 많이 쓰이는 Attention은 주로 additive attention &amp; dot-product attention인데 본 논문에서 쓴건 dot-product쪽임, 1/$\\sqrt{d_k}$로 스케일링 해줬다는 차이가 있긴 함 이론적으론 둘 다 복잡도는 비슷하나, dot-product attention이 훨씬 빠르고 space-efficient한 이유는 highly optimized matrix multiplication code로 구현되어있기 때문임 1/$\\sqrt{d_k}$로 스케일링한 이유는 (이제 나오네) $d_k$ 값이 클 경우 dot product 값이 커지고, 이는 softmax function이 small gradients를 갖게 만드는 것이 아닌지 의심이 되었고 이러한 효과를 막기 위해 스케일링 한 것임 (얼추 맞았다) ($d_k$가 작은 경우 스케일링을 해주지 않으면 additive attention 성능이 더 좋다고 함)12345678910111213def scaled_dot_product_attention(self, Q, K, V, mask=None, flag=None): matmul_qk = tf.matmul(Q, K, transpose_b=True) # (batch, head_num, seq, split_embed_dim) * (batch, head_num, split_embed_dim, seq) = (batch, head_num, seq, seq) dk = tf.cast(tf.shape(K)[-1], tf.float32) # dk dim scaled_dot_product_qk = matmul_qk / tf.math.sqrt(dk) if mask is not None: minus_infinity = -1e9 scaled_dot_product_qk += mask * minus_infinity # broadcasting, masking에서 seq은 마지막자리 # mask와 scaled_dot_product_qk의 차원은 다르지만, 마지막 차원이 같기 때문에 broadcasting이 가능함 attention_weight = tf.nn.softmax(scaled_dot_product_qk, axis=-1) scaled_attention_output = tf.matmul(attention_weight, V) # (batch, head_num, seq, seq) * (batch, head_num, seq, split_embed_dim) = (batch, head_num, seq, split_embed_dim) return scaled_attention_output, attention_weight 3.2.2. Multi-Head Attention Single Attention function을 $d_{model}$차원의 keys, values, queries에 적용하기보다 다른 $d_{k}$, $d_{k}$, $d_{v}$차원을 갖는 queries, keys, values에 h times 적용하는 것이 더 좋다는걸 알게됨 한 마디로하면, 그냥 한번만 Attention function쓰는게 아니라, 기존 Dim을 쪼개서 여러개로 나누고 거기에 여러번 Attention funcion 적용하면 더 다양한 Attention이 적용되고(여기엔 살짝 랜덤한..부분이 있겠지) 더 다양한 representation을 얻을 수 있게 된다는 말임 차원을 나눈 상태에서 Attention function은 병렬적으로 계산되고 $d_v$ 차원의 output vectors가 생성됨 {: height=”50%” width=”50%”} Q, K, V는 문장 내에서 sequence정보를 포함하고 있는 Notation인듯 dim of Q == num of tokens X $d_{model}$ 로 생각하면 될 듯 본 논문에서는 mutli-head를 8개로 나눠서, 전체 512 차원을 64 차원의 8개 유닛으로 만듬 원래는 input 임베딩을 쪼개고, 거기에 맞는 fc를 넣어주면 된다고 생각했는데, $W_{i}^Q$를 $Q$에 곱해 주는거 자체가 input 임베딩을 쪼개는 것임 결과 차원이 num of toknes X $d_{k}$로 나오니까 1234567891011121314151617181920212223def split_head(self, vector): &quot;&quot;&quot;Split the last dimension into (num_heads, depth). Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) &quot;&quot;&quot; batch_size = tf.shape(vector)[0] # (batch, seq, embed_dim) -&gt; (batch, seq, head_num, split_embed_dim) x = tf.reshape(vector, (batch_size, -1, self.head_num, self.split_embed_dim)) return tf.transpose(x, perm=[0, 2, 1, 3]) # (batch, head_num, seq, split_embed_dim)def call(self, Q, K, V, mask=None, flag=None): # Query, Key 꺼낼 필요 없이 3개 복사해서 쓰면 됨 # 쪼갠다음에 weight 선언 후 매트릭스 곱? -&gt; 쪼갠 다음에 Dense -&gt; 쪼개면 for loop 때문에 병렬처리 안되잖아 -&gt; 다 계산후에 쪼개자 -&gt; 쪼개지말고 reshape으로 하면 더 깔끔하다 multi_head_Q = self.split_head(self.Wq(Q)) multi_head_K = self.split_head(self.Wk(K)) multi_head_V = self.split_head(self.Wv(V)) self.scaled_attention_output, self.attention_weight = self.scaled_dot_product_attention(multi_head_Q, multi_head_K, multi_head_V, mask, flag) # (batch, head_num, seq, split_embed_dim) -&gt; (batch, seq, split_embed_dim) self.concat_scaled_attention = tf.reshape(self.scaled_attention_output, (tf.shape(Q)[0], -1, self.embed_dim)) return self.concat_scaled_attention, self.attention_weight 3.2.3. Applications of Attention in our Model 트랜스포머에서는 멀티헤드 어텐션을 3곳에 적용함 첫번째, “encoder-decoder Attention” layer 에 적용함 decoder의 input에 대해서 Attention 적용하고 그 결과를 Query로 만든 다음 레이어에 적용할때 encoder의 output을 key,value로 사용함 이렇게 하면 decoder의 input도 모두 고려하면서, encoder의 output도 모두 고려하는 seq2seq 모델에 attention을 적용한것과 비슷하게 됨 두번째, “self-attention layer” in encoder 에 적용함, 이 역시도 sequence 내의 모든 position을 다 고려할 수 있음 세번째, “self-attention layer” in decoder 에 적용함, 보지못한 정보를 보는 것을 막기 위해 (to prevent leftward information flow in the decoder to preserve the auto-regressive property) scaled dot-product attention안에 마스킹을 적용함(minus infinity) (Figure 2) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def create_padding_mask(seq): seq = tf.cast(tf.math.equal(seq, 0), tf.float32) # add extra dimensions so that we can add the padding # to the attention logits. return seq[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len)def create_look_ahead_mask(step_size): &quot;&quot;&quot; - decoder에서 각 상태에 대한 self-attention이 inference step에 맞게 future token을 보지 못하게 해야됨 - 각 step이 소유하고 있는 attention은 step개수 만큼임 - future token보지 못하게 하려면 각 step에서 future step에 대해서 마스킹 해야함 - 1 step에서는 나머지 n-1개 masking, 2번째 스텝에서는 앞에 두개 빼고 나머지 n-2개 마스킹 - 이렇게 하면 역삼각형 모양의 마스킹 매트릭스가 나옴 - step * step 을 대각선으로 나눈 모양임 example) x = tf.random.uniform((1, 3)) temp = create_look_ahead_mask(x.shape[1]) temp: &lt;tf.Tensor: id=311521, shape=(3, 3), dtype=float32, numpy= array([[ 0., 1., 1.], [ 0., 0., 1.], [ 0., 0., 0.]], dtype=float32)&gt; Special usecase: tf.matrix_band_part(input, 0, -1) ==&gt; Upper triangular part. tf.matrix_band_part(input, -1, 0) ==&gt; Lower triangular part. tf.matrix_band_part(input, 0, 0) ==&gt; Diagonal. :param step_size: :return: &quot;&quot;&quot; mask = 1 - tf.linalg.band_part(tf.ones((step_size, step_size)), -1, 0) return mask # (seq_len, seq_len)def create_masks(inp, tar): # Encoder padding mask enc_padding_mask = create_padding_mask(inp) # Used in the 2nd attention block in the decoder. # This padding mask is used to mask the encoder outputs. dec_padding_mask = create_padding_mask(inp) # Used in the 1st attention block in the decoder. # It is used to pad and mask future tokens in the input received by # the decoder. look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1]) dec_target_padding_mask = create_padding_mask(tar) combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) return enc_padding_mask, combined_mask, dec_padding_mask 3.3. Position-wise Feed-Forward Networks Attention sub-layers 다음엔 FC(Fully connected feed-forward network)가 붙게 됨 two linear transformation with ReLU가 적용됨{: height=”50%” width=”50%”} input and output dim, $dim_{model}$ = 512 inner-layer dim, $dim_{ff}$ = 2048 3.4. Embeddings and Softmax 다른 sequence transduction model과 같이 여기서도 $d_{model}$ 차원을 갖는 learned embeddings을 사용함 learned linear transformation &amp; softmax function을 사용함 two embedding layers, pre-softmax linear transformation에 대해서 weight matrix를 공유함 embedding layer에서 weights에 $\\sqrt{d_{model}}$를 곱해줌 (스케일링) 123x = self.embed(inputs) # (batch, seq, word_embedding_dim)x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))x = self.add_positional_encoding(x) 3.5. Positional Encoding 본 모델에서는 recurrence도 convolution도 없기 때문에 position 정보를 알 수가 없음 그렇기 때문에 position information을 inject해줘야함 “positional encodings”를 input embedding에 더하겠음 (input embedding + positional encodings) {: height=”50%” width=”50%”} 1234567891011121314151617181920212223def add_positional_encoding(self, embed): # ref: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/transformer.ipynb#scrollTo=1Rz82wEs5biZ def get_angles(pos, i, d_model): angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model)) return pos * angle_rates def positional_encoding(position, d_model): angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model) # apply sin to even indices in the array; 2i sines = np.sin(angle_rads[:, 0::2]) # apply cos to odd indices in the array; 2i+1 cosines = np.cos(angle_rads[:, 1::2]) pos_encoding = np.concatenate([sines, cosines], axis=-1) pos_encoding = pos_encoding[np.newaxis, ...] return tf.cast(pos_encoding, dtype=tf.float32) pos_encoding = positional_encoding(self.vocab_size, self.embed_dim) seq_len = tf.shape(embed)[1] return embed + pos_encoding[:, :seq_len, :] 4. Why Self-Attention self-attention과 다른 알고리즘 비교하겠음 대부분은 Self-Attention이 좋음 Complexity빼고! 이 부분은 주변의 r개만 보는 restricted self-attention 버전으로 해결할수 있을듯 {: height=”50%” width=”50%”} 5. Training5.1. Training Data and Batching Data1: WMT 2014 English-German dataset 4.5 million sentence pairs byte-pair encoding source-target vocabulary of about 37,000 tokens Data2: larger WMT 2014 English-French dataset 36M sentences split tokens in a 32,000 word-piece vocabulary sentence length가 비슷한 애들끼리 batch 처리함 각 배치당 25,000 source tokens, 25,000 target tokens 정도를 포함함 5.2. Hardware and Schedule 8 NVIDIA P100 GPUs 사용 base model each training step은 0.4 초 걸림 학습에 사용한 steps or time: 100,000 steps or 12 hours big models 스텝당 1.0 초 걸림, 300,000steps (3.5 days) 소요 5.3. Optimizer Adam $\\beta_1 = 0.9$, $\\beta_2 = 0.98$, $\\epsilon = 10{^-9}$ learning rate 바꿔줌 warmup_steps 에서는 lr이 linearly 증가함 그 후에는 inverse square root of the step number 비율로 감소함 warmup_steps = 4,000으로 셋팅함{: height=”50%” width=”50%”} 5.4. Regularization 3가지 기법 적용함 (왜 논문에는 근데 레벨이 2개밖에 없지..) Residual Dropout 각 sub-layer의 output에 미리 적용해서 나중에 sub-layer input에 더해주고 정규화함 input embedding과 positional embedding을 더한 결과에 대해서도 적용함 (encoder &amp; decoder 모두) $P_{drop} = 0.1$ Label Smoothing label smoothing 적용함 $\\epsilon_{ls} = 0.1$ This hurts perplexity, as the model learns to be more unsure 하지만 Accuracy와 BLEU scroe는 올라감 출처: [36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015. 6. Results6.1. Machine Translation Beam search 적용함 beam size = 4 length penalty $\\alpha = 0.6$ Hyper params는 Development set 기준으로 실험적으로 선택함 Maximum output length = input length + 50{: height=”50%” width=”50%”} 6.2. Model Variations 모델 컴포넌트들의 중요도를 평가하기 위해 varied model에 대해서 평가함{: height=”50%” width=”50%”} 6.3. English Constituency Parsing Transformer가 generalize 잘 되는지 평가함 생각보다 잘 됨{: height=”50%” width=”50%”} 7. Conclusion Attention만 의존하는 모델 처음으로 발표함 recurrent layer를 multi-headed self-attention을 쓰는 encoder-decoder 구조로 대체함 rnn, cnn보다 학습 빨리됨 NMT에서 SOTA 찍음 다른 도메인에도 적용 될수 있을거라 생각함 Acknowledgements ByteNet저자였던 Nal Kalchbrenner와, Stephan Gouws의 comments, corrections and inspiration에 감사함 Attention 시각화{: height=”50%” width=”50%”} {: height=”50%” width=”50%”} {: height=”50%” width=”50%”}","link":"/2019-05-01-Attention_is_All_you_need/"},{"title":"Neural Machine Translation in Linear Time","text":"Author 저자:Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, Koray Kavukcuoglu (Google Deepmind, London UK) 딥마인드 (말 다했다) Who is an Author?{: height=”50%” width=”50%”} Abstract 이 당시엔 Novel architecture였다 (2016년, 후에 Attention is all you need 논문에서도 인용함) ByteNet이라고 부름 {: height=”50%” width=”50%”} one-dimensional CNN이고 Encoder-Decoder구조인데 Decoder를 Encoder위에 Stacking한 구조임 sequence의 temporal resolution 보존 source와 target의 lengths가 다른 걸 잘 처리함 dilation방식(스케일을 넓힐 때 주로 사용함 자세한 내용은 아래 블로그 참고)의 CNN을 encoder(not masked), decoder(masked)에서 모두 사용함 https://m.blog.naver.com/PostView.nhn?blogId=laonple&amp;logNo=220991967450&amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F) {: height=”50%” width=”50%”} Contribution sequence length에 linear한 runtime을 자랑함 (단점은 메모리가 좀 더 많이 필요함, layer를 더 쌓아서 그런건가) character-level LM에서 SOTA기록함 (이전 SOTA였던 RNN 프레임워크 성능 깸) character-to-character MT (English-to-German WMT translation task)에서도 SOTA기록함 (attenton게열의 quadratic run time 을 갖는 RNN 계열 프레임워크의 성능도 깸) 우리가 만든 representation이 토큰간의 latent alignment structure 정보를 갖고 있음을 알아냄 1. Introduction NMT에서 NeuralNet은 주어진 source lang의 시퀀스의 분포로부터 target lang의 시퀀스 분포를 estimate함 Network는 크게 Encoder, Decoder 구조로 구성됨 (Kalchbrenner &amp; Blunsom, 2013) RNN은 시퀀스 모델링에 파워풀하고 (Hochreiter &amp; Schmidhuber, 1997), language modeling에 많이 사용되지만 (Mikolov et al., 2010) 잠재적 단점들이 있음 RNN 구조상 parallel하게 실행이 안됨 Forward &amp; Backword 할때도 path의 full distance에 대한 조건이 요구됨 (전체 path를 다 봐야함) larger distance? -&gt; dependency 배우기 어려워짐 (Hochreiter et al., 2001) 몇가지 네트워크가 계속 제안되었음 encoder-decoder networks networks with attention pooling two dimensional networks 이런 모델들은 성능이 좋지만 running time이 sequence length에 super-linear하고 (실제로 이렇게 적혀있음, linear보다 기울기가 높은 함수를 말함) source sequence를 contant size의 representation으로 바꾸거나 무거운 memorization step을 사용한다든지 하는 단점이 있음 (sequence 길이가 증가할수록 심해짐) 제안 모델은 encoder-decoder 구조를 사용하면서 2가지 방법으로 위에서 말한 단점을 극복하고자함 첫번째는 encoder representation 위에 decoder를 쌓는 것임. 이렇게 하면 시퀀스의 temporal resolution이 보존됨(dependency 고려가 쉬워짐) fixex-size에 source sequence의 representation 저장하는 한계를 어느정도 극복할 수 있게됨 (Kalchbrenner &amp; Blunsom, 2013; Sutskever et al., 2014) 두번째방법은 dynamic unfoldingmechanism을 사용하는 것임 source와 target의 길이가 다른 걸 간단하고 효율적으로 대응하게 해줌 ByteNet(제안모델)에서 사용하는 CNN은 fixed-dpeth형태의 one-dimensional CNN임 The two CNNs (Encoder&amp;Decoder) use increasing factors of dilation to grow the receptive fields Decoder에서 사용된 CNN은 target sequence에서의 future tokens을 보는걸 막기 위해 masking 처리가 되어있음(transformer 아이디어와 비슷하네~ 본래의 이 아이디어는 Pixel recurrent neural networks 논문에서 먼저 사용됨, 본 논문의 저자가 2저자로 참여) (van den Oord et al., 2016b). 이 네트워크는 learning과 computational 관점에서 이점이 있음 computational 관점에서는 source &amp; target sequence 길이에 linear한 runtime을 가짐 encoding 부분과 decoding부분을 training시에 parallel하게 돌릴 수 있음 (Sec2 참고) learning 관점에서는 source sequence의 representation이 resolution preserving됨 단, 메모리는 더 필요함 덕분에 encoder-decoder 사이의 bandwidth도 최대가 됨 본 논문에서는 ByteNet을 character-level langauge modeling과 character-to-character machine translation에 적용함 Hutter Prize Wikipedia task로 decoder network에 대해 평가함 (이 당시 SOTA는 1.31 bits/character임) English-to-German WMT benchmark로 평가함 (이 당시 SOTA는 BLEU score: 22.82(0.38 bits/character), 25.53(0.389 bits/character) on 2014, 2015 test sets) 그 당시 SOTA인 GNMT 모델 보다 더 나은 결과를 보여줌 2. Neural Translation Model NMT model은 다음의 분포를 estimate함 {: height=”50%” width=”50%”} s: source toekns, t: target tokens 토큰은 단어가 될수도, 캐릭터가 될 수도 있음 source network(encoder)와 targer network(decoder)로 source representation을 target string이 되게 만들 것임 문장을 만들어내는 거니까 당연히 decoder가 language model처럼 동작한다고 볼 수 있음 NMT의 basic properties autoregressive: 자기 자신의 값을 사용함 (decoder) source와 target token의 order에 민감함 It is also useful for the model to be able to assign a non-zero probability to any string in the target language and retain an open vocabulary 보통의 NMT가 이러한 basic properties를 갖지만 본 논문에서의 모델은 이것 이외의 properties를 갖게 하고자함 linear run time (parallel computing) size of source representation (fixed -&gt; in the length of the source string); resolution preserving, not constant size 더 짧은 signal path (network 내의 forward, backward signal); 아마 층이 쌓일 수록 멀리 있는 것까지 고려해주는 계산 방식(dilation)이 있어서 그런듯 3. ByteNet [encoder] + [decoder stacked on an encoder] Generate variable-length outputs via dynamic unfolding masking은 decoder 부분에만 적용함 decoder 부분엔 Residual block이 적용됨 3.1. Encoder-Decoder Stacking 제안하는 논문의 특징중 하나가 바로 Encoder와 Decoder를 연결하는 방법이 새롭다는 것 (이 당시 기준) To maximize the representational bandwidth between the encoder and the decoder fixed-size vector 또는 attentional pooling (Bahdanau et al., 2014 including 조경현교수님)과 같은 방법과는 대조된다고 말함 그렇지만 decoder가 encoder의 state까지 참조하는건 아님, encoder output의 representation까지만 참조함 단순히 참조하는 state가 한개가 아니라 여러개로 늘어났다는게 의의가 있을듯.. 3.2. Dynamic Unfolding Encoder와 Decoder가 처리하는 sequence의 length가 다르다면 둘은 directly connected 될 수 없음 이를 해결하기 위해 Dynamic Unfolding이라는 메커니즘을 제안함{: height=”50%” width=”50%”} 원리는 간단함. encoder의 output의 representation이 적당한 길이 |t^|(|s|에 linear relationship)를 갖도록 생성함; source sequence length |s|와 target sequence length |t| 로 적절하게(?) 생성 대게 |t| 보다는 |t^|가 길게 나옴 (a=1.20, b=0으로 논문에서 설정함) {: height=”50%” width=”50%”} 이 적절한 길이의 encoder output representation으로 decoder에서 상황에 맞게(decoding되는 길이에 맞게) 가져다 쓰며 decoding함{: height=”50%” width=”50%”} EOS가 미리 나오면 그 time step까지만 쓰고, |t^|보다 길게 나오면 encoder output representation 없이 EOS 나올때 까지 decoding 진행함 source에 padding을 추가해서 생성함 (논문에 자세히는 안나옴) 3.3 Input Embedding Tensor target sequence에서 처음 n개만 우선 input으로 임베딩시킴 (0~n-1) (1~n)번까지 prediction을 위한 target으로 셋팅 input으로 임베딩된 텐서 + prediction으로 사용되는 tensor가 concat되면서 n x 2d 차원을 가짐 (d == number of inner channels in the network) 3.4 Masked One-dimensional Convolutions decoder에서는 Pixel Recurrent Neural Networks (DeepMind, 2016)에서 사용한 masked one-dimensional convolution을 maksed kernel size *k*로 input embedding tensor에 대해서 적용함 Masking은 future tokens에 대해서 적용함 (prediction시에 current token이 영향받지 않도록!, Loss 계산을 생각해보면 좋을듯) kernel weight를 zero로하거나 input map에 padding처리해서 구현함 3.5 Dilation masked convolution에 적용됨 target network (decoder)의 receptvie field 크기 넓히려고 사용함 Network depth 커질수록, Dilation을 통한 receptive field도 exponentially 커짐 (layer depth에 double로 증가, dilation rates r = 16으로 사용함, 1부터 시작해서 16까지 커짐) 3.6 Residual Blocks 각 레이어는 residual block으로 래핑됨 residual block은 additional convolutional layer가 있고 filter size는 1 x 1 임 본 논문에서는 2가지 버전의 residual blocks을 사용함 하나는 ReLU(NMT에서 많이 씀) 나머지 하나는 Multiplicative Units(Language Modeling에서 많이씀)을 활성함수로 씀 Multiplicative Units(MU)는 Video pixel Networks (Kalchbrenner et al., 2016b; 본인이 썼던 논문) 에서 가져온 개념임 두 버전 모두 다 활성함수 전에 Layer-Norm 사용함 {: height=”50%” width=”50%”} 4 Model Comparison4.1 Recurrent ByteNets 비교를 위해 Recurrent 버전까지 추가하겠음 [1] Decoder를 RNN으로 바꿔보자 [2] Enocder, Decoder 모두 RNN으로 바꾸되 stacked decoder 유지하자 (RNN Enc-Dec 구조와 가장 유사함){: height=”50%” width=”50%”} 4.2 Comparison of Properties Runtime 비교 참고용어 RCTM: Recurrent Continuous translation RP: Resol4ution Preserving Path_S: length from source token to any output target token Path_T: length from input target token to any output target token ByteNet이 linear runtime도 보존 되고, RP도 만족시킴{: height=”50%” width=”50%”} 5. Character Prediction Character-level language modelling benchmark에 대해서 평가함 Hutter Prize version of the Wikipedia dataset 사용 90 million bytes: training 5 million bytes: validation 5 million bytes: testing ByteNet Decoder’s hyper params 30 residual Blocks: 6 sets * 5 blocks 5 blocks’s dillation rates: 1, 2, 4, 8 and 16 masked kernel size: 3 315 characters를 커버 할 수 있는 receptive field가 됨 hidden units d: 512 여기선 Residual Multiplicative blocks 사용 Optimizer: Adam lr: 0.0003 weight decay: 0.001 dropout to the last ReLU layer before the softmax drop_rate: 0.1 a batch of sequences of character: 500 100: for minimum context 400: for prediction Table 3의 결과는 모두 LSTM 기반임 ByteNet이 성능이 제일 좋음 (현재는 24-layer transformer-XL이 SOTA) 링크: http://nlpprogress.com/english/language_modeling.html{: height=”50%” width=”50%”} 6. Character-Level Machine Translation WMT English to German translation Task에 대해서 평가함 Validation: NewsTest 2013 Testing: NewsTest 2014, NewsTest 2015 German character vocab size: 323 English character vocab size: 296 ByteNet’s hyper params &amp; training process 30 residual Blocks: 6 sets * 5 blocks 5 blocks’s dillation rates: 1, 2, 4, 8 and 16 여기선 Residual blocks with ReLUs 사용 hidden units d: 800 kernel size in the source network: 3 maksed kernel size in the target network: 3 Optimizer: Adam lr: 0.0003 각 문장은 special characters로 패딩적용됨; 패딩의 20% 정도는 dynamic unfolding을 적용하기 위해 source sentence에 적용함 학습시 효율적인 배치처리를 위해 패딩된 길이에 맞게 bucketting 사용함 Beam search 사용: beam of size 12 Table2와 Table4에 따르면 ByteNet이 Character-level과 subword-level NMT와 비교할때 성능이 제일 좋음 그러나 word-pieces를 사용하는 GNMT보다는 낮은데 word-pieces는 subword라고 생각안하고 word-level이라고 생각하는듯 {: height=”50%” width=”50%”} {: height=”50%” width=”50%”} Table 5는 ByteNet의 English-German Translation 결과를 보여줌 recodering, trasliteration(단어 그대로 갖다 쓰는거)이 일어나는 특징이 있음 {: height=”50%” width=”50%”} Figure 6는 gradient를 heatmap으로 시각화한건데, 단어는 단어를 구성하는 characters의 gradient를 합치고, 각 컬럼에 대해서 normalization한 것임 두가지 dependency를 표현함 source와 output의 dependency target과 previous target input의 dependency도 보여줌 {: height=”50%” width=”50%”} 7. Conclusion linear running time 갖는 NMT 제안함 RP 및 signal progation path 길이 줄이는 장점도 있음 character-level language model에서 SOTA 찍고 RNN 이김 character-to-character machine translation에서 SOTA 찍음 기대하던대로 tokens간의 alignment도 잘 나옴","link":"/2019-04-12-Neural_Machine_Translation_in_Linear_Time/"},{"title":"BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding","text":"Author 저자:Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (Google AI Language, Google AI니 말다했지) Who is an Author?Jacob Devlin is a Senior Research Scientist at Google. At Google, his primary research interest is developing fast, powerful, and scalable deep learning models for information retrieval, question answering, and other language understanding tasks. From 2014 to 2017, he worked as a Principle Research Scientist at Microsoft Research, where he led Microsoft Translate’s transition from phrase-based translation to neural machine translation (NMT). He also developed state-of-the-art on-device models for mobile NMT. Mr. Devlin was the recipient of the ACL 2014 Best Long Paper award and the NAACL 2012 Best Short Paper award. He received his Master’s in Computer Science from the University of Maryland in 2009, advised by Dr. Bonnie Dorr. {: height=”50%” width=”50%”} 느낀점 Masking 기반의 Language Model과 context 추출을 위한 문장 연관성 (NSP) Task를 동시에 학습시켜서 Rich representation을 얻는다는 아이디어가 참신했음. 두마리 토끼를 한번에..! Bidirectional feature가 상당히 중요함 pre-train 중요함 NSP도 매우 중요함 여기서도 Loss Masking이 중요함 CLS Loss와 LM Loss를 따로 떼서 계산해야함 gelu, masking scheme 썼을때와 안썼을때 성능차이가 꽤 남 segment embedding 처리하는게 은근 귀찮음, 전처리 할때 아예 생성해버리는게 편하긴함 CLS acc 올리기보다 LM acc 올리는게 더 쉬움 Abstract BERT는 Bidirectional Encoder Representations from Transformers의 약자임 최근의 language representation models과 다르게 BERT는 Bidirectional representations을 pre-train하기 위해 디자인됨 결과적으로는 pre-trained BERT representations는 단지 output layer 한개만 추가해도 다양한 영역에서 SOTA를 찍는 fine-tune이 가능함 11개의 NLP Task에서 new SOTA기록하고, SQuAD v1.1 QA에서 사람보다 2.0 높은 F1 성능 기록함 GLUE benchmark에서 80.4% 달성함 기존 것보다 7.6% 향상시킴 1. Introduction pre-trained LM은 예로부터 NLP의 성능을 올리기에 효과적인 방법이었음 Pre-trained Language Representation을 적용하는데는 2가지 전략이 있음 (feature-based and fine-tuning) feature-based: ELMo (Peters al., 2018), 특정 아키텍처를 사용하는데 이때, pre-trained representation이 addtional features로 얻어짐 fine-tuning: GPT (Generative Pre-trained Transformer) (Radford et al., 2018; OpenAI) 기존 연구에선 두 접근법 모두 같은 objective function을 사용함; pre-trainining시에 unidirectional LM이 language representation을 학습하기 위해 쓰는 objective function 본 연구에서는 그러한 현재의 기법이 특별히 fine-tuning approach에서는 pre-trained representation의 power를 매우 제한(serverly restrict)한다고 주장함 주된 한계는 Standard LM이 unidirectional하다는 것임. 이는 아키텍처의 선택을 제한하게됨. 예를들면, OpenAI의 GPT의 경우 left-to-right 구조로써, self-attetnion에서 모든 토큰들이 previous token에만 attention을 갖게됨 (Vaswani et al., 2017) 이러한 제한들은 sentence level에서 sub-optimal에 도달할 수 밖에 없게 함(SQuAD같은 token-level task에서 이러한 구조의 fine-tuning은 안좋을 수 있음(could be devastating)) 결론: Bidirectional 하게 해야함. it is crucial to incorporate context from both directions 본 논문에서는 fine-tuning based approach를 BERT를 제안함으로써 개선시킴! (현재로썬 살짝 GPT에 숟가락 얹은것 같기도..) BERT에서는 기존에 비판했던 objective function을 쓰진 않음(left-to-right 구조에 dependent했던). 대신에 MLM(Masked Language Model; Taylor, 1953)의 objective function을 사용함 MLM은 랜덤하게 input token의 일부를 masking처리 후 그 부분을 예측하는 것을 목표로함. MLM objective allows the representation to fuse the left and the right context (해석보단 원문으로) 본 논문의 contribution은 Bidirectional pre-training for LM by MLM (maksed language models) task-specific architecture에 대한 model engineering 안해도됨. BERT는 fine-tuning based representation model로는 sentence-level, token-level tasks에서 첫번째로 SOTA 찍은 모델임 11개의 NLP Task에서 SOTA 찍었음. 코드도 공개함(https://github.com/google-research/bert) 2. Related work2.1. Feature-based Approaches non-neural과 neural(word2vec)한 방법으로 나뉨 pre-trained word embedding은 learned from scratch로부터 얻은 embedding보다 확연히 개선된 결과를 보였었음 ELMo는 traditional word embeddign research를 different dimension에 따라 일반화시킴 ELMo는 context-sensitive features를 LM으로부터 추출함 contextual word embedding과 task-specific architectures의 결합으로 ELMo는 여러 NLP task(QA on SQuAD, SA, NER)에서 SOTA를 기록함 2.2. Fine-tuning Approaches 최근 트렌드라고 할 수 있음, LM에 transfer learning을 적용하는 것임 LM Objective에 대해서 pre-training 후에 fine-tuning하는 것임 장점중 하나는 few parameter만 다시 learning이 필요하다는것임 이러한 기법을 사용한 OpenAI GPT가 GLUE bechmark에서 SOTA 찍었었음 (Wang et al., 2018) 2.3. Transfer Learning from Supervised Data unsupervised pre-training의 장점은 거의 unlimited한 data를 쓸 수 있다는 것이지만, 최근 supervised task with large datasets로부터 transfer 하는 연구도 제안됨 Natural Language Inference Machine Translation CV에서는 transfer learning이 이미 많이 사용됨 (to fine-tune models pre-trained on ImageNet) 3. BERT 본 섹션에서는 아래와 같은 항목을 다룸 Model architecture input representation pre-training tasks pre-training procedures fine-tuning procedures differences between BERT and OpenAI GPT 3.1 Model Architecture BERT는 multi-layer Bidirectional Transformer encoder를 기반으로 함 (tensor2tensor에 배포된 코드 참고) Transformer 자체는 요즘 어디에서나 쓰임 (the use of Transformer has become ubiquitous) Transformers의 상세한 구조는 본 논문에서 스킵함(다음 링크 참고: http://nlp.seas.harvard.edu/2018/04/03/attention.html) notations $L$ : the number of layers (Transformer blocks) $H$ : the hidden size $A$ : the number of self-attention heads Hyper params in all cases the feed-forward/filter size = 4$H$, i.e,. 4x768 = 3072, 4x1024 = 4096 BERTBASE: $L$=12, $H$=768, $A$=12, ToTal Params=110M BERTLARGE: $L$=24, $H$=1024, $A$=16, ToTal Params=340M BERTBASE는 OpenAI GPT랑 같은 모델 사이즈를 갖게함 BERT Transformer는 Bidirectional self-attention을 쓰고, GPT는 constrained self-attention(left의 context만 볼 수 있음)을 씀 Transformer Encoder: the Bidirectional Transformer Transformer Decoder: the left-context-only version Transformer {: height=”50%” width=”50%”} 3.2 Input Representation BERT의 Input Reresentation은 sigle text sentence와 pair of text sentences([Qeustion, Answer])을 모두 하나의 토큰 시퀀스(one token sequence)에 담아서 처리함 WordPiece embeddings 사용함 (with a 30,000 token vocabulary) (Wu et al., 2016) split word peices 는 ## 으로 처리함 learned positional embeddings 사용함 (sequence lengths up to 512 tokens) 모든 sequence의 첫 토큰은 항상 the speical classification embedding([CLS])로 함. 이 토큰에 대응되는 output vector를 classification할 때 사용함 Sentence pairs는 single sequence에 들어가게 됨. Speical token ([SEP])로 sentences를 분리함 첫번째 sentence에는 every tokens마다 learned sentence A embedding을 더함 두번째 sentence에는 every tokens마다 learned sentence B embedding을 더함 Single-sentence inputs에 대해서는 sentence A embeddings만 사용함{: height=”50%” width=”50%”} 3.3 Pre-training Tasks traditional left-to-right or right-to-left language model 방식으로 BERT를 pre-train 하지 않음 대신에, two novel unsupervised prediction task로 BERT를 pre-train함 3.3.1 Task #1: Masked LM Deep Bidirectional model이 left-to-right model 같은 단방향 모델보다, left-to-right와 right-to-left를 얕게 concat한 모델보다 더 powerful하다는건 reasonable 함 하지만, Standard conditional language models은 left-to-right or right-to-left 처럼 한 방향만 학습이 가능함 왜냐하면 Bidirectional condition은 결국 multi-layered context 안에서 각각의 단어가 자기 자신을 간접적으로 보기는게 가능하기 때문임 (since bidirectional conditioning would allow each word to indirectly “see itself” in a multi-layered context) Deep bidirectional representation을 학습시키기 위해, input tokens을 랜덤하게 특정 비율로 마스킹하고, 마스킹된 토큰을 맞추는 방법을 사용하고자 함(a straightforward approach of mask- ing some percentage of the input tokens at random, and then predicting only those masked tokens) 이러한 procedure를 &quot;masked LM&quot; (MLM)이라 칭하겠음 이는 마치 Cloze task (빈칸 채우기)와 비슷함 mask token에 대응되는 hidden vector를 output softmax로 보내서 계산하는건 Standard LM과 같음 각 sequence안에 있는 All WordPiece tokens에 15%를 마스킹함 denoising auto-encoder (Vincent et al., 2008)처럼 entire input을 reconstruct하기보다, masked word 만 예측함 mismatch between pre-training and finetuning [MASK] token은 fine-tuning시에 안나옴 이 문제를 해결하기 위해 마스킹 해야하는 단어를 항상 [MASK]토큰으로 바꾸진 않음 다음과 같은 방식으로 적용 (이게 LM 성능 향상에 효과가 좋음!!) 80%: Replace the word with the [MASK] token, e.g., my dog is hairy -&gt; my do is [MASK] 10%: Replace thw rod with a random word, e.g., my dog is hairy -&gt; my dog is apple(Noise 주고 정답 맞추게!) 10%: Keep the word unchange, e.g., my dog is hairy -&gt; my dog is hairy 1234567891011if random.random() &lt; 0.8: copy_training_ids[mask_LM_position_index_elm] = tokenizer.piece_to_id('[MASK]') masked_training_ids_batch.append(copy_training_ids)else: # 10% of time, keep original if random.random() &lt; 0.5: masked_training_ids_batch.append(copy_training_ids) # 10% of time, replace with random word else: copy_training_ids[mask_LM_position_index_elm] = list(vocab_word2idx.values())[random.randint(0, len(vocab_word2idx) - 1)] masked_training_ids_batch.append(copy_training_ids) Transformer의 encoder는 어떤 단어를 예측하게 될지, 어떤 단어가 랜덤하게 대체될지는 모름 그렇기 때문에 distributional contextual representation of every input token을 유지할 수 있음 (모든 토큰의 컨텍스트에 대한 분산 표현) MLM의 단점은, 15%정도만 예측에 사용하기 때문에, 모델이 학습을 통해 분포에 수렴하기 위한 pre-training step이 일반적인 LM보다 더 많이 필요함 3.3.2 Task #2: Next Sentence Prediction QA나 NLI (Natural Language Inference)는 두 문장간의 관계를 이해하는것에 기초해 있음 이런 것들은 LM에서는 배울 수 없는 것임 이러한 문장간의 관계를 배우기 위해, binarized next sentence predictiontask를 pre-train 시켜봄 corpus에서 랜덤으로 50%는 임의의 문장, 나머지 50%는 다음문장으로 구성되도록 학습셋을 만듬{: height=”50%” width=”50%”} pre-trained model은 97~98% Accuracy를 기록함 3.4 Pre-training Procedure Corpus BooksCorpus (800M words) (Zhu et al., 2015) + English Wikipedia (2,500M words) Next Sentence Prediction task를 위해 문장 2개씩 샘플링함 50%는 진짜 다음 문장, 나머지 50%는 랜덤한 문장 첫번째 문장엔 $A$ embedding 더하고 두번째 문장엔 $B$ embedding 더함 combined length is $\\leq$ 512 tokens LM masking은 WordPiece tokenization 적용 후 적용됨 (15%) speicial token에 대해서는 partial word pieces 없음 ( &lt;s&gt; 이런 애들은 word piece로 나누지 않았다는 뜻인듯?) Hyper Params Batch Size 256 sentences (256 sequences * 512 tokens = 128,000 tokens/batch) Steps 1,000,000 steps == 40 epochs over the 3.3 billion word corpus Adam lr = 1e-4 $\\beta_{1}$ = 0.9 $\\beta_{2}$ = 0.999 L2 weight decay = 0.01 learning rate warmup : first 10,000 steps and linear decay of the learning rate Dropout prob: 0.1 on all layers Activation: gelu(Gaussian Error Linear Units) activation (OpenAI GPT 따라함, 이게 은근 성능 향상에 효과가 좋음!) 1234567891011121314def gelu(x): &quot;&quot;&quot;Gaussian Error Linear Unit. This is a smoother version of the RELU. Original paper: https://arxiv.org/abs/1606.08415 Args: x: float Tensor to perform activation. Returns: `x` with the GELU activation applied. &quot;&quot;&quot; # ref: https://github.com/google-research/bert/blob/master/modeling.py#L264 cdf = 0.5 * (1.0 + tf.tanh( (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))) return x * cdf Training loss: sum of (the mean masked LM likelihood) and (mean next setence prediction likelihood) GPUs BERTBASE: 4 Cloud TPUs in Pod configuration (16 TPU chips total) BERTLARGE: 16 Cloud TPUs (64 TPU chips total) 4 days to complete 3.5 Fine-tuning Procedure sequence-level classification task: fixed-dimensional pooled representation을 얻기 위해 [CLS] 토큰에 대응되는 final hidden state를 사용함 [CLS] 토큰에 대응되는 output vector를 $C \\in \\mathbb{R}^{H}$라 표현하겠음 약간의 파라미터만 추가되는데, classification layer $W \\in \\mathbb{R}^{K \\times H}$ 만 추가된다고 보면됨. $K$는 classifier의 labels 개수임 Label prob: $P = softmax(CW^{T})$ BERT의 모든 params + $W$가 함께 학습됨 span-level, token-level prediction task: Section 4 참조 대부분의 hyper parms은 pre-training 때와 같지만(dropout은 항상 0.1로~!) 3가지가 다름 Batch size: 16, 32 Learning rate(Adam): 5e-5, 3e-5, 2e-5 Number of epcohs: 3, 4 Dataset 크면 hyper params 선택에 영향 덜 받음 Fine-tuning은 대게 되게 빨리됨, 그러므로 validation set에 대해서 exhaustive search를 해서 최적의 hyper parms을 갖는 모델을 고르는게 좋음 3.6 Comparison of BERT and OpenAI GPT 기존에 존재하는 pre-training 방법과 비교하면, OpenAI GPT가 가장 유사함 GPT는 left-to-right Transfomer LM 사용 Dataset GPT is trained on the BooksCorpus (800M words) BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words) Sentence Separator GPT uses a sentence separator ([SEP]) and classifier token ([CLS]) which are only introduced at fine-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training Batch Size GPT was trained for 1M steps with a batch size of 32,000 words BERT was trained for 1M steps with a batch size of 128,000 words Learning Rate GPT used the same learning rate of 5e-5 for all fine-tuning experiments BERT chooses a task-specific fine-tuning learning rate which performs the best on the development set 4 ExperimentsBERT를 fine-tuning해서 11개의 NLP Task에 적용함 4.1 GLUE Datasets The General Langauge Understanding Evaluation (GLUE) benchmark (Wang et al., 2018)는 여러개의 NLU task의 모음임 MNLI: Multi-Genre Natural Language Inference는 주어진 문장들을 entailment, contradiction, or neutral로 분류 QQP: Quora Question Pairs는 문장이 semantically 일치하는지 여부를 이진분류 QNLI: Question Natural Language Inference는 SQuAD를 binary classification task로 답을 포함한 문장인지 아닌지를 분류 SST-2: The Stanford Sentiment Treebank는 single-sentence에 대한 감성분석 이진 분류 CoLA: The Corpus of Linguistic Acceptability는 single-sentence classification task로써 영어 문장이 언어적으로 acceptable한지 아닌지 분류 STS-B: The Semantic Textual Similarity Benchmark는 문장 페어가 얼마나 유사한지 1~5점으로 스코어링 해놓은 데이터셋임. 의미적으로 얼마나 유사한지 분류 MRPC: Microsoft Research Paraphrase Corpus도 문장페어가 얼마나 의미적으로 유사한지를 판단하는 데이터셋 RTE: Recognizing Textual EntailmentMNLI의 binary 분류 버전 WNLI: Winograd NLI는 natural language inference dataset임 {: height=”50%” width=”50%”} 4.1.1 GLUE Results GLUE Leaderboard가 궁금한다면 클릭 OpenAI, Language-unsupervised fine-tune option batch size: 32 epoch: 3 lr: 5e-5, 4e-5, 3e-5, 2e-5 BERTLARGE의 경우 small dataset에서 finetune 할 경우 unstable한 현상이 있음 BERT는 4.4~6.7% 정도의 견고한 차이로 SOTA보다 높은 결과를 얻음{: height=”50%” width=”50%”} 4.2 SQuAD v1.1 Wikipedia로부터 만든 100k의 question/answer pairs Input paragraph에서 Input Question에 대한 정답찾기{: height=”50%” width=”50%”} Classification Task와는 좀 달리, Start Token $S \\in \\mathbb{R}^{H}$, End Token $E \\in \\mathbb{R}^{H}$찾기임 fine-tuning때는 새로 도입되는 param은 Start, End token뿐임 input token $i$에 대한 Final hidden vector를 $T_{i} \\in \\mathbb{R}^{H}$라 할 때, word $i$가 Start token이 될 확률은 $T_{i}$와 $S$의 dot-product의 softmax로 계산함 $P_{i} = {e^{S \\cdot T_{i}} \\over \\Sigma_{j}e^{S \\cdot T_{j}}}$ End token도 똑같이 적용함 Start, End token의 position 일치 여부에 대한 log-likelihood를 objective로 두고 학습시킴 Hyper params: 3 epoch, 5e-5 lr, 32 batch size START, END token에 대한 vector를 새로 생성해서 따로 갖고 있다가 기존 seq token과 Attention 만 비교하는건가..이러면 self-attention은 아니고 기존에 알고 있던 attention vector를 쓰는 해석인데.. 체크해봐야겠음 SQuAD는 testing procedure가 매우 까다로워서 submitter가 직접 SQuAD organizers에게 컨택해서 hidden test set에 대해 테스트해야함. 본 논문에서는 자체 평가기준으로 best system만 제출함 결과는 TOP을 갱신함. Public data인 TriviaQA도 학습시켰더니 점수 더 높아짐{: height=”50%” width=”50%”} 4.3 Named Entity Recognition CoNLL 2003 NER dataset (200k training words)에 대해서 fine-tune함 Person, Organization, Location, Miscellaneous, Other 로 나뉨 Final hidden representation $T_{i} \\in \\mathbb{R}^{H}$를 NER label set에 대한 classification layer에 feed 시킴 CRF 없음, non-autoregressive임 (이런 상황에서 NER은 사실 굉장히 어려움) BERT는 WordPeiece tokenizer쓰기 때문에 특성상 NER 라벨과 align이 틀어질 수 있음 이를 해결하기 위해 CoNLL-tokenized input word를 WordPiece tokenizer로 다시 tokenization함{: height=”50%” width=”50%”} X 라고 표시된 곳은 prediction하지 않음 (loss를 계산하지 않는건가?) 결과는 ELMo+BiLSTM+CRF 같이 기존의 LSTM 계열의 NER 끝판왕 모델을 이기고 SOTA 찍음{: height=”50%” width=”50%”} 4.4 SWAG 생략 5. Ablation Studies BERT의 요소를 한개씩 분해해서 실험해봄으로써 관계적인 중요성을 이해하고자함 parameter나 pre-training 데이터는 같은거 씀 5.1 Effect of Pre-training Tasks No NSP: “masked LM” (MLM)은 적용했지만, “next sentence prediction” (NSP)는 적용 안함 LTR &amp; No NSP: Left-to-Right (LTR) LM 적용, every input word를 예측하고 masking은 따로 안함. left-only constraint 적용함, fine-tuning때 bidirectional context를 적용하면 성능이 항상 떨어졌기 떄문 (left-only constraint가 뭘까? look-ahead masking 같은건가). NSP도 적용 안함. GPT와 directly하게 비슷함! LTR과 RTL 모델을 각각 학습시키고, ELMo처럼 concat하면 성능 오르지만 비용이 2배가 되고 QA 같은 태스크에서 직관적이지 않고 (RTL이 적합하지 않음) Deep bidirectional model에 비해서 less powerful함 (deep 에서는 left or right context 쓰는 것에 대해서 선택이 가능하기 때문) {: height=”50%” width=”50%”} 5.2 Effect of Model Size Deep하고 params 많은게 좋음{: height=”50%” width=”50%”} 5.3 Effect of Number of Training Steps 정말 100만 스텝이나 필요한가요? Yes!{: height=”50%” width=”50%”} 5.4 Feature-based Approach with BERT 모든 NLP task를 Transformer Encoder architecture로 표현하는게 쉬운건 아님 pre-compute 할 수 있으면 비용이 절약될 것임 ELMo가 contextual representation을 만든것 처럼 BERT도 얼마나 잘 만들었는지 CoNLL-2003 NER Task에 대해서 평가함 BERT의 어떤 param도 fine-tuning하지 않고 two-layer 768-dim BiLSTM을 추가해서 테스트함 BERT 전체를 fine-tune한게 96.4인데 BERT는 건드리지 않고 마지막 Layer 4개를 concat한게 96.1이 나옴 BERT는 fine-tuning해서도 잘 쓰이지만, feature-based approach에서도 효과적임이 입증됨{: height=”50%” width=”50%”} 6. Conclusion Transfer learning with language models이 뜨고 있음, 많이 개선됨 Unsupervised pre-training이 NLU system에 합쳐질 수 있음 Contribution은 deep bidirectional architecture를 갖는 pre-trained model이 generalization 될 수 있음을 보인 것임 실험결과는 매우 뛰어남, 어떤 케이스는 사람보다 잘 함. 앞으론 BERT가 잡아내거나 그러지 못한 liguistic phenomena에 대해서 연구할 것임","link":"/2019-05-09-BERT/"},{"title":"gPRC + Healthcheck 뽀개기","text":"gRPC 개념 설명 Microservices with gRPC gRPC 사용할 때 주의할점 retry 이슈 gRPC는 HTTP2 기반인데, 양방향 통신이라 커넥션을 계속 붙잡고 있는데, 이게 가끔 30분에 한번씩 끊길때가 있다 (뭐가 헤더 크기를 넘어가면서..어쩌구저쩌구 들었던거 같은데 다시 찾아봐야함) 그럴땐 클라이언트 쪽에서 보낸 요청이 fail되고 서버가 못듣게 되는데 단순히 클라가 한번 더 retry해주면 된다. 보통 http2를 쓰는 프로토콜은 retry 로직이 필수라한다 헬스체크 이슈 (ulimit, channel close, Too many open files) grpc는 status 를 제공하고 health check 프로토콜도 제공한다. 어찌다가 try except으로 에러날때 status 코드를 꺼내는 방식으로 꼼수로 구성한적이 있었다..(이럼 안되지.. 이것 때문에 연차썼다가 출근해서 반차처리한 적이..흑흑) 이때 grpc connect을 따로 close해주지 않으면 소켓연결이 쌓이게 되고 리눅스 운영체제에서 file open개수에 대한 ulimit을 초과하면 Too many open files 에러가 뜬다 보통 이런경우 ulimit을 올려주면 되지만, 근본적인 에러원인인 소켓 증가 이유를 찾아야했고 찾다보니 health check때 retry 이슈로 except뜬게 쌓이고 있었다는 결론을 내렸다 결과적으로 connect close를 잘해주자 안그러면 too many file opens 에러뜨니까 gRPC.proto 살펴보기 filename: projectname.proto 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// Copyright 2015 The gRPC Authors//// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);// you may not use this file except in compliance with the License.// You may obtain a copy of the License at//// http://www.apache.org/licenses/LICENSE-2.0//// Unless required by applicable law or agreed to in writing, software// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.// See the License for the specific language governing permissions and// limitations under the License.syntax = &quot;proto3&quot;;option java_multiple_files = true;option java_package = &quot;com.eagle705.demogrpc.proto.projectname&quot;;option objc_class_prefix = &quot;projectname_prefix&quot;;package projectname;// Interface exported by the server.service Chatbot { // A simple RPC. // // Obtains the feature at a given position. // // A feature with an empty name is returned if there's no feature at the given // position. rpc 호출할함수명(input자료구조명) returns (stream output자료구조명) {} // A server-to-client streaming RPC. // // Obtains the Features available within the given Rectangle. Results are // streamed rather than returned at once (e.g. in a response message with a // repeated field), as the rectangle may cover a large area and contain a // huge number of features. // rpc ListFeatures(Rectangle) returns (stream Feature) {} // A client-to-server streaming RPC. // // Accepts a stream of Points on a route being traversed, returning a // RouteSummary when traversal is completed. // rpc RecordRoute(stream Point) returns (RouteSummary) {} // A Bidirectional streaming RPC. // // Accepts a stream of RouteNotes sent while a route is being traversed, // while receiving other RouteNotes (e.g. from other users). // rpc RouteChat(stream RouteNote) returns (stream RouteNote) {}}// 프로퍼티에 입력되는 값을 순서를 의미하는 듯?!(TBD: 확인필요)message input자료구조명 { string 프로퍼티1 = 1; int32 프로퍼티2 = 2; repeated int32 프로퍼티3 = 3; // repeadted는 리스트를 뜻하는 듯 string 프로퍼티4 = 4;}message output자료구조명 { int32 프로퍼티1 = 1; double 프로퍼티2 = 2;} gRPC python file 생성 grpc module이 설치되어 있어야함 pip install grpcio 1python -m grpc_tools.protoc -I./ --python_out=. --grpc_python_out=. ./grpc_modules/projectname.proto 결과: projectname_pb2.py, projectname_pb2_grpc.py 두가지 파일이 생성됨 서버 실행12345678910111213import grpcfrom grpc_modules import projectname_pb2_grpc# gRPC 서버 실행server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))projectname_pb2_grpc.add_ProjectnameServicer_to_server(projectname_engine, server)server.add_insecure_port('[::]:8980')server.start()try: while True: time.sleep(_ONE_DAY_IN_SECONDS)except KeyboardInterrupt: server.stop(0) gRPC python examples Official Repo Examples Health Checking gRPC는 Health Checking도 기존 RPC와 동일하게 핸들링함 Official gRPC Health Checking Protocol Official gRPC Python Health Checking reference: http://blog.naver.com/PostView.nhn?blogId=wideeyed&amp;logNo=221313389714&amp;parentCategoryNo=&amp;categoryNo=&amp;viewDate=&amp;isShowPopularPosts=false&amp;from=postView https://grpc.io/docs/quickstart/python/ https://john-millikin.com/sre-school/health-checking https://github.com/grpc/grpc/blob/master/src/python/grpcio_health_checking/grpc_health/v1/health.py https://github.com/grpc/grpc/blob/master/doc/health-checking.md https://github.com/grpc/grpc/blob/master/src/proto/grpc/health/v1/health.proto https://github.com/grpc/grpc/blob/master/doc/statuscodes.md","link":"/2019-07-12-grpc_healthcheck/"},{"title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing","text":"Author 저자:Taku Kudo, John Richardson (Google, Inc) EMNLP 2018 Official Repo: https://github.com/google/sentencepiece Recommended Tutorial: https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb Who is an Author?{: height=”50%” width=”50%”} 장점 언어에 상관없이 적용 가능 OOV 대처 가능 적은 vocab size로 높은 성능기록 빠름 Note dictionary 형태의 사전은 따로 프로퍼티로 선언되어있진 않음 github issue 참고 Install python module 설치 tf에서 사용가능한 모듈이 따로 있음 (computational graph안에 tokenizer 포함됨) 참고: https://github.com/google/sentencepiece/blob/master/tensorflow/README.md12pip install sentencepiecepip install tf_sentencepiece UsageTraining 전체적인 arg는 아래 그림 참조{: height=”50%” width=”50%”} input은 String이 아니라 문서 파일을 사용함 vocab_size 때문에 에러가 날때가 있음, 실행할 때 에러메세지에서 적합한 vocab_size 알려주니 거기에 맞추면됨 아래와 같이 코드를 실행해주면 sentencepiece tokenizer가 학습이 됨 123456789import sentencepiece as spmtemplates = '--input={} --model_prefix={} --vocab_size={} --control_symbols=[CLS],[SEP] --user_defined_symbols=[MASK] --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'vocab_size = 778prefix = 'm'input_file = './data_in/sentencepiece_train.txt'cmd = templates.format(input_file, prefix, vocab_size)spm.SentencePieceTrainer.Train(cmd) SentencePiece에서는 Custom token을 2가지로 나누는데, Control symbol과 User defined symbols임 Control symbol은 &lt;s&gt;, &lt;/s&gt;와 같은 텍스트를 인코딩하고 디코딩할때 사용하는 특수 토큰임 User defined symbol은 그냥 넣고 싶은거 넣는것임. 얘는 input text에 들어가면 나중에 extract할때 다른 것과 같이 하나의 piece로 인식됨 문서 참고 보통 Control symbol을 많이 쓰기 문에 추가해줘야함 control symbol인 [CLS], [SEP] 토큰을 추가해주기 위해 --control_symbols 옵션을 사용함 user defined symbol인 [MASK] 토큰을 추가해주기 위해 --user_defined_symbols 옵션을 사용함 default control token으로 pad, bos, eos, unk 토큰등이 있음 pad 토큰의 경우 default 값은 비활성화라서 사전의 0번째 인덱스는 보통 &lt;s&gt; 토큰임 우리는 pad 토큰도 쓸거기 때문에 활성화 시켜줘야하는데, 옵션값으로 id를 부여하면 활성화됨 --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3 결과 화면 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980sentencepiece_trainer.cc(116) LOG(INFO) Running command: --input=./data_in/sentencepiece_train.txt --model_prefix=m --vocab_size=778 --control_symbols=[CLS],[SEP] --user_defined_symbols=[MASK] --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3sentencepiece_trainer.cc(49) LOG(INFO) Starts training with :TrainerSpec { input: ./data_in/sentencepiece_train.txt input_format: model_prefix: m model_type: UNIGRAM vocab_size: 778 self_test_sample_size: 0 character_coverage: 0.9995 input_sentence_size: 0 shuffle_input_sentence: 1 seed_sentencepiece_size: 1000000 shrinking_factor: 0.75 max_sentence_length: 4192 num_threads: 16 num_sub_iterations: 2 max_sentencepiece_length: 16 split_by_unicode_script: 1 split_by_number: 1 split_by_whitespace: 1 treat_whitespace_as_suffix: 0 control_symbols: [CLS] control_symbols: [SEP] user_defined_symbols: [MASK] hard_vocab_limit: 1 use_all_vocab: 0 unk_id: 3 bos_id: 1 eos_id: 2 pad_id: 0 unk_piece: &lt;unk&gt; bos_piece: &lt;s&gt; eos_piece: &lt;/s&gt; pad_piece: &lt;pad&gt; unk_surface: ⁇}NormalizerSpec { name: nmt_nfkc add_dummy_prefix: 1 remove_extra_whitespaces: 1 escape_whitespaces: 1 normalization_rule_tsv:}trainer_interface.cc(267) LOG(INFO) Loading corpus: ./data_in/sentencepiece_train.txttrainer_interface.cc(315) LOG(INFO) Loaded all 251 sentencestrainer_interface.cc(330) LOG(INFO) Adding meta_piece: &lt;pad&gt;trainer_interface.cc(330) LOG(INFO) Adding meta_piece: &lt;s&gt;trainer_interface.cc(330) LOG(INFO) Adding meta_piece: &lt;/s&gt;trainer_interface.cc(330) LOG(INFO) Adding meta_piece: &lt;unk&gt;trainer_interface.cc(330) LOG(INFO) Adding meta_piece: [CLS]trainer_interface.cc(330) LOG(INFO) Adding meta_piece: [SEP]trainer_interface.cc(330) LOG(INFO) Adding meta_piece: [MASK]trainer_interface.cc(335) LOG(INFO) Normalizing sentences...trainer_interface.cc(384) LOG(INFO) all chars count=39749trainer_interface.cc(392) LOG(INFO) Done: 99.9522% characters are covered.trainer_interface.cc(402) LOG(INFO) Alphabet size=771trainer_interface.cc(403) LOG(INFO) Final character coverage=0.999522trainer_interface.cc(435) LOG(INFO) Done! preprocessed 251 sentences.unigram_model_trainer.cc(129) LOG(INFO) Making suffix array...unigram_model_trainer.cc(133) LOG(INFO) Extracting frequent sub strings...unigram_model_trainer.cc(184) LOG(INFO) Initialized 5206 seed sentencepiecestrainer_interface.cc(441) LOG(INFO) Tokenizing input sentences with whitespace: 251trainer_interface.cc(451) LOG(INFO) Done! 4681unigram_model_trainer.cc(470) LOG(INFO) Using 4681 sentences for EM trainingunigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=3116 obj=17.083 num_tokens=12289 num_tokens/piece=3.94384unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=2902 obj=15.6584 num_tokens=12336 num_tokens/piece=4.25086unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=2176 obj=16.7119 num_tokens=13290 num_tokens/piece=6.10754unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=2176 obj=16.4999 num_tokens=13299 num_tokens/piece=6.11167unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=1632 obj=18.2899 num_tokens=14896 num_tokens/piece=9.12745unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=1632 obj=17.9621 num_tokens=14925 num_tokens/piece=9.14522unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=1224 obj=19.8661 num_tokens=16922 num_tokens/piece=13.8252unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=1224 obj=19.5698 num_tokens=16937 num_tokens/piece=13.8374unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=918 obj=22.0437 num_tokens=19383 num_tokens/piece=21.1144unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=918 obj=21.6499 num_tokens=19435 num_tokens/piece=21.171unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=0 size=855 obj=22.3546 num_tokens=20096 num_tokens/piece=23.5041unigram_model_trainer.cc(486) LOG(INFO) EM sub_iter=1 size=855 obj=22.2358 num_tokens=20096 num_tokens/piece=23.5041trainer_interface.cc(507) LOG(INFO) Saving model: m.modeltrainer_interface.cc(531) LOG(INFO) Saving vocabs: m.vocab Load model &amp; Encoding, Decoding 학습 후 위키피디아 본문의 일부를 SentencePiece로 tokenization 해봄 default control symbol은 학습할때 넣어주었던 값대로 나옴 SentencePiece에서는 default control symbol을 인코딩시에 text 앞뒤에 추가할 수 있는 옵션이 있음 bos:eos 옵션은 문장에 &lt;s&gt; , &lt;/s&gt; 토큰을 추가함 reverse옵션은 순서를 거꾸로 만들어서 인코딩함 : 표시로 중첩해서 사용할 수 있음 BERT에서는 굳이 쓸 필요 없고, 따로 추가하는 작업을 하는게 맞을 듯 12extra_options = 'bos:eos' #'reverse:bos:eos'sp.SetEncodeExtraOptions(extra_options) SentencePiece tokenizer APIs (나머지는 문서 참조): raw_text-to-enc_text: sp.EncodeAsPieces raw_text-to-enc_id: sp.EncodeAsIds enc_text-to-raw_text: sp.decode_pieces enc_id-to-enc_text: sp.IdToPiece 코드 1234567891011121314151617181920212223242526272829303132333435# Load modelsp = spm.SentencePieceProcessor()sp.Load('{}.model'.format(prefix))print(sp.pad_id()) # 결과: 0print(sp.bos_id()) # 결과: 1print(sp.eos_id()) # 결과: 2print(sp.unk_id()) # 결과: 3training_corpus = &quot;&quot;&quot;초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 &quot;기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것&quot;이다.그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다.인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다.&quot;&quot;&quot;training_corpus = training_corpus.replace(&quot;\\n&quot;, '').split('.')[:-1] # 개행문자제거, 문장 분리training_corpus = [_.strip() for _ in training_corpus] # 문장 앞 뒤의 불필요한 공백 제거# 사실상 extra_options은 쓰지 않아도됨, 각자 추가해야할 듯extra_options = 'bos:eos' #'reverse:bos:eos'sp.SetEncodeExtraOptions(extra_options)training_ids = []for sent in training_corpus: encode_piece = sp.EncodeAsPieces(sent) training_ids.append(sp.EncodeAsIds(sent)) print(&quot;raw text: &quot;, sent) print(&quot;enc text: &quot;, encode_piece) print(&quot;dec text: &quot;, sp.decode_pieces(encode_piece)) print(&quot;enc ids: &quot;, sp.EncodeAsIds(sent)) print(&quot;&quot;)# 사전 구성을 확인하자for i in range(10): print(str(i)+&quot;: &quot;+sp.IdToPiece(i)) 결과 12345678910111213141516171819202122232425raw text: 초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 &quot;기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것&quot;이다enc text: ['&lt;s&gt;', '▁', '초', '기', '▁', '인', '공', '지', '능', '▁', '연', '구', '에', '▁', '대', '한', '▁', '대', '표', '적', '인', '▁', '정', '의', '는', '▁', '다', '트', '머', '스', '▁', '회', '의', '에', '서', '▁', '존', '▁', '매', '카', '시', '가', '▁', '제', '안', '한', '▁', '것', '으', '로', '▁', '&quot;', '기', '계', '를', '▁', '인', '간', '▁', '행', '동', '의', '▁', '지', '식', '에', '서', '와', '▁', '같', '이', '▁', '행', '동', '하', '게', '▁', '만', '드', '는', '▁', '것', '&quot;', '이', '다', '&lt;/s&gt;']dec text: 초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 &quot;기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것&quot;이다enc ids: [1, 7, 656, 22, 7, 43, 669, 30, 776, 7, 668, 81, 14, 7, 88, 16, 7, 88, 218, 54, 43, 7, 53, 9, 10, 7, 20, 82, 294, 41, 7, 290, 9, 14, 60, 7, 199, 7, 160, 637, 51, 19, 7, 123, 181, 16, 7, 667, 202, 25, 7, 77, 22, 101, 17, 7, 43, 247, 7, 139, 119, 9, 7, 30, 176, 14, 60, 50, 7, 337, 11, 7, 139, 119, 38, 91, 7, 65, 125, 10, 7, 667, 77, 11, 20, 2]raw text: 그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다enc text: ['&lt;s&gt;', '▁', '그', '러', '나', '▁', '이', '▁', '정', '의', '는', '▁', '범', '용', '인', '공', '지', '능', '(', 'A', 'G', 'I', ',', '▁', '강', '한', '▁', '인', '공', '지', '능', ')', '에', '▁', '대', '한', '▁', '고', '려', '를', '▁', '하', '지', '▁', '못', '한', '▁', '것', '▁', '같', '다', '&lt;/s&gt;']dec text: 그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다enc ids: [1, 7, 252, 120, 46, 7, 11, 7, 53, 9, 10, 7, 377, 99, 43, 669, 30, 776, 24, 74, 183, 168, 15, 7, 357, 16, 7, 43, 669, 30, 776, 23, 14, 7, 88, 16, 7, 28, 145, 17, 7, 38, 30, 7, 634, 16, 7, 667, 7, 337, 20, 2]raw text: 인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다enc text: ['&lt;s&gt;', '▁', '인', '공', '지', '능', '의', '▁', '또', '다', '른', '▁', '정', '의', '는', '▁', '인', '공', '적', '인', '▁', '장', '치', '들', '이', '▁', '가', '지', '는', '▁', '지', '능', '이', '다', '&lt;/s&gt;']dec text: 인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다enc ids: [1, 7, 43, 669, 30, 776, 9, 7, 116, 20, 439, 7, 53, 9, 10, 7, 43, 669, 54, 43, 7, 89, 208, 36, 11, 7, 19, 30, 10, 7, 30, 776, 11, 20, 2]0: &lt;pad&gt;1: &lt;s&gt;2: &lt;/s&gt;3: &lt;unk&gt;4: [CLS]5: [MASK]6: [SEP]7: ▁8: .9: 의 전체 코드123456789101112131415161718192021222324252627282930313233343536373839404142434445import sentencepiece as spmtemplates = '--input={} --model_prefix={} --vocab_size={} --control_symbols=[CLS],[SEP] --user_defined_symbols=[MASK] --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3'vocab_size = 778prefix = 'm'input_file = './data_in/sentencepiece_train.txt'cmd = templates.format(input_file, prefix, vocab_size)spm.SentencePieceTrainer.Train(cmd)# Load modelsp = spm.SentencePieceProcessor()sp.Load('{}.model'.format(prefix))print(sp.pad_id()) # 결과: 0print(sp.bos_id()) # 결과: 1print(sp.eos_id()) # 결과: 2print(sp.unk_id()) # 결과: 3training_corpus = &quot;&quot;&quot;초기 인공지능 연구에 대한 대표적인 정의는 다트머스 회의에서 존 매카시가 제안한 것으로 &quot;기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것&quot;이다.그러나 이 정의는 범용인공지능(AGI, 강한 인공지능)에 대한 고려를 하지 못한 것 같다.인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다.&quot;&quot;&quot;training_corpus = training_corpus.replace(&quot;\\n&quot;, '').split('.')[:-1] # 개행문자제거, 문장 분리training_corpus = [_.strip() for _ in training_corpus] # 문장 앞 뒤의 불필요한 공백 제거# 사실상 extra_options은 쓰지 않아도됨, 각자 추가해야할 듯extra_options = 'bos:eos' #'reverse:bos:eos'sp.SetEncodeExtraOptions(extra_options)training_ids = []for sent in training_corpus: encode_piece = sp.EncodeAsPieces(sent) training_ids.append(sp.EncodeAsIds(sent)) print(&quot;raw text: &quot;, sent) print(&quot;enc text: &quot;, encode_piece) print(&quot;dec text: &quot;, sp.decode_pieces(encode_piece)) print(&quot;enc ids: &quot;, sp.EncodeAsIds(sent)) print(&quot;&quot;)# 사전 구성을 확인하자for i in range(10): print(str(i)+&quot;: &quot;+sp.IdToPiece(i)) Note vocab_size 이슈를 해결하기 위해 hard_vocab_limit 옵션쪽을 확인해볼 것!12templates = &quot;--input={} --model_prefix={} --vocab_size={} --model_type={} --user_defined_symbols={} --hard_vocab_limit=false&quot; Reference https://lovit.github.io/nlp/2018/04/02/wpm/ https://github.com/google/sentencepiece#redefine-special-meta-tokens https://github.com/google/sentencepiece/blob/master/doc/special_symbols.md https://coffeedjimmy.github.io/sentencepiece/","link":"/2019-05-16-SentencePiece/"},{"title":"Improving Language Understanding by Generative Pre-Training (GPT)","text":"Author 저자:Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever (Open AI, Open AI다! 부럽(?)다) Who is an Author?Alec Radford라는 친군데, GPT논문 인용수가 젤 많겠지 했는데 오히려 Vision쪽에서 한 Generative model 인용수가 넘사임.. 원래 유명한 친구였음 {: height=”50%” width=”50%”} 느낀점 작은 변화가 큰 성능의 변화를 가져다줌 Add auxiliary objective pre-training LM Abstract NLU는 다양한 범위의 태스크를 가짐 ex) textual entailment, qa, semantic similarity assessment and document classification unlabeled data는 엄청 많고 labeled data는 매우 적음 unlabeled data를 잘 쓰면 모델 성능에 도움줄 것임 우리는 generative pre-training of LM에 이를 적용했고 discriminative fine-tuning으로 성능을 높임 이전 방법과의 차이가 뭐냐면, 우리는 효과적인 fine-tuning을 위해 모델의 Architecture는 거의 안바꾸고 input을 바꾸는 방법을 썼음 (task-aware input transformation을 사용함) 우리가 제안하는 방법을 설명하기 위해 NLU의 다양한 태스크에 대한 benchmark를 보여줄 것임 우리의 general task-agnostic model이 특정 task에 특화되게 학습한 모델 보다 성능이 높았고 12개 Task 중에 9개에서 SOTA 찍었음 특히 commonsense reasoning(Stories Cloze Test) 은 8.9% 올랐고 qa(RACE)는 5.7%, text entailment(MultiNLI)는 1.5% 올랐음 1. Introduction raw text로부터 효과적으로 학습하는 능력은 supervised learning에 의존하는걸 완화시켜줄 수 있음 supervised learning을 위해 일일이 손으로 labeling하는건 비용도 매우 비쌈 supervision이 가능하다고쳐도, unsupervised 방식으로 good representation을 학습하는건 performace에 큰 boost를 줌 예를 들자면 word embedding 학습이 있음 word-level information from unlabeled data 이상의 것을 얻어내는 건 두가지 이유 때문에 challenging함 First, 어떤 타입의 optimization objectives이 차후 transfer learing을 위한 text representation을 배우는데 효과적인지 분명치 않음 최근 연구들은 LM, NMT, discourse coherence등 다양한 objectives들에 대해 적용되었음 Second, 가장 효과적으로 학습된 representation을 target task에 대해서 transfer하는 방법에 대해서 합의 된게 없음(no consensus) 최근 연구들은 보통 task가 바뀜에 따라 model Architecture를 바꾸는 바꾸거나, learning objectives를 바꾸거나 하는 방법들을 썼었음 본 논문에서는 Language unserstanding task에 대해서 unsupervised pre-training and supervised fine-tuning을 결합한 semi-supervised approach방법을 제안함 본 연구팀의 목적은 다양한 task에 little adaptation이 가능한 universial representation을 학습하는 것임 처음엔 LM objective 쓸거고 그 다음엔 task에 맞는 supervised objective 쓸 것임 Model backbone은 Trasnformer 쓸 것임 2. Related workSemi-supervised learning for NLP 본 연구는 semi-supervised learning for NLP 범주에 속해 있음 sequence labeling, text classification등에서 많이 쓰임 이전에는 unlabeled data로 word-level or phrase-level statistics를 계산해서 supervised model의 feature로 사용하는 방법이 있었음 몇년 전에는 word embedidngs 같은 것에 대한 연구도 많이 이뤄졌는데, 이건 word-level information에 대해 주로 다루지만 본 연구에서는 higher-level semantics에 대해서도 다루는 것을 목표로함 최근에는 unlabeled corpus로부터 Phrase-level, Sentence-level embeddings에 대한 연구도 많이 이뤄짐 Unsupervised pre-training Unsupervised pre-training은 semi-supervised learning의 speicial case인데, good initialzation point를 찾는 게 목적임 이전의 연구들은 image classificaiton, regression task 등에서 쓰였음 그 후의 연구들은 regularization scheme이나 better generalization을 위해서도 쓰였 최근 연구들은 뉴럴넷 학습을 돕기 위해서 쓰임 (image classificaiton, speech regcognition, entity disambiguation, and machine translation) 여기서 본 연구와 가장 가까운건 train을 돕는 것임; LM objective로 pre-training 후에 target task에 대해서 fine-tuning하는 것 with supervision Auxiliary training objectives Auxiliary unsupervised training objectives를 추가하는 건 semi-supervised learning의 alternative 형태임 그 유명한 Collobert는 POS tagging, chunking, NER, LM등 다양한 auxiliary NLP Task를 사용해서 semantic role labeling의 성능을 올렸 본 연구에서도 auxiliary objective를 사용하지만 이미 unsupervised pre-training 단계에서 여러 언어적 특징들을 학습한걸 확인할 수 있었음 3. Framework 학습은 크게 두 단계로 나눔 Pre-training stage: high-capacity language model을 학습 Fine-tuning stage: labeled data를 통해 discriminative task에 대해서 학습 3.1 Unsupervised pre-training unsupervised corpus of token $U = {u_1, …, u_n}$에 대해서 standard LM objective를 최대로 하는 likelihood는 다음과 같음 $${L_1(U) = \\sum_i logP(u_i|u_{i-k}, …, u_{i-i} ; \\Theta )}$$ Model backbone: a multi-layer Transformer decoder (for LM) $h_0 = UW_e+W_p$$h_l = transformer_block(h_{l-1}) \\forall i \\in [1, n] $$P(u) = softmax(h_nW_e^T)$ Notations $k$ : size of the context window $P$ : $\\Theta$ param을 갖는 neural net으로 모델링 된 조건부 확률; SGD로 학습됨 $U = {u_{-k}, …, u_{-1}}$ : context vector of tokens $n$ : num of layers $W_e$ : token embedding matrix $W_p$ : position embedding matrix 3.2 Supervised fine-tuning Standard LM objective로 학습 후에 supervised target task에 대해서 fine-tuning 해야함 final transformer block’s activation $h_l^m$을 $W_y$ param을 갖는 linear output layer에 한번 더 통과시켜서 $y$ 값을 예측함 $${P(y|x^1, … , x^m) = softmax(h_l^mW_y)}$$ 새로운 objective는 다음과 같음 $${L_2(C) = \\sum_{(x,y)}logP(y|x^1, … , x^m)}$$ 여기에 auxiliary objective를 추가함 fine-tuning할 때 generalization에 도움줌 학습할때 더 빠르게 수렴하게 해줌 최종 objective는 다음과 같음 (with weight $\\lambda$) $L_3(C) = L_2(C) + \\lambda * L_1(C)$ Notations $C$ : a labeled dataset 최종적으로 fine-tuning때 추가되는 extra param은 $W_y$과 delimiter tokens을 위한 embeddings임 {: height=”50%” width=”50%”} 3.3 Task-specific input transformations text classification 같은건 바로 fine-tune되지만, qa, text entailment 같은건 input format 바꿔줘야함 (input transformation) 모든 transformation에는 randomly initialized start and end token ($, $)이 들어감 Textual Entailment: premise(전제) $p$와 hypothesis(가설) $h$ token sequence를 concat하는데 중간에 delimiter token ($)을 넣음 Similarity: 유사문장에는 순서가 의미가 없으므로 AB, BA 순서 모두 input으로 사용하고 중간에 delimiter 넣어줌. 그 후 각각 seq에서 $h_l^m$ 을 얻은 후 element-wise로 합치고 linear output layer에 feed 시킴 Question Answering and Commonsense Reasoning: document $z$, question $q$, set of possible answers {$a_k$}에 대해서 answer의 경우의 수에 맞게 3개를 delimiter 사용해서 concat함 [$z;q;$;a_k$] 각 seq에서 결과 값 구한 후 softmax layer에서 normalized해서 possible answer에 대한 output distribution을 구함 4 Experiments4.1 SetupUnsupervised pre-training Data: the BooksCorpus dataset for training LM contains over 7,000 unique unpublished books contains long stretches of contiguous text the 1B Word Benchmark (alternative dataset) for training LM, which used by a similar approach, ELMo shufffled at a sentence level destroying long-range structure Model specifications Model Backbone: 12-layer decoder-only transformer with masked self-attention heads 768 dimensional staets 12 attetnion heads position-wise feed-forward network 3072 dimensional inner states optim: Adam: lr=2.5e-4 Warmup: 0~2000 step까지는 linear하게 lr을 증가 시키다가 다시 증가 된 값을 0으로 감소시킴 (cosine schedule 사용) epoch: 100 minibatches: 64 randomly sampled token lenghth: contiguous sqeunces of 512 tokens Layernorm: $N(0,0.002)$ token format: BPE dropout: 0.1 (embeddings, residual, attention) Modified version of L2 regularization: $w=0.01$ Activation function: Gaussian Error Linear Unit (GELU) position embedding : use learnable embedding not sinusoidal version Fine-tuning detials Add dropout to the classifier with a rate of 0.1 Use Learning rate of 6.25e-5 and a batchsize of 32 fine-tune speed: 3 epochs use linear learning rate decay schedule with warmup over 0.2% of training; $\\lambda$ was set to 0.5 4.2 Supervised fine-tuning natural language inference, question answering, semantic similarity, and text classification 등의 Task에 대해서 해봄 Natural Language Inference Textual entailment라고도 알려짐 문장 pair를 읽어보고 그 둘의 관계가 (entailment / contradiction / neural) 인지 알아내는 것 SNLI, MNLI, QNLI, SciTail, RTE 등 여러 데이터셋에서 평가 SOTA도 이겼음 Question answering and commonsense reasoning RACE, Story Cloze 등 여러 데이터셋에서 평가 {: height=”50%” width=”50%”} Semantic Similarity Recognizing rephrasing of concepts, understanding negation and handling syntactic ambiguity MRPC, QQP, STS-B 등의 데이터 셋에서 평가 classification CoLa (Corpus of Linguistic Acceptability): 문법적으로 맞는지 판단 SST-2: 감성분석 {: height=”50%” width=”50%”} 결과적으로 12개 데이터셋 중에서 9개가 SOTA 찍음 5. AnalysisImpact of number of layers transferred 여러개의 layer를 transfer learning 할 때 효과에 대해서 관찰해봄 the standard result that transferring embeddings improves performance and that each transformer layer provides further benefits up to 9% for full transfer on MultiNLI. This indicates that each layer in the pre-trained model contains useful functionality for solving target tasks. Zero-shot Behaviors LM pre-training이 왜 효과적인지 알아보고자함 일단 transformer가 다른 애들보다 더 LM을 잘 학습하고 있음 the performance of these heuristics is stable and steadily increases over training suggesting that generative pretraining supports the learning of a wide variety of task relevant functionality {: height=”50%” width=”50%”} Ablation studies w/o pre-training: 성능이 14.8%까지 떨어짐.. ptre-training 매우 중요함 w/o aux LM: pre-training은 했지만 fine-tune할때 LM빼면 데이터 적은 경우에 대해서는 성능 오히려 더 잘나왔지만 데이터 많은 경우에 대해서는 확실히 성능이 좀 떨어짐 (그렇게 큰 차이는 없긴했음) LSTM w/ aux LM: 딴건 다 똑같고 model backbone을 transformer에서 LSTM으로 바꿔쓴ㄴ데 성능이 전체적으로 떨어짐(MRPC만 더 높음) 결론: pre-training도 aux objective(보조적인 목표함수)도 중요하다 {: height=”50%” width=”50%”} 6. Conclusion generative pre-training + discriminative fine-tuning의 힘은 강력했음 12개 dataset 중 9개에서 SOTA 찍었음 본 논문의 연구가 unsupervised learning에 대한 새로운 연구에 도움이 되길 바람(실제로 매우 그렇게 됨) Reference Latex Symbols","link":"/2019-08-14-GPT1/"},{"title":"Research to Production","text":"Research 개발 기간이 필요함 어떤 데이터를 쓸지 어떤 데이터를 모을 수 있을지 어디까지 라벨링 할 수 있을지, 어떤 데이터로 원하는 데이터를 비슷하게 대체할 수 있을지 등을 생각해야함 대략적인 시간을 산정해서 보고해야함 논문을 많이 읽으면 읽을수록 좋긴함 갖다 쓸수있는건 빠르게 갖다 써야함 케글이나 이런데서 빠르게 참조할 필요가 있음 프로토 타이핑은 매우 빠르게~! 개인적으로는 처음부터 좋은 모델보단 baseline부터 서서히 올려가는게 결과를 확인하고 model capacity를 조정하면서 추후 모델 선택할 때 좋음 Speed한 프로토타이핑이 생명 (빠르게 짜는게 중요함, gpu로 학습한다고 노는것도 별로 안좋음) Hyper Params 에 대한 실험 관리 + feature에 대한 실험 관리 도구가 좀 필요함 git 관리를 잘해야함 gitignore 안해본 것에 대한 두려움이 없어야함 DL Framework Prototyping: PyTorch Service: TensorFlow or PyTorch eager mode로 logit + loss 까지 tensor format &amp; shape 확인 graph mode로 학습시켜서 pb 추출 AutoML: 어떤 오픈소스 쓸지 TBD 앙상블까지 고려해야함 Model 관리하는 configuration 부분이 매우 귀찮아질 수 있음 (여러개의 모델을 사용하기 때문에) Data driven Software 될 수 있게 코드단이 아니라 configuration으로 모델의 구조를 변경 할 수 있어야함 (caffe나 claf 처럼) 처음 모델 짤때는 파이프라인+간단한 구조부터해서 구조를 업데이트하는 쪽으로 방향을 잡고 짜야함 모델평가를 쉽게 돌리고 비교할 수 있는 파이프라인..!이 필요함 feature store를 어떻게 구성할지, 실시간 학습 어떻게 구성할지 고민 Production 환경셋팅 문서화 (한번에 뜨고 설치할 수 있게 도커가 그립다) 클래스, 플로우 다이어그램: PlantUML (https://meetup.toast.com/posts/117) 다이어그램 &amp; 마인드맵 그리고 개발하면 좋음 L4 (Load Balancer) L4에서는 1초간 계속 health check를 해서 서버 하나가 꺼지면 떼버리고 다시 살아있으면 붙임 반응못하면 아마 500에러 낼듯 네트워크 프로토콜 패킷 찍어보기 HTTP, HTTP2, gRPC(HTTP2+Protocol buf), status code 등등 체크 Timeout 관리 (conn timeout, read timeout) 서비스로 사용하는 프로토콜의 doc 숙지 HTTP, HTTP2 관련 문서 HTTP2 관련문서2_구글 healthcheck 프로세스 관리 JandiAlert 부하테스트 실서버와 동일한 환경인 Sandbox가 필요함 nGrinder 프로파일링 Network Distillation TC 작성 (testcase) DB 연결 부분은 local에서도 테스트 할 수 있게끔 default value를 하나 만들어줘서 debug할 수 있게 해야함 pylint등으로 개발스타일 통일 로그 관리 파이썬 실행 전체 로그 파일로도 남기기~! python gRPC_server.py &gt; /home/디렉토리/logs/python_logs.log 2&gt;&amp;1 &amp; HTTP protocol에 따른 에러 처리 안해본 것에 대한 두려움이 없어야함 Jandi Alert Code 상용에서는 로그를 남기기 때문에, 모듈별로 테스트할때 로그가 남을 수 있는데 그러면 안됨! 왜냐하면, 로그를 모듈별로 일치시켜줘야하기 때문에~!(ex, 채팅 클라이언트/채팅API/챗봇엔진) 그러므로 로그를 안남기기 위한 API 테스트를 고려해서 인터페이스를 설계해야함 (로그를 안남기거나, 테스트를 위한 로그를 따로 남겨야함) 디펜던시 없는 테스트(DB, API 서버등..과 분리)를 위해 테스트 케이스와 모듈을 만들어놔야함. 그래야 배포때 편함. 서버 실행시 자원 얼마나 소모하는지 모니터링, 체크 패치 프로세스 기록 필요함, 연동 테스트용 코드도. Script화 해놔서 다른 사람이 언제든지 사용할 수 있게 해야함. 123456789101112131415161718192021222324252627282930313233343536373839404142import jsonimport requestsimport linecacheimport sysimport osdef jandi_alert(body, server, webhook_url, api=&quot;name of API&quot;): &quot;&quot;&quot; ref: https://drive.google.com/file/d/0B2qOhquiLKk0TVBqc2JkQmRCMGM/view ERROR_COLOR = &quot;#FF0000&quot;; INFO_COLOR = &quot;#0000FF&quot;; WARNING_COLOR = &quot;#FFFF00&quot;; DEFAULT_COLOR = &quot;#FAC11B; &quot;&quot;&quot; # ref: https://stackoverflow.com/questions/14519177/python-exception-handling-line-number exc_type, exc_obj, tb = sys.exc_info() f = tb.tb_frame lineno = tb.tb_lineno file_name = f.f_code.co_filename linecache.checkcache(file_name) line = linecache.getline(file_name, lineno, f.f_globals) # print 'EXCEPTION IN ({}, LINE {} &quot;{}&quot;): {}'.format(filename, lineno, line.strip(), exc_obj) file_name = os.path.basename(file_name) payload = { &quot;body&quot;: body, &quot;connectColor&quot;: &quot;#FF0000&quot;, &quot;connectInfo&quot;: [{ &quot;title&quot;: &quot;___ 서버 이상&quot;, &quot;description&quot;: &quot;server: {}\\napi: {}\\nfile_name: {}\\nLINE {} '{}': {}&quot;.format(server, api, file_name, lineno, line.strip(), exc_obj) }] } requests.post( webhook_url, data=json.dumps(payload), headers={'Accept': 'application/vnd.tosslab.jandi-v2+json', 'Content-Type': 'application/json'} )","link":"/2019-08-14-research_to_production/"},{"title":"ML Basic - 머신러닝과 확률","text":"Prior &amp; Posterior 사전 확률(prior probability): 관측자가 관측을 하기 전에 시스템 또는 모델에 대해 가지고 있는 선험적 확률. 예를 들어, 남여의 구성비를 나타내는 p(남자), p(여자) 등이 사전확률에 해당한다. 특정 사상이 일어나기 전의 확률을 뜻한다. 선험적 확률은 베이즈 추론에서 관측자가 관측을 하기 전에 가지고 있는 확률 분포를 의미한다. ex) 동전을 던져서 앞면이 나올 확률은 1/2, 특이한 동전은 1/3이다. 사전 확률은 일반적으로 실험하는 대상에 대해 잘 알고 있는 전문가가 선택하거나(informative prior), 혹은 전문적인 정보가 없는 무정보적 분포(uninformative prior)로 주어진다. 사후 확률(Posterior): 사건이 발생한 후(관측이 진행된 후) 그 사건이 특정 모델에서 발생했을 확률 사건 발생 후에 어떤 원인으로부터 일어난 것이라고 생각되어지는 확률 조건부 확률을 통해 사후 확률을 표현할 수 있음 사전 확률과 가능도(likelihood)가 주어졌을 때, 관측자는 관측값을 얻은 다음 베이즈 정리에 의해 사후 확률을 얻을 수 있음 ex) 물건이 불량품이 생산되었을때 A공장에서 생산되었을 확률 $posterior = {likelihood \\times prior \\over evidence}$ MLE &amp; MAP 예시 MLE(Maximum Likelihood Estimation) 방법 MLE 방법은 남자에게서 그러한 머리카락이 나올 확률 p(z|남)과 여자에게서 그러한 머리카락이 나올 확률 p(z|여)을 비교해서 가장 확률이 큰, 즉 likelihood가 가장 큰 클래스(성별)를 선택하는 방법 MAP(Maximum A Posteriori) 방법 MAP 방법은 z라는 머리카락이 발견되었는데 그것이 남자것일 확률 p(남|z), 그것이 여자것일 확률 p(여|z)를 비교해서 둘 중 큰 값을 갖는 클래스(성별)를 선택하는 방법 즉, 사후확률(posterior prabability)를 최대화시키는 방법으로서 MAP에서 사후확률을 계산할 때 베이즈 정리가 이용됨 즉 MLE는 남자인지 여자인지를 미리 정해놓고 시작해서 비교하는거고 MAP는 남자인지 여자인지를 모르는 상태에서 그것이 정해지는 확률까지도 고려해서 비교하는 것임 MAP가 그래서 특정 경우가 정해지는 것에 대한 사전확률을 고려한다고 하는 것임 Maximum Likelihood Estimation (MLE) https://ko.wikipedia.org/wiki/%EA%B0%80%EB%8A%A5%EB%8F%84 Maximum a Posteriori Estimation (MAP) https://ko.wikipedia.org/wiki/%EC%B5%9C%EB%8C%80_%EC%82%AC%ED%9B%84_%ED%99%95%EB%A5%A0 It is very common to use regularized maximum likelihood. MLE vs MAP{: height=”50%” width=”50%”} 최대 사후 확률에 대응하는 모수(Parameter)는 최대우도(MLE)와 마찬가지로 모수의 점 추정으로 사용할 수 있지만, 최대우도에서는 어떤 사건이 일어날 확률을 가장 높이는 모수를 찾는 것에 비해, 최대 사후 확률 모수는 모수의 사전 확률(Prior)과 결합된 확률을 고려한다는 점이 다르다. 한줄 정리: MAP는 MLE에 비해서 Params(성비로 생각하면 편함)로 인해 발생할 사건의 사전확률(성비를 생각하면 편함)을 고려함! MLE보단 MAP 방법이 정확하지만 대부분 Params의 사전확률(성비)을 모르는 경우가 많기 때문에 MLE를 사용함 Params의 사전 확률을 왜 알기 어렵나?? (Blog Reference) 123456789101112131415161718192021영상에서 피부색을 검출하는 문제는 결국, 영상의 각 픽셀이 피부색인지 아닌지 여부를 결정하는 classification 문제로 볼 수 있다.피부색 검출을 위해서는 먼저 샘플 영상들을 열심히 수집해서 피부색 DB와 일반 색상 DB를 구성해야 한다. DB구성이 끝나면 이제 입력 영상의 각 픽셀값이 피부색인지 여부를 베이지언 방법으로 판단해 보기로 하자. 입력 픽셀값이 z라 하면 p(z|피부색)은 피부색 DB에 있는 데이터들 중에서 z와 같은 색을 가진 데이터의 비율을 세면 된다. 또한 p(z|일반색)은 일반색 DB에 있는 데이터들 중에서 z와 같은 색을 가진 데이터의 비율이다.만일 ML로 피부색 검출을 한다면 p(z|피부색)과 p(z|일반색)을 비교해서 확률이 큰 값을 선택하면 될 것이다.그런데, 이 문제를 MAP로 풀면 어떻게 될까? 수집된 DB에 있는 데이터의 개수를 이용하여 p(피부색) = |피부색DB|/(|피부색DB|+|일반색DB|), p(일반색) = |일반색DB|/(|피부색DB|+|일반색DB|)라 놓고 MAP를 적용하면 되는 것일까?대답은 NO!p(피부색)은 세상에 존재하는 모든 이미지 색상들 중에서 피부색이 얼마나 되느냐를 나타내는 말이다. 따라서, 자신이 수집한 피부색 DB와 일반색 DB의 크기만을 가지고 이 확률을 추정하는 것은 무리가 있다. 오히려 일반색 DB에 있는 데이터들 중에서 피부색 DB에 있는 색과 같은 색을 갖는 데이터들의 비율을 p(피부색)이라 잡는 것이 보다 합리적일 것이다.이와 같이 prior 확률 p(x)를 구하는 것은 쉬운 문제가 아니기 때문에 현실적으로는 MAP 대신 ML이 사용되는 경우도 많다. 여기까지 안가도, 정확한 성비를 구하려면 모든 인구의 인원과 성별별로 인원을 구해야되니.. prior를 구하기 어려울 수 있다 하겠다. Q) MAP나 이런게 Neural Net이나 이런 부분에선 어떻게 적용될 수 있는걸까? DL에서는 거의 MLE 쓰는거 같은데..?! 아무래도 이런건 정보이론을 좀 더 공부해야 할듯.. Notation P(A, B): A,B의 Joint Prob Generative vs Discriminative Generative: Joint Prob: P(X, C) = P(X|C)P(C) // Bayes rule로 해결 여러개의 그래프 후보 토픽을 정하고 각 토픽의 분포에 따라 단어가 생성될 확률 계산 평균 분산만 두개 구하면 N 분포 쓸수있고 대부분을 N 으로 가정하니까 예전에 데이터 적을땐 Discriminative로 feature만드는것보다 Generative로 분포 가정해서 사용했음 multi feature를 보기 힘듬..! Discriminative: Conditional Prob: P(C|X) 한개의 그래프 feature등을 통해 class 추측 Probability Theory vs Decision Theory vs Information Theory Probability Theory: Decision Theory: Information Theory: 획득가능한 정보량 = 불확실성 비트로 처음에 연구됨 (2진수) 담아야되는 정보량(로그 밑은 2임) n = -logP(x) = log(1/P(x)) 2진수가 표현가능한 정보량 정보량이란 그 사상에 대해 모르는 정도에 대한 양 etc Generative: HMM (전이확률 + 생성확률) Discriminative: NB (각 feature는 독립으로 보자~) Maximum Entropy: 본 데이터에 대한 확률(feature function(있으면 1 없으면 0으로 나타내고 실제 중요도는 그 앞에 곱해지는 가중치인 alpha값을 학습해서 정함) + 유니폼(엔트로피 최대) MEMM: HMM과 비슷하지만 전이확률 대신 엔트로피로..?! 하지만 Label bias라는 문제가 있음 (길이 없으면 가지마라 라는.. 데이터 없으면 방해될 수 있음) CRF: MEMM 저자가 1년 후 만들어낸 모델임.. Linear Chain CRF가 우리가 아는 CRF임 원래 CRF는 어떠한 클릭(사이클 만족시키지 않는..어떤 조건이 있는데 그걸 만족시키면 클릭임)도 자질로 쓸 수 있다 이전 상태의 feature function의 가중치도 상태에 따라 달라지고.. 전이 확률이 너무 Label bias 문제를 만드니 이걸 하나의 feature로 보면서 영향을 줄여보겠다는게 CRF의 컨셉이라고 할 수 있음 Structured SVM: SVM은 원래 분류만하는데 SVM으로 sequence 라벨링 문제 해결하려고 할때 씀 Examples 가정: 어느 공장이 있다고 가정하자. 공장은 3개가 있다. 목표: $P(Y \\mid X)$ ; 불량품이 생산되었을때 어떤 공장에서 생산되었는지에 대한 확률을 구하는 것 노테이션: A, B, C: 공장 X: 불량인 경우의 클래스 조건: $P(A)$ = 35% : A 공장에서 물건을 생산할 확률이 35%임 $P(A \\cap X)$ = 1% : A 공장에서 생산하고 불량인 확률 $P(B)$ = 20% : B 공장에서 물건을 생산할 확률이 20%임 $P(B \\cap X)$ = 1.3% : B 공장에서 생산하고 불량인 확률 $P(C)$ = 45% : C 공장에서 물건을 생산할 확률이 45%임 $P(C \\cap X)$ = 2% : C 공장에서 생산하고 불량인 확률 이번엔 $P(A \\mid X)$에 대한 값을 구해보도록 하겠다. $P(A \\mid X)$ = ${P(X \\mid A) P(A) \\over P(X)}$ = ${P(X \\cap A) \\over P(A)} P(A) \\over P(X)$ = $P(X \\cap A) \\over P(X)$ 와 같이 정리할 수 있다. 이때, $P(A)$ = 35% 지만 쓸일이 없고, $P(X)$ = (불량 개수 / 전체 생산수) 로 구할 수 있거나 marginal prob로 구할수 있었던것 같다. 아무튼 남은건 $P(X \\cap A)$인데, 이 친구는 독립인경우에 $P(X) * P(A)$로 바꿔서 쓸수있지만 (그렇게 되면 결국 $P(A|B)=P(A)$이다) 여기선 독립이 아니기 때문에 단순하게 곱하기로 하면 안된다. $P(X \\cap A)$ 는 A 공장에서 생산했고 불량인 제품의 확률을 사용해야하므로 위에 정의된 1%를 써야한다.결과적으로 다음과 같다.$P(A \\mid X)$ = $P(X \\mid A) P(A) \\over P(X)$ = ${P(X \\cap A) \\over P(A)} P(A) \\over P(X)$ = $P(X \\cap A) \\over P(X)$ = $0.01 \\over P(X)$ A를 클래스로 해석해서 그렇지 파라미터등으로 해석해서 모델을 찾는걸로 바꾼다면, 위의 값을 Maximize하는것이 중요하기 때문에, $P(X)$ 의 값은 사실상 고려하지 않아도 된다. 저 형태의 값의 크기가 가장 크게 나오는 theta만 찾으면 된다. 기타 Forward Viterbi(Dynamic으로 해결하는데, forwad일때 이전 상태에서 최대값의 확률을 갖는 path를 저장해놓고 나중에 backward할때 다시 계산하지 않고 저장한 path를 사용하면서 해결하는 방식음) N-Best하면 계산량 너무 많으니 계산량 줄이기 위해 beam search 같은거 하는 것..! Shortest path loss function 정리 용어정리: multi-class vs multi-label == 여러 클래스중 1개 맞추기 혹은.. 그냥 클래스가 여러개일 때를 의미 vs 여러 클래스중 N개 맞추기 categorical_crossentropy: one-hot encoding 을 label로 하는 multi-class용 Loss sparse_categorical_crossentropy: class index를 label로 하는 multi-class용 Loss (1개의 클래스에 대해서만 계산하면 되는 sparse한 상황이니까 이걸쓰면 계산상의 이득이 있다로 이해하면 될듯..) binary_crossentropy: multi-hot encoding 을 label로 하는 multi-label에도 사용 가능한 Loss(Sigmoid Cross-Entropy라고도 불리움, Activation이 Softmax가 아닌 Sigmoid기 때문에 다른 확률값에 영향 받지 않아서 multi-label 문제)0,1 을 갖는 index로 하고 싶으면 마지막 차원의 크기를 1로 셋팅하면 됨 (MLP for binary classification참고: https://keras.io/getting-started/sequential-model-guide/) 추가: Multi-hot Sparse Categorical Cross Entropy라는 것도 있다(?)https://cwiki.apache.org/confluence/display/MXNET/Multi-hot+Sparse+Categorical+Cross-entropy 참고: Loss function 설명: https://gombru.github.io/2018/05/23/cross_entropy_loss/ Multi-label image cf 예제: https://github.com/suraj-deshmukh/Keras-Multi-Label-Image-Classification/blob/master/miml.ipynb Eras BCE: https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy Reference MLE &amp; MAP https://hwiyong.tistory.com/27 https://darkpgmr.tistory.com/62 http://sanghyukchun.github.io/58/ https://m.blog.naver.com/PostView.nhn?blogId=ynca333&amp;logNo=221314899811&amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F http://www.synapsoft.co.kr/blog/6002 베이즈 정리 https://ratsgo.github.io/statistics/2017/07/01/bayes/ Generative VS Discriminative http://sanghyukchun.github.io/61/ 옥스포드 자료 (Generative, Discriminative, MLE) http://www.stats.ox.ac.uk/~flaxman/HT17_lecture5.pdf 이 자료가 최종 정리본 CMU! http://www.cs.cmu.edu/~epxing/Class/10701-08f/Lecture/lecture5.pdf 비슷한글 정리한 블로그 https://devkihyun.github.io/study/Machine-learining-and-Probability/ ROC, AUC, True/False Pos/Neg 정리 (매우 잘됨) https://www.waytoliah.com/1222 https://nittaku.tistory.com/297 https://m.blog.naver.com/PostView.nhn?blogId=sw4r&amp;logNo=221015817276&amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F Normal Equation을 풀기 어려운 이유 https://daeson.tistory.com/172","link":"/2019-09-11-MLbasic/"},{"title":"Stochastic Answer Networks for Natural Language Inference (SAN)","text":"Author 저자:Xiaodong Liu†, Kevin Duh and Jianfeng Gao (Microsoft Research, Johns Hopkins University) Who is an Author?Xiaodong Liu 라는 친구인데 꽤 꾸준히 연구활동을 하는 친구인것 같다. {: height=”50%” width=”50%”} 느낀점 turn의 정보를 반영하기에 attention은 필수 하지만 5턴 이상 반영하는건 쉬운게 아님(여기서도 10개까지 했지만 5~6개가 best라고 했음) multi turn을 위한 architecture를 pretrained model를 feature extractor로 써서 결합해서 쓰는게 앞으로의 연구 트렌드가 될 듯 Abstract multi-step 에서 inference를 좀 더 잘하게 해주기 위한 방법을 연구함 그냥 주어진 input에 대해서만 쓰는게 아니라 state를 저장하면서 iteratively refine하면 모델이 잠재적으로 더 복잡한 inference도 가능하게 만들어줄 것임 SNLI, MultiNLI, SciTail, Quora Question Pairs 데이터셋에서 SOTA를 기록함 1. Motivation (intro가 아니라 Motivation이네 특이하군..) NLI, 다른말로 Recognizing textual entailment (RTE)로 알려진 태스크는 sentence pair의 관계를 추론하는 것임 (e.g. premise and hypothesis) 이게 왜 어렵냐면 문장의 의미를 완벽히 이해해야 하기 때문임 (문법적, 요소적인 의미 둘다) 예를들어, MultiNLI dataset 같은 경우엔 premise와 hypothesis간의 정보들에 대해서 multi-step synthesis가 필요함 여러가지 선행 연구들에 따라서 multi-step inference strategies on NLI에 대해 조사해보고자함 2. Multi-step inference with SAN{: height=”50%” width=”50%”} NLI task는 P, H의 관계 R을 찾는게 목표임 관계 R은 entailment, neutral and contradiction 이거 3개로 이루어짐 (연관, 중립, 모순) 기존 single-step에서는 $f(P, H) \\rightarrow R$ 를 만족하는 f를 학습하는게 목표였지만, multi-step에서는 여기에 recurrent state $s_t$ 를 추가하고 이것을 업데이트해서 사용함 제안하는 모델은 기존 MRC multi-step inference literature를 착안함 본 모델에서는 4가지 레이어가 있음 Lexicon encoding layer: computes word representations word embedding이랑 char embedding을 concat 해서 OOV 해결 word랑 char에 대해 각각 position-wise feedforward network 태워서 계산함 outputs: $ E^{p} \\in \\mathbb{R}^{d \\times m} \\text { and } E^{h} \\in \\mathbb{R}^{d \\times n} $ notation: premise가 m개의 토큰, hypothesis가 n개의 토큰임, d는 hidden size Contextual encoding layer: modifies these representations in context maxout layer 사용했고, BiLSTM 썼는데 두 방향에 대해서 concat해서 썼음 outputs: $ C^{p} \\in \\mathbb{R}^{2 d \\times m} $, $ C^{h} \\in \\mathbb{R}^{2 d \\times n} $ Memory generation layer: gathers all informa- tion from the premise and hypothesis and forms a “working memory” for the final answer module Attention Mechanism으로 working memory 구성함 P와 H에 있는 토큰들의 유사성을 측정하기 위해 dot product attention을 사용함 보통 쓰는 scalar norm을 쓰지 않고, Layer projection 해서 사용함. 그래서 notation에 hat이 붙은 것임 $ A=d r o p o u t\\left(f_{a t t e n t i o n}\\left(\\hat{C}^{p}, \\hat{C}^{h}\\right)\\right) \\in \\mathbb{R}^{m \\times n} $ A를 attention matrix라 칭함, dropout 적용되어있음 Information Gathering Layer: premise와 hypothesis의 모든 정보를 모아서 다음과 같이 나타냄 $ U^{p}=\\left[C^{p} ; C^{h} A\\right] \\in \\mathbb{R}^{4 d \\times m} $ $ U^{h}=\\left[C^{h} ; C^{p} A^{\\prime}\\right] \\in \\mathbb{R}^{4 d \\times n} $ notation: ; 은 concatenation을 의미함 ′은 transpose를 의미함 outputs: $ \\begin{array}{l}{ M^{p}=\\operatorname{BiLSTM}\\left(\\left[U^{p} ; C^{p}\\right]\\right) \\text { and } M^{h}=} {\\operatorname{BiLSTM}\\left(\\left[U^{h} ; C^{h}\\right]\\right)}\\end{array} $ Final answer module: predicts the relation between the premise and hypothesis. compute over T memory steps and output the relation label. states와 이전 메모리에 대한 weighted sum값인 x를 feature화 해서 그 스텝에서의 $ P_{t}^r $(확률) 값을 구하고, 실제 inference할땐 이전 스텝의 모든 $ P_{t}^r $ 에 대해 평균 취해서 구함 inintial state $ s_{0} $ $ M^{h}: s_{0}=\\sum_{j} \\alpha_{j} M_{j}^{h} $ $ \\alpha_{j}=\\frac{\\exp \\left(\\theta_{2} \\cdot M_{j}^{h}\\right)}{\\sum_{j^{\\prime}} \\exp \\left(\\theta_{2} \\cdot M_{j^{\\prime}}^{h}\\right)} $ time step t는 {1, 2, …, T - 1} 까지임 $ s_{t}=G R U\\left(s_{t-1}, x_{t}\\right) $ $ x_{t}=\\sum_{j} \\beta_{j} M_{j}^{p} \\text { and } \\beta_{j}=\\operatorname{softmax}\\left(s_{t-1} \\theta_{3} M^{p}\\right) $ 정리하면 initial state는 h에서 꺼내옴 state $ s_t $는 GRU에 이전 state와 $ x_t $ 값을 태워서 만들어내는데, $ x_t $는 premise의 메모리들에 대한 weighted sum 값임. 결국 이전 states와 메모리에 대한 weighted sum 값을 보겠다는 것임 이 weighted sum은 이전 state ($ s_{t-1} $)와 현재 메모리에 $ \\theta_3 $ param을 곱해서 만든 값에 softmax를 한 것임 (여기서 어떻게 다른 값들이 나와서 softmax 를 할 수 있는건지 고민이 되는데, time step에 대한 메모리가 아니라 전체에 대한 메모리 값에 대해서 연산해서 그런듯) t step에 대한 결과 값은 이러한 states와 x값(메모리에 대한 weighted sum) 들을 feature화 해서 softmax 씌움 t step outputs: $P_{t}^{r}=\\operatorname{softmax}\\left(\\theta_{4}\\left[s_{t} ; x_{t} ;\\left|s_{t}-x_{t}\\right| ; s_{t} \\cdot x_{t}\\right]\\right)$ $\\theta_{4}$가 class로 맵핑시켜주는 param일듯..! Each $ P_{t}^{r} $ is a probability distribution over all the relations final output: $P^{r}=\\operatorname{avg}\\left(\\left[P_{0}^{r}, P_{1}^{r}, \\ldots, P_{T-1}^{r}\\right]\\right)$ stochastic prediction dropout 학습 도중에 stochastic prediction dropout이라는 기법을 적용함 (avg ops 전에 적용!) Decoding 때 all outputs에 대해 avg해서 robustness를 개선함 보통의 dropout at the final layer level은 다음과 같은 문제가 있음 Dropout at the final layer level, where ran- domness is introduced to the averaging of predic- tions, prevents our model from relying exclusively on a particular step to generate correct output 새로 적용한 기법은 intermediate node-level에 dropout을 적용함 (이게 무슨뜻일까) 3. Expriments3.1 DatasetSNLI MultiNLI Quora SciTail 3.2 Implementation details The spaCy tool2 is used to tokenize all the dataset PyTorch is used to implement our models word embedding with 300-dimensional GloVe word vectors character encoding, we use a concatenation of the multi-filter Convolutional Neural Nets with windows 1, 3, 5 and the hidden size 50, 100, 150 lexicon embeddings are d=600-dimensions The hidden size of LSTM in the contextual encod- ing layer, memory generation layer is set to 128 the input size of output layer is 1024 (128 * 2 * 4) as Eq 2 The projection size in the atten- tion layer is set to 256 Training weight normalization dropout rate is 0.2 mini-batch size is set to 32 Our optimizer is Adamax learning rate is initialized as 0.002 and decreased by 0.5 after each 10 epochs 3.3 Results Single-step과 multi-step (SAN) 비교 multi-step 비교한 모델이 더 잘함{: height=”50%” width=”50%”} 대부분 잘나왔고 BERT랑 GPT에 좀 밀리는 감이 있지만 적은 파라미터로 잘했다고 저자는 어필함 BERT위에 SAN answer module 얹어서 해봤는데 잘나옴 infernece step은 2보다 5, 6등이 더 잘나옴.. 실험에서는 5로 fix하고 실험함{: height=”50%” width=”50%”} 4. Conclusion multi-step infernece를 위한 방법을 탐색해봄 stochastic answer network (SAN)라는 이름으로 제안함 몇몇 task에서 SOTA 찍음 다음엔 pretrained contextual embedding (ELMo)와 함께 써보거나 GPT내의 multi-task learning중 하나로 적용해볼 생각임 Reference Latex Symbols Latex generation tools","link":"/2019-10-08-SAN_for_NLI/"},{"title":"Universal Language Model Fine-tuning for Text Classification (ULMFiT)","text":"Author 저자:Jeremy Howard, Sebastian Ruder (fast.ai University of San Francisco) Who is an Author? Google Scholar에 안나와서.. Author’s Twitter 느낀점 pretrained model을 범용적으로 쓰려고 시도하려는 시기의 초기 논문인것 같다 저자가 어필을 되게 많이 하는 듯 각 레이어마다 feature가 다르니 다르게 finetune시켜줘야한다는 아이디어가 검증하긴 좀 어렵지만 직관적으론 꽤 설득력있었음. 한편으론 꼭 그래야되나 싶긴하면서도 나쁘지 않았던? warm up등 테크닉이 여기서부터 점점 변형되면서 제안되는 듯 Abstract transfer learning(CV에서 큰 임팩트를 줬던)이 NLP에서는 task-specific modification이 필요하거나 scratch에서 다시 학습해야했음 (2018년이라는걸 기억하자) 이를 개선하기 위해 Universal한 구조로 모델을 만듬 (ULMFiT) 6개의 텍스트 분류문제에서 SOTA 찍고, 18-24% error를 낮춤 오직 100개의 라벨된 데이터로 100배는 많은 데이터로 학습한 모델과 비슷한 성능을 냄 1. Introduction 기존 NLP 에서는 transductive learning (semi-supervised learning)에 집중해왔음 (transductive vs inductive) inductive learning의 예로 pretrained word embedding은 좋은 성과를 냈음 inductive learning의 핵심은 좋은 random initialization이지만 이게 NLP에서 잘 안돼왔음 Dai and Le (2015)은 처음으로 LM을 fine-tune하는 방법을 제안해지만 이 방법을 위해선 수백만의 코퍼스가 필요했음 CV쪽 모델에 비해 NLP쪽은 Shallow한 모델이니 다른 방법의 finetune이 요구됨 Universal Language Model Fine-tuning (ULMFiT) 모델을 제안함 3-layer LSTM architecture로 다른 engineered models를 이김 예를 들면, On IMDb, with 100 labeled examples, ULMFiT matches the performance of training from scratch with 10× and—given 50k unlabeled examples—with 100× more data Contribution: propose Universal Language Model Fine-tuning (ULMFiT), a method that can be used to achieve CV-like transfer learning propose discriminative fine-tuning, slanted triangular learning rates, and gradual unfreezing, novel techniques to retain previous knowledge and avoid catastrophic forgetting during fine-tuning outperform the state-of-the-art on six representative text classification datasets, with an error reduction of 18-24% on the majority of datasets extremely sample-efficient transfer learning and perform an extensive ablation analysis make the pretrained models and our code available to enable wider adoption 2. Related work Transfer learning in CV Hypercolumns Multi-task learning Fine-tuning 3. Universal Language Model Fine-tuning{: height=”50%” width=”50%”} Given static source task $ \\mathcal{T}{S} $ 가 있고 any target task $ \\mathcal{T}{T} $ 가 있다고 할 때, $ \\mathcal{T}{S} \\neq \\mathcal{T}{T} $ 라고 정의 이때 우리의 목표는 $ \\mathcal{T}_{T} $의 성능을 높이는 것이다.(도메인 다르다고 생각하면 됨, 언어모델 태스크 -&gt; 텍스트분류 태스크) a pretrained LM can be easily adapted to the idiosyncrasies of a target 제안하는 모델은 large general-domain corpus에서 LM을 pretrain하고 target task에 대해서 몇가지 기술들을 적용해서 fine-tune함 SOTA LM 모델인 AWD-LSTM (Merity et al., 2017a), a regular LSTM (with no attention, short-cut connections, or other sophisticated ad- ditions) 모델을 사용함 제안 모델을 구성하는 스텝들 일반 도메인에 대해 LM 학습 사용하려는 도메인에 대해 LM 튜닝 사용하려는 도메인에 대해 classifier 붙여서 튜닝 3.1 General-domain LM pretraining LM 학습 데이터: Wikitext-103 (Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words 특징: 단점: While this stage is the most expensive, 장점: it only needs to be performed once and improves perfor- mance and convergence of downstream models 3.2 Target task LM fine-tuning general domain에 대해서 pretrain해도 target data는 다른 분포를 갖고 있음 그러므로 target data에 대해서 fine-tune 해야함 이 작업 수행하면 small dataset에 대해서도 잘 학습됨 본 논문에서는 discriminative fine-tuning and slanted triangular learning rates for fine-tuning the LM 기법을 제안함 Discriminative fine-tuning 다른 레이어는 다른 타입의 정보를 캐치함 (As different layers capture different types of information (Yosinski et al., 2014)) 그러므로 다르게 fine-tune 되야한다고 주장 모든 레이어에 같은 lr 을 적용하는게 아니라 다르게 적용! (실제로 이게 의미 있는지 궁금하긴 하네) 보통 SGD 식은 아래와 같음$$\\theta_{t}=\\theta_{t-1}-\\eta \\cdot \\nabla_{\\theta} J(\\theta)$$ 제안하는 방법의 식은 아래와 같음$$\\theta_{t}^{l}=\\theta_{t-1}^{l}-\\eta^{l} \\cdot \\nabla_{\\theta^{l}} J(\\theta)$$ 모델의 파라미터를 다음과 같이 $\\theta \\text { into }\\left{\\theta^{1}, \\ldots, \\theta^{L}\\right} $ 각 레이어에 해당되는 파라미터로 레이어 notation을 통해 나타낼 수 있음 각 레이어의 모델 파라미터는 각 레이어에 맞는 lr로 업데이트 된다는게 위에서 제안하는 방법임 $ {\\eta^{l-1}=} {\\eta^{l} / 2.6} $ 공식으로 lr을 실험적으로 정함 Slanted triangular learning rates 빠르게 수렴시키기 위해서 고정된 lr을 쓰거나 annealed lr을 사용하는건 best way가 아닐 수 있음 Slanted triangular learning rates(STLR)을 제안함 처음엔 linear하게 증가했다가 추후 작아짐 (warm up이랑 거의 똑같네?) T: num of training iteration cut_frac: the fraction of iterations cut: the iteration when we switch from increasing to decreasing the LR p: the fraction of the number of iterations we have increased or will decrease the LR respectively STLR modifies triangular learning rates (Smith, 2017) with a short increase and a long decay period (CV에서 썼던 기법) $$ \\begin{aligned} c u t &amp;=\\left\\lfloor T \\cdot c u t_{-} f r a c\\right\\rfloor \\ p &amp;=\\left{\\begin{array}{ll}{t / c u t,} &amp; {\\text { if } t&lt;c u t} \\ {1-\\frac{t-c u t}{c u t \\cdot(1 / 1 c u t-f r a c-1)},} &amp; {\\text { otherwise }} \\end{array} \\right. \\ {\\eta_{t}} &amp; {=\\eta_{\\max } \\cdot \\frac{1+p \\cdot(\\text {ratio }-1)}{\\text {ratio}}} \\end{aligned} $$ {: height=”50%” width=”50%”} 7. Conclusion transfer learning에 효과적이고 효율적인 모델인 ULMFiT을 제안함 몇가지 fine-tuneing technique도 제안함 (prevent catastrophic forgetting, enable robust learning) 기존 transfer learning technique보다 낫고, 6개 분류 태스크에서 SOTA 찍음","link":"/2019-10-11-UniversalLMFinetuneforTextClf/"},{"title":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","text":"Author 저자: Victor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF (Hugging Face) (허깅페이스에서 일해보고 싶다) Who is an Author? AAAI를 들고 있는 NLP 하던 분인 듯 Thomas Wolf(huggingface)와 주로 작업하는 듯함{: height=”50%” width=”50%”} 느낀점 일단 논문이 짧다. 좋아. soft target probability로 CE한거랑, MLM, Cosine Embedding Loss만으로 좋은 성적을 얻음 (cosine embedding을 사용한건 기여할만함) 최근 나왔던 MobileBERT처럼 Attention에 자체에 대해서 distillation하지 않아도 나쁘지 않은 결과가 나오는구나 싶긴함 물론 MobileBERT가 더 최신이니 Attention 자체에 대해서도 적용하면 좋겠지만.. 이건 BERT끼리만 가능한 approach니.. weight initialization을 teacher network 에서 가져오는것도 나쁘진 않았음(layer 차이가 나서 좀 다르긴하지만) pre-train도 distillation 쓰고, fine-tune도 distillation 쓰면 잘되는건 알겠음.. 괜찮은 방법이긴한데 여러케이스가 존재할 수 있을것 같아 좀 더 비교가 필요해보임 Abstract NLP에서 large-scale pre-trained models에서 Transfer Learning하는게 매우 보편화됨 그에 따라 computational training or inference가 제한되는 환경(one-the-edge)에서 이런 모델을 돌리는게 challenge가 됨 본 논문에서는 DistilBERT라는 좀 더 작은 general purpose language representation model을 pre-train 하는 방법을 제안하고자 함 대부분의 이전 연구들은 task-specific models을 만드는데 집중되어있었지만, 저자는 pre-training phase때 knowledge distillation하는데 집중함 모델 크기는은 버트대비 40%감소했고 언어 이해 능력은 97%로 유지했고 속도도 60% 더 빠름 pre-training할 때, inductive biases를 larger model로부터 학습하기 위해 3가지를 조합한 loss를 제안함(language modeling, distillation and cosine-distance losses) 제안하는 더 작고, 빠르고, 가벼운(smaller, faster and lighter (smaller와 lighter 차이가 뭐야..)) 모델은 더 적은 비용으로 pre-train도 할 수 있음 capabilities for on-device computation에 대해 proof-of-concetp experiment와 비교할만한 on-device 연구들을 통해 설명할 것임 1. Introduction{: height=”50%” width=”50%”} 지난 몇년간 NLP에서는 Transfer Learning이 아주 핫했고 large-scale pre-trained LM을 쓰는건 NLP Tasks에서 하나의 기본적인 tool이 되어버림 (Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019) 모델 성능은 좋아졌지만 빈번하게 수백만의 파라미터를 갖게되었고 현재까지의 연구로는 더 모델크기를 키우면 downstream task에서 더 높은 성능이 나오고 있음 모델이 커지는건 몇가지 우려를 낳고 있음 첫째는, 환경적 비용이 문제가됨 (First is the environmental cost of exponentially scaling these models’ computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]) 둘째는, on-device에서 real-time으로 실행하면 새로운 application들을 가능케해줄수있을지라도, 계산비용이 증가하고 필요한 메모리가 커져서 다양한 곳에 적용하기엔 어렵게 만듬(Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption) 본 논문에서는 knoledge distillation으로 경량화한 LM 모델도 비슷한 성능을 낼수 있는걸 보임. 경량화헀으니 학습 비용도 적음 3가지의 loss를 써서, bigger Trasnformer LM로 supervision해서 distillation한 결과 40% smaller Transformer를 만듬 (60% 더 빠른 inference time 갖게됨) 2. Knowledge distillation Knowledge distillation (Bucila et al., 2006, Hinton et al., 2015)은 모델을 압축하는 기술임 the student model(compact model)은 the teacher model(larger model or ensemble of models)을 재생산하기 위해 학습됨 지도학습에서 분류모델은 instance class를 예측하기위해 학습되는데, gold label의 estimated probability를 최대로하게 끔 학슴함 보통의 training objective는 모델이 예측한 확률 분포와 one-hot empirical distribution of training labels의 cross-entropy를 최소화하게끔 함 학습셋에서 잘 동작하는 모델은 correct class에 대해서 output distribution을 높은값을 갖는 확률로 예측하고 나머지는 거의 0에 가까운 확률로 예측하게됨 하지만 이러한 “0”(“near-zero”)에 가까운 확률들중 일부는 다른것보다 더 큰데, model의 generalization capbilities를 반영하고 test set에서 얼마나 잘 동작할지를 보여줌 Training loss the student는 soft target probabilities of the teacher에 대한 distillation loss로 학습함 $ t_{i} $는 teacher고 $ s_{i} $는 student라 할때 Loss는 다음과 같음 $$ L_{c e}=\\sum_{i} t_{i} * \\log \\left(s_{i}\\right) \\text { where } t_{i}\\left(\\text { resp. } s_{i}\\right) $$ 이러한 목적함수는 the full teacher distribution을 활용할 수 있어서 충분한 training signal이 됨 Hinton을 따라, softmax-temperature를 사용했음 $$ p_{i}=\\frac{\\exp \\left(z_{i} / T\\right)}{\\sum_{j} \\exp \\left(z_{j} / T\\right)} $$ $ T $는 output distribution의 sommthness를 조절하는 텀이고, $ z_{i}$는 class $ i $에 대한 model score를 의미함 학습시에는 same temperature $ T $를 student &amp; teacher에게 적용하고 inference할때는 $ T $ 값을 1로 셋팅해서 standard softmax를 사용함 the final training objective는 distillation loss $ L_{ce} $를 the supervised training loss와 linear combination한 것임 maksed language modeling loss $ L_{mlm} $ (BERT니까..) cosine embedding loss $ L_{cos} $ (하다보니 발견하게 되었다고 함, student model과 teacher model의 hidden states vectors의 방향을 align 해줌! (오..!)) 3. DistilBERT: a distilled version of BERTStudent architecture the student - DistilBERT는 BERT의 general architecture과 같음 The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2 (레이어수는 2배 감소) Transformer architecture에서 쓰이는 대부분의 Ops인 linear layer나 layer normalisation은 modern linear algebra frameworks로 highy optimized됨 조사결과, last dimension (hidden size dimension)에 대한 변형은 layer 개수 변경에 비하면 computation efficiency에서 큰 임팩트가 없음을 알게됨 그렇기 때문에 레이어를 줄이는 것에 포커스를 맞춤 Student Initialization 최적화와 모델 구조 선택뿐만 아니라, 올바른 initialization 방법을 찾는게 중요함 teacher model과 student model의 dim이 같으니, teacher model에서 빼오자 (Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two) Distillation RoBERTa(Roberta: A robustly optimized bert pretraining approach) 논문에서 나온 버트를 학습시키기 위한 best practices를 적용함 gradient accumulation (up to 4K examples per batch) dynamic masking without NSP objective Data and compute power DistilBERT는 original BERT와 같은 학습 corpus(English Wikipedia and Toronto Book Corpus)를 사용함 8개의 16GB V100 GPU로 90시간 정도 학습함 (RoBERTa의 경우 1024개의 32GB V100으로 하루 학습) 4. Experiments{: height=”50%” width=”50%”} General Language Understanding DistilBERT의 language understanding과 generalization capabilities 를 확인하기 위해 General Language Understanding Evaluation (GLUE) benchmark 로 평가함 표 1에 9개의 task에 대해 정리했고 macro-scroe (average of individual score)도 함께 표시함 ELMo보다 항상 좋고 기존 BERT의 97% 성능을 냄 (40% 적은 파라미터로) 4.1 Downstream task benchmarkDownstream tasks As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.5 points of the full BERT we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation) two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. 한마디로하면, fine-tuning도 fine-tune 된 teacher을 이용해서 하겠다는 것임 결과는 매우 잘됨.. Table 2에서 DistilBERT (D)가 이것임 Size and inference speed 속도에 대한건 Table 3에 정리함 the inference time needed to do a full pass on the STS- B development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1 DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT On device computation on-the-edge applications에서 DistilBERT가 쓸만한지 테스트해보기 위해 QA 모바일앱을 만듬 총 용량을 207 MB까지 낮췄음(quantization 이용하면 더 줄일 수 있음) We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization) 4.2 Ablation study Table 4 presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little impact while the two distillation losses account for a large portion of the performance.{: height=”50%” width=”50%”} 5. Related work Task-specific distillation Tang et al. [2019] transfer fine-tune classification model BERT to an LSTM-based classifier Chatterjee [2019] distill BERT model fine-tuned on SQuAD in a smaller Transformer model previ- ously initialized from BERT Turc et al. [2019] use the original pretraining objective to train smaller student, then fine-tuned via distillation. 저자는 general-purpose pre-training distillation이 task-specific distillation보다 좋다고 주장(it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation.) Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. However, as shown in the ablation study, leveraging the teacher’s knowledge with initialization and additional losses leads to substantial gains. Other compression techniques Pruning: Recent developments in weights pruning reveal that it is possible to remove some heads in the self-attention at test time without significantly degrading the performance Michel et al. [2019]. Quantization: Some layers can be reduced to one head. A separate line of study leverages quantization to derive smaller models (Gupta et al. [2015]) 6. Conclusion and future work DistilBERT를 소개함 BERT의 genenral-purpose pre-trained version이며 40% 더 작고, 60% 더 빠르면서, 97%의 the language understanding capabilities를 유지함 general-purpose language model이 distillation을 통해서도 성공적으로 학습이 됨을 밝히면서 각 components를 ablation study로 분석함 edge applications에서도 설득력있는 옵션이라는걸 입증함 (약간 더 개선이 필요해보이는데..) Codehttps://github.com/huggingface/transformers/blob/7edb51f3a516ca533797fb2bb2f2b7ce86e0df70/examples/distillation/distiller.py#L366","link":"/2019-11-28-DistilBERT/"},{"title":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","text":"Author 저자: Zhenzhong Lan, Sebastian Goodman, Piyush Sharma Radu Soricut (Google Research) Mingda Chen, Kevin Gimpel (Toyota Technological Institute at Chicago) Who is an Author? 원래는 CV를 위주로 하던 친구인데 이번에 NLP꺼도 해본듯 (CVPR도 들고 있고..) 논문 인용수도 꽤 됨 Google VR팀에서도 인턴했었음{: height=”50%” width=”50%”}http://www.cs.cmu.edu/~lanzhzh/ 느낀점 간단한 아이디어인데 실험을 엄청 많이 해놔서 paper를 만든느낌 실험이 의미는 있지만 직관적으로 예측가능한 결과임 간단한 아이디어도 사실 예전부터 적용되어야 했음 (weight sharing, decomposition) transformer 논문이 처음에 pretraining용이 아니다보니 당시 그 논문에서 빼먹었지만 당연히 앞으론 적용되었어야할 아이디어가 2년이 지나서야 적용된 느낌 SOP가 NSP보단 Good이다 SOP 할때 문장 단위가 아니라 textual segments로 한거 괜찮았음 (SEP도 그러면 segment단위로 넣겠네) MLM 을 n-gram masking 한건 좀 신기하네 나쁘지 않음 transformer에서 dropout을 없애는게 pretraining할 때 진짜 좋은지는 좀 더 검증해봐야할 듯 이 논문은 모델 그림이 없다(?) Abstract Natural langauge representation을 pretraining을 통해 학습시키려 할때 모델 사이즈를 키우는건 성능향상을 하는데 도움을 줌 하지만 특정 포인트 이상으로 모델 사이즈가 커지는건 GPU/TPU 메모리 크기의 제한, 학습시간이 길어짐, 예상치 못한 model degradation등으로 인해 문제가 될 수 있음 이런 문제를 해결하기 위해 적은 메모리 사용과 BERT의 학습 속도를 높이기 위한 two parameter-reduction techniques을 제안함 실험적인 증거들은 제안하는 방법이 orginal BERT보다 더 낫다는 걸 보여줌 추가로, self-supervised loss를 추가헀는데 이는 inter-sentence coherence를 modeling하는데 초점을 맞췄음 결과적으로 BERT-large에 비해 적은 parameter를 갖고도 GLUE, RACE, SQuAD benchamrk에서 SOTA를 기록함 1. Introduction Full network pre-training은 langauge representation learning 에서 새로운 breakthroughs를 기록해왔음 (Dai &amp; Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard &amp; Ruder, 2018) 많은 NLP tasks들이 학습 데이터가 제한되어있는데 이런 pre-trained model은 매우 큰 benefit을 주었음 그중 하나는 중국 중고등학교 영어 시험에 대한 reading comprehension task (RACE test)임 (reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017)) (저자가 중국인이라 그런가..이걸 첫번째 예시로 드네) 논문에서 발표할땐 SOTA 44.1% acc였음 최근 논문에서는 SOTA 83.2% acc 본 논문에서 제안하는 모델은 89.4% acc로 SOTA가 되었고 pretrained language representations은 큰 도움이 되었음 large network이 SOTA를 위해선 매우 중요했지만 이젠 real applications을 위해서 경량화하려는게 흔한 practice가 됨 (It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications) 모델 사이즈가 중요해졌기에, 이런 research question을 던짐 Is having better NLP models as easy as having larger models? 이런 질문에 대한 한 가지 장애물은 하드웨어의 메모리 제한임 SOTA 모델은 보통 millions 또는 billions of params을 가짐 분산학습 할때 param 개수가 많으면 거기에 비례해서 커뮤니케이션 오버헤드도 더 커지기 때문에 학습 속도도 느려짐 모델의 hidden size를 BERT-large 처럼 단순히 늘려버리면 성능도 안좋아지는것 또한 발견함 (Figure 1, Table 1 참고) 이러한 문제들은 모델 병렬화와 메모리 관리로 어느정도 해결가능하지만 communication overhead와 model degradation 문제는 해결할 수 없음 {: height=”50%” width=”50%”} {: height=”50%” width=”50%”} 본 논문에서는 이러한 문제 해결을 위한 A Lite BERT (ALBERT) 구조를 디자인했음 (훨씬 적은 param을 사용) ALBERT는 two parameter reduction techniques을 사용함 factorized embedding parameterization large vocabulary embedding matrix를 두개의 작은 matrices로 decomposing함 we separate the size of the hidden layers from the size of vocabulary embedding. 이렇게 분리하면, vocabulary embeddings의 parameter size를 크게 늘리지 않아도 hidden size를 더 쉽게 늘릴 수 있음 cross-layer parameter sharing layer 간에 param을 share함으로써 network의 depth가 깊어져도 parameter가 증가하는걸 막을 수 있음 위의 두가지 techniques으로 심각한 performance 손상 없이 param를 줄일 수 있었음 BERT-large와 비슷한 configuration으로 ALBERT는 18배 적은 param을 갖고 학습도 1.7배 빨랐음 param을 줄이는 기술은 Regularization 처럼 동작해서 학습도 안정적으로 할 수 있게 해주고, Generalization에도 도움을 줬음 더 나아가, ALBERT의 성능을 더 높이기 위해, sentence-order prediction (SOP)를 위한 self-supervised loss를 제안함 inter-sentence coherence에 포커싱을 맞춤 BERT에서 사용된 NSP loss가 효과가 없다는 걸 보이기 위해 사용함 (Yang et al., 2019; Liu et al., 2019) 이러한 방법들로 ALBERT configuration보다 더 scale up이 가능하게 되면서도 동시에 BERT-large 보다는 fewer param이 가능하게 되었음. 하지만 성능은 더 좋음. benchmark the RACE accuracy to 89.4% the GLUE benchmark to 89.4 the F1 score of SQuAD 2.0 to 92.2 2. Related work2.1 Scaling Up Representation Learning For Natural Language natural langauge에서 representation을 학습하는 것이 매우 유용하다는건 다양하게 보여져 왔음 (e.g. word2vec) (Mikolov et al., 2013; Le &amp; Mikolov, 2014; Dai &amp; Le, 2015; Pe- ters et al., 2018; Devlin et al., 2019; Radford et al., 2018; 2019) 지난 2년간 가장 큰 변화는 pre-training word embeddings이나 contextualized embedding 패러다임에서 full-network pre-training 을 통해 task-specific fine-tuning하는 패러다임으로 넘어갔다는 것임 이러한 연구의 연장선에서, 모델의 크기가 크면 성능이 향상된다는 것들도 보여져왔음 하지만 기존 연구들은 hidden size를 1024로 셋팅한데 까지만 연구했고 본 논문에서는 2048로 늘려보았지만 성능이 더 악화되었음 그러므로 representation learning에서 scaling up 하는 것은 단순히 모델 사이즈를 증가시키는것 만큼 쉬운 것은 아님 추가로 computational constraints 때문에 실험하기도 어려움 Chen et al. (2016) 연구에서는 gradient checkpointing이라는 기법으로 memory requirements를 줄임 Gomez et al. (2017) 연구에서는 next layer로부터 이전 layer의 activation을 reconstruction 하는 방법을 제안해서 intermeidate activations을 저장하지 않아도 되게함 두 방법 모두 메모리 사용은 감소시켰지만 속도가 느림 제안하는 방법은 속도도 빠르게하고 paramter-reduction으로 메모리 사용도 줄임 2.2 Cross-Layer Parameter Sharing Transformer 구조 자체는 standard encoder-decoder task를 다뤘지 pretraining/finetuniong setting을 염두해둔건 아님 기존의 연구들은 corss-layer parameter sharing (Universal Transformer, UT)이 LM과 subject-verb agreement에 도움이 됨을 보여줌 (Dehghani et al. (2018)) 최근 Bai et al. (2019) 가 제안한 Deep Equilibrium Model (DQE) 모델에서는 input embedding과 output embedding이 특정 layer에서 equilibrium point에 도달할 수 있다는걸 보여줌 (무슨 말이죠..) 본 논문에서 발견한 것은 이와 다름 (Our observations show that our embeddings are oscillating rather than converging) 2.3 Sentence Ordering Objectives ALBERT는 pretraining loss중 하나로 두개의 연속된 text 세그먼트의 순서를 맞추는걸 사용함 담화의 coherence와 coheision은 이전에도 많은 연구가 이뤄졌었음 (Coherence and cohesion in discourse have been widely studied and many phenomena have been identified that connect neighboring text segments (Hobbs, 1979; Halliday &amp; Hasan, 1976; Grosz et al., 1995)) Skip-thought (Kiros et al., 2015) and FastSent (Hill et al., 2016) 처럼 sentence embeddings들은 문장 주변의 단어들을 예측하도록 sentence를 인코딩하는 방식을 사용했음 다른 objectives로는 단지 주변이 아닌 미래의 문장들을 예측하거나 discourse markers를 예측하는 방법으로 sentence embedding을 학습하기도 했음 (Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019)) 본 논문에서 사용하는 방법은 Jernite et al. (2017)에서 제안된 sentence ordering objective와 비슷함 대부분의 above work와는 달리 본 논문에서 사용하는 loss는 textual segments 단위임. sentence가 아니라. BERT에서 사용된 NSP와 비교해서 SOP이 더 challenging pretarining task라는걸 확인함 본 논문과 거의 동시에 Wang et al. (2019) 논문도 SOP를 다뤘는데 차이점은 본논문에서는 binary classificaiton인데 저기서는 three-way classification임 (어떤 종류로 클래스를 나눴을까) 3. The Elements of ALBERT3.1 Model Architecture Choices ALBERT의 backbone은 BERT와 비슷함 (transformer encoder with GELU nonlinearities) 구조에 있어 3가지의 contribution은 다음과 같음 Factorized embedding parameterization BERT, XLNet, RoBERTa 등에서 WordPiece embedding size $ E $는 hidden layer size $ H $ 와 묶여있다. i.e., $ E \\equiv H $ (왜냐하면, attention 자체가 dimension 자체를 줄이는게 아니라 weighted sum 개념이기 때문에 input dim -&gt; output dim이 되기 때문) 이러한 선택은 modeling 과 practical reasons에 있어서 suboptimal임 modeling 관점: WordPiece embeddings은 context-independent representations를 학습하는 것임 반면, hidden-layer embeddings은 context-dependent representation을 학습하는 것임 BERT-like representation의 power는 context-dependent representation에서 옴 WordPiece embedding size $ E $를 hidden layer size $ H $와 분리하는 것은 더 효율적인 모델 파라미터 사용으로 이어질 수 있음 ($ H \\gg E $) practical 관점: NLP에서 vocab size $ V $ 는 굉장히 큼 만약 $ E \\equiv H $ 라면 $ H $ 가 증가하면 size of embedding matrix도 증가하게됨 ($ V \\times E $) 결과적으로 매우 쉽게 billions of params을 갖게됨 하지만 실제 업데이트되는 건 적음 (update sparsely) 그러므로 본 논문에서는 factorization을 사용해서 embedding params을 two smaller matrices로 분리함 one-hot을 바로 hidden space로 보내는게 아니라 embedding space로 보낸 후에 hidden space로 projection함 (사실 너무나 당연한거 아닌가..확실히 transformer의 naive한 초기셋팅이 안고쳐지다가 여기에 와서야 고쳐지는군) $$O(V \\times H) \\text { to } O(V \\times E+E \\times H)$$ 이러한 parameter reduction은 $ H \\gg E $ 일때 매우 중요함 Cross-layer parameter sharing 다른 여러 방법들이 있었지만 ALBERT에서는 layer간 모든 parameter를 공유함 본 논문에서의 관찰과는 다른 결과를 보였던 선행연구들 Universal Transformer (UT)와 Deep Equilibrium Models (DQE)에서도 비슷한 전략이 있었지만 UT 논문에서는 UT가 vanilla Transformer보다 높은 성능을 보여줌 (본논문에서 실험한 결과와는 다름) DQE도 equilibrium point가 있다고 했지만 본 논문의 실험에서는 embeddings이 수렴하기보단 oscillating 하게 보였음 layer를 올라갈 수록 ALBERT가 BERT보다 더 smooth하게 값이 바뀌는걸 볼 수 있는데 이건 weighty-sharing이 network parameters의 stabilizing에 효과가 있음을 보여줌 DQE와는 다르게 절대 0으로 수렴하는 현상이 보여지지 않는걸로 봐서 ALBERT의 solution space가 DQE와 매우 다르다는걸 알 수 있음 {: height=”50%” width=”50%”} Inter-sentence coherence loss MLM loss에 추가해서 BERT에서는 NSP loss를 사용했음 하지만 그간의 연구들 (Yang et al., 2019; Liu et al., 2019)은 NSP가 unreliable하다 판단했고, 지워버렸음 (실제로 downstream task performance 결과도 그랬음) 본 논문에서는 NSP가 효과가 없는 것이 MLM에 비해 어렵지 않은 태스크기 때문이라고 추측함 NSP는 topic prediction이나 coherence prediction task의 결합으로 볼 수 있음 (topic으로 볼수 있는 또 다른 이유는 문장을 랜덤으로 뽑을때 다른 문서에서 뽑음) 하지만 topic prediction은 coherence prediction에 비해 비교적으로 학습하기 쉽고 MLM loss로 학습해서 배우는 것과 비교적으로 더 오버랩됨 langauge understanding 관점에서 inter-sentence modeling은 중요하기 때문에 을 유지시키기로 함 대신 coherence에 기반을 둔 loss를 제안하기로 했고 sentence-order prediction (SOP) loss 를 사용하기로 함 topic prediction을 피하고, 대신 inter-sentence coherence에 집중함 positive examples은 BERT와 같이 같은 문서에서 연속된 2개의 segments를 사용 negative examples은 동일한 two consecutive segments를 순서만 바꿔서 사용함 (동일한걸 쓰면 의미가 있나.. 약간 난이도를 낮춘건가..segment embedding에 대한 값이 매우 중요하게 학습되는 결과만 낳을거 같은데) 이러한 task는 모델이 discourse-level coherence properties에 대해 finer-grained distinctions을 학습하게 함 NSP는 SOP를 거의 하나도 못풀지만, SOP는 NSP를 reasonable한 수준에서 해결 할 수 있는 것으로 나타남 결과적으로 ALBERT는 multi-sentence encoding task에 대한 downstream task performance를 개선할 수 있었음 3.2 Model Setup BERT와 ALBERT의 모델을 hyperparameter setting으로 비교해봄 {: height=”50%” width=”50%”} ALBERT-large는 BERT-large에 비해 18배 적은 params을 가짐 (18M vs 334M) Hidden size를 2048까지 하면 BERT-xlarge는 1.27 billion params을 가짐 반면에 ALBERT는 60M밖에 안함 ALBERT-xxlarge 모델의 경우 12-layer network에 대한 결과를 기록함 24-layer network은 결과는 비슷한데 computationally more expensive해서 기록 안함 4. Experimental Results4.1 Experimental Setup BERT와 비교하기 위해 비슷하게 셋팅함 Data: (for pretraining, 16GB) BookCORPUS English Wikipedia Format: $ [\\mathrm{CLS}] x_{1}[\\mathrm{SEP}] x_{2}[\\mathrm{SEP}], \\text { where } x_{1}=x_{1,1}, x_{1,2} \\cdots \\text { and } x_{2}=x_{1,1}, x_{1,2} \\cdots $ length: 512 Noise: randomly generate input (10%) Vocab: 30000 (tokenized using SentencePiece) MLM: n-gram masking (Joshi et al., 2019) 사용 each n-gram mask의 length는 랜덤하게 선택되고, 길이 n에 대한 확률은 다음과 같음 $ p(n)=\\frac{1 / n}{\\sum_{k=1}^{N} 1 / k} $ (길이가 길면 선택될 확률이 낮다?!) maximum length of n-gram: 3 (이렇게하면 “White House correspondents” 과 같은 단어도 잡을 수 있음) Training: Batch size: 4096 Optim: LAMB Lr: 0.00176 (You et al., 2019) Steps: 125,000 Machine: Cloud TPU V3 (# of TPUs used for training ranged from 64 to 1024) 4.2 Evaluation Benchmarks4.2.1 Intrinsic Evaluation 학습 과정을 모니터링 하기 위해 SQuAD와 RACE로부터 dev set 만들어서 테스트함 (Deview때 네이버가 보여준거랑 똑같네) MLM과 sentence classification task의 accuracy를 모두 확인함 모델이 잘 학습되서 수렴하는지만 확인하려고함 4.2.2 Downstream Evaluation GLUE (the General Language Understanding Evaluation) SQuAD (the Standford Question Answering Dataset) RACE (the ReAding Comprehension from Examinations dataset) dev set에 대해서 early stopping 해서 학습시킴 4.3 Overall Comparison Between BERT and ALBERT with only around 70% of BERT-large’s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%) We also observe that BERT-xlarge gets significantly worse results than BERT-base on all metrics. This indicates that a model like BERT-xlarge is more difficult to train than those that have smaller parameter sizes. Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. {: height=”50%” width=”50%”} 4.4 Factorized Embedding Parameterization the effect of changing the vocabulary embedding size E using an ALBERT-base configuration setting Under the non-shared condition (BERT-style): larger embedding sizes give better performance, but not by much. Under the all-shared condition (ALBERT-style): an embedding of size 128 appears to be the best {: height=”50%” width=”50%”} 4.5 Cross-Layer Parameter Sharing experiments for various cross-layer parameter-sharing strategies, using an ALBERT-base configuration (Table 2) with two embedding sizes (E = 768 and E = 128) compare all-shared strategy (ALBERT-style) hurts performance under both conditions but it is less severe for E = 128 (- 1.5 on Avg) compared to E = 768 (-2.5 on Avg) the not-shared strategy (BERT-style) intermediate strategies in which only the attention parameters are shared (but not the FNN ones) sharing the attention parameters results in no drop when E = 128 (+0.1 on Avg), and a slight drop when E = 768 (-0.7 on Avg) only the FFN parameters are shared (but not the attention ones) most of the performance drop appears to come from sharing the FFN-layer parameters other strategy divide the L layers into N groups of size M , and each size-M group shares parameters the smaller the group size M is, the better the performance we get. However, decreasing group size M also dramatically increase the number of overall parameters. We choose all-shared strategy as our default choice {: height=”50%” width=”50%”} 4.6 Sentence Order Prediction (SOP) compare head-to-head three experimental conditions for the additional inter-sentence loss none (XLNet- and RoBERTa-style) NSP (BERT-style) SOP (ALBERT-style) (base config) the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the “None” condition) NSP ends up modeling only topic shift{: height=”50%” width=”50%”} 4.7 Effect of Network Depth and Width check how depth (number of layers) and width (hidden size) affect the performance of ALBERT the performance of an ALBERT-large configuration Layer 개수로 실험: 12 layer 이상부터는 큰 성능차이 없음{: height=”50%” width=”50%”} Hidden size로 실험: 4096 정도가 괜찮았음 (왜 3-layer로 한거지..){: height=”50%” width=”50%”} 4.8 What if we train for the same amount of time? Table 3에서는 BERT-large 가 ALBERT-xxlarge 보다 3.17배 빠름 보통 학습을 오래할수록 성능도 좋아짐 데이터의 epoch을 맞춰서 실험하기보다 절대 시간을 맞춰서 실험해보기로 함 (comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours)) BERT로 34시간동안 400k step돌린 것과 ALBERT로 32시간동안 125k 돌린것이 얼추 시간이 비슷하니 성능을 비교해보기로함 (the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training)) 결과는 ALBERT-xxlarge가 BERT-large보다 확실히 좋음 (ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%) {: height=”50%” width=”50%”} 4.9 Do very wide ALBERT models need to be deep(er) too? Hidden size가 크면 네트워크도 더 깊게 쌓아야되는지 실험함 H=1024인 경우에 12-layer와 24-layer차이가 크지 않았음 (sec 4.7) H=4096인 경우는 또 다를수 있으니 테스트해봤는데 결과는 큰 차이가 없게 나옴 {: height=”50%” width=”50%”} 4.10 Additional training data and dropout effects pretraining data는 Wikipedia와 BookCORPUS만 썼는데, XLNet이나 RoBERTa에서 썼던것 처럼 추가 데이터를 쓰면 어떤 효과가 있는지 알아보고자함 additional data가 있을때 확실히 MLM이 좋아짐 downstream task도 좋아졌지만 SQuAD는 나빠짐 SQuAD는 Wikipedia-based라서 out-of-domain training material의 영향을 받았을거라 추측 1M steps을 학습시켜도 모델이 training data에 overfit이 안되서 dropout을 없애버림 dropout 없애니 MLM이 더 잘됨 As a result, we decide to remove dropout to further increase our model capacity. (왜 드랍아웃 없앤게 model capacity를 올리는거지.. 약간 다른개념 아닌가.. 학습이 더 잘되서 저렇게 말한건가 -&gt; 학습 잘되면 오버핏도 잘되고 == model capa 높으면 오버핏도 잘되고 == 드랍아웃 없으면 오버핏도 잘되고) {: height=”50%” width=”50%”} dropout을 빼니 downstream 결과도 더 좋아짐 실험적으로 (Szegedy et al., 2017) 그리고 이론적으로 (Li et al., 2019) batchnorm과 dropout을 CNN에 쓰면 성능이 더 떨어진다는 결과도 있음 (combination of batch normalization and dropout in Convolutional Neural Networks may have harmful results) 예측하기로는 transformer 구조에서 dropout이 performance를 떨어뜨리는거 같은데 다른 transformer based model에서도 검증이 필요할 것 같다고함{: height=”50%” width=”50%”} 4.11 Current state-of-te-art on NLU tasks dev set 성능 기준으로 선택함 The single-model ALBERT: the best-performing settings discussed: an ALBERT-xxlarge configuration (Table 2) using combined MLM and SOP losses, and no dropout the final ensemble model ALBERT: the number of checkpoints considered for this selection range from 6 to 17, depending on the task single model과 ensemble 모두 SOTA 기록 GLUE score of 89.4 SQuAD 2.0 test F1 score of 92.2 RACE test accuracy of 89.4 RACE test의 경우 크게 좋아짐 (jump of +17.4% absolute points over BERT (Devlin et al., 2019), +7.6% over XLNet (Yang et al., 2019), +6.2% over RoBERTa (Liu et al., 2019), and 5.3% over DCMI+ (Zhang et al., 2019)) single model의 기존 SOTA의 ensemble보다 좋다고 어필{: height=”50%” width=”50%”} {: height=”50%” width=”50%”} 5. Discussion ALBERT-xxlarge 모델이 BERT-large보다 params이 적고 성능도 더 좋지만 computationally more expensive함 앞으로는 spare attetnion (Child et al., 2019)나 block attention (Shen et al., 2018)등으로 ALBERT의 infernece speed를 높이는게 중요해보임 representation을 위해 더 나은 LM 학습 방법이 필요해보임 langauge representation에서는 SOP가 훨씬 더 유용한걸 확인함 더 다양한 self-supervised training losses가 있을 것임","link":"/2019-11-11-Albert/"},{"title":"Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond","text":"Author 저자: Mikel Artetxe (University of the Basque Country (UPV/EHU)) Holger Schwenk (Facebook AI Research) Who is an Author?Mikel Artetxe 라는 친구인데 주로 번역쪽 태스크를 많이 한 것 같고 조경현 교수님하고도 co-author 이력이 있음. 페북에서 인턴할때 쓴 논문임. {: height=”50%” width=”50%”} 느낀점 결국 이 논문도 parallel corpus가 필요하다고함. 이걸 통해 multilingual sentence embedding을 얻는 것임 Translation이 되게 학습시켜서 encoder를 훈련함 대신에 그 양이 좀 적어도 다양한 언어에 대해서 얻을 수 있게 하는 것 영어로만 transfer learning 시켰는데도 다른언어도 적용된다는 점은 의미있음 encoder가 BPE를 통해 language independent하게 모델링했다는게 좀 의미가 있긴한데 한편으로는 universal한 구조다보니 좀 개별언어에 대해서 성능이 최적화되진 않겠다는 생각(이지만 논문에선 결과가 괜찮음) language ID로 decoder에 언어정보를 주는건 꽤 괜찮은 아이디어였다고 생각 parallel corpus alignment하는거 어떻게하니.. 고생이 눈에 훤함 (꼭 다할 필요가 없다고 했지만서도) 이번 논문은 약간 Scaling 으로 승부한 케이스인것 같음 (제목 자체가 그렇지만) Scaling을 키워서 실험할 줄 아는것도 결국 연구자의 역량..이라면 인프라가 중요하고 인프라가 중요하다면 configuration 잘하는건 기본이고, 실험비가 많거나 회사가 좋아야(?) 너무 스케일 싸움으로 가는것 같은 논문을 보면 왠지 모르게 아쉽고 씁쓸하다(?) 보통 transfer랑 one-shot, few-shot 등의 용어가 나오는데 fine-tune 안한다고해서 zero-shot이라고 한듯 Language-Agnostic 라는 용어: 언어에 구애받지 않는다라는 뜻 BERT 등 최신 논문과도 비교했지만(1년이 지났으니 최신이라고 이제 할수있을지..) 본 논문의 기법 자체는 좀 옛날 기법이라는 생각이 듬 논문의 설명이 잘나와있으나 몇가지 좀 생략되어있음 (은근 불친절한) Abstract 93개의 언어에 대해 joint multilingual sentence embedding representation을 학습하는 아키텍처를 제안함 single BiLSTM encoder에 shared BPE vocab을 사용함 (cover all language) auxiliary decoder와 결합시켜서 parallel corpora에 대해 학습시킴 이러한 방식으로 English annotated data만 사용해서 분류기를 학습시킨 후 93개 언어에 대해 모델 구조 변경없이 transfer가 가능하게 함 실험에 사용한 데이터셋에서는 의미있는 결과를 얻었음 cross-lingual natural language inference (XNLI dataset) cross-lingual document classification (ML- Doc dataset) parallel corpus mining (BUCC dataset) 112개의 언어가 aligned setence되어있는 새로운 테스트셋(Tatoeba)도 소개함 적은 언어 자원으로도 multilingual similarity search가 꽤 잘나오는 sentence embedding을 얻은 것을 보여줌 trained encoder &amp; test set: https://github.com/facebookresearch/LASER 1. Introduction 딥러닝 나와서 NLP가 발전했지만 이런 방법은 data hungry하고 많은 현실적인 시나리오에서 응용되기에 제약이 있음 여러 인기있는 방법들은 이런 이슈를 없애려했고, 그중 첫번째가 unlabeled data로 general langauge representation을 만드는 것임 가장 대표적인게 word embeddings (Mikolov et al., 2013b; Pennington et al., 2014) 최근엔 sentence-level representation에 대해서 연구가 이를 대체했음 ex. BERT (Peters et al., 2018; Devlin et al., 2019) 이런 연구들은 각 언어에 대해 따로 모델을 학습시킴 그러므로 다른 언어들에 대해 연관된(?) 정보를 얻을 순 없음(these works learn a separate model for each language and are thus unable to leverage information across different languages) (BERT의 multilingual도 결국 따로따로 학습한거라서 안된다고 지적하는건가) low-resource language에 대해서 성능에 잠재적 제약이 있음 본 논문에서는 universal language agnostic sentence embeddings 을 제안함 input langauge와 NLP task에 general한 vector representation Motive 제한된 언어 자원을 가질때, 다른 언어들과 joint training을 통한 benefit이 있게 하기 위함 특정언어에서 다른 언어로 zero-shot transfer 를 하기 위함 code-switching 을 핸들링 하기 위함 (Robust하게 만들자는 뜻인가) 이러한 동기때문에, single encoder로 multi langauge를 handling하도록 다른 언어가 embedding space에서 가까워지도록 학습시킴 93개의 언어 대해 학습한 single pre-trained BiLSTM encoder로 어떠한 fine-tuning 없이 XNLI, MLDoc, BUCC, 그리고 새로운 multilingual similarity search 데이터셋에 대해서 매우 의미 있는 결과를 얻음 여러가지 태스크에 대해 다룬 ‘massively’ multilingual sentence representation으로는 첫번째 시도라고 주장 2. Related work single langauge word embeddings (Mikolov et al., 2013b; Pennington et al., 2014) 이후 사람들이 continuous vector representation 학습에 관심 갖게됨 sentence embeddings unsupervised 방법으로 대량의 corpora에서 RNN encoder 로 학습 skip-thought model of Kiros et al. (2015) Multilingual representation cross-lingual word embeddings parallel corpora에서 jointly 학습 (Ruder et al., 2017) 각각 언어에 대해서 학습 후 bilingual dictionary안에서 shared space로 맵핑 (Mikolov et al., 2013a; Artetxe et al., 2018a) 좀 더 괜찮은건, seq2seq encoder-decoder architecture! (Schwenk and Douze, 2017; Hassan et al., 2018) end-to-end on parallel corpora에서 학습 어떤 연구에서는 언어마다 encoder 다르게 해야한다고 했지만 그냥 언어에 상관없이 encoder share해도 괜찮은 결과 나왔음 하지만 대부분의 결과는 적은 언어자원을 가진 언어에 대해서는 한계가 있음 기존의 large number of langauges에 대한 multilingual representation 연구는 word embeddings, typology prediction, machine translation 등의 영역에서 한계가 있음 대부분의 sentence embedding에대한 선행 연구는 fixed-length representation을 학습하는 거였음 최근엔 variable-length representation을 다루고 더 강력한(?) 좋은 결과를 냄 (contextualized embeddings of word!!) (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) -&gt; BERT (사실 결국엔 하나의 벡터로 들어가는걸 보면 fixed length라고도 볼수 있을거같은데 context를 봐서 variable-length라고 하는건가.. 근데 이전 RNN seq2seq도 context를 본다고 할수도있을거같은데 음..한번에 다보는거랑 이전꺼에 의존하는거랑 좀 다르다고 봐야되나) 이러한 이유로, RNN or self-attentional encoder를 unlabeled corpora에 대해서 학습시킴(LM) classification 할 때는 top layer 하나 (붙여서) fine-tune해서 씀 제안하는 방법은 task-specific fine-tuning이 필요없음 3. Proposed method{: height=”50%” width=”50%”} langauge agnostic BiLSTM encoder 사용 (to build sentence embeddings) auxiliary decoder와 묶어서 parallel corpora에 대해 학습함 우리가 결국 사용하려는건 인코더고 디코더는 인코더 학습을 위한 보조적인 용도로만 쓰겠다는 것 3.1 Architecture 본 구조는 Schewenk (2018) 논문의 모델을 기반으로함 sentence embedding은 BiLSTM output에 대해 max-pooling해서 얻음 sentence embedding에 W를 곱해서(linear transformation) LSTM decoder에 init hidden 값으로 사용함 input 값에 대해서도 매 time step마다 sentence embed를 concat해서 사용함 Note: relevant information을 sentence embed에서만 얻게하려고 encoder와 decoder간의 connection은 주지 않음(그래서 사실 사뭇 옛날 모델의 구조와..같다는 생각) encoder, decoder는 모든 언어에 대해서 share 됨 (기존 연구중에는 각각 다르게 하는 연구가 있었음, 어떻게 다르게 하는지는 논문봐야알듯) encoder는 어떤 langauge인지 모르게 하자 모든 언어에 대해 training corpora를 concat해서 joint byte-pair encoding (BPE) vocab을 얻었고 50k operation정도 사용했음 BPE를 통해 encoder는 language independent representation을 학습할 수 있게 되었다고함 (vocab의 중요성인가) decoder에서는 어떤 langauge인지 알 수 있게 하자 decoder에서는 langauge ID를 embedding해서 input에 concat함 특정 언어를 생성해낼 수 있게하기 위해서 Scaling up to almost 100 langauges for an encoder! encoder stacked layer: 1 to 5 each dim: 512 sentence embed representation dim: 1024 decoder one layer dim: 2048 input embed size: 320 language ID embed: 32 3.2 Training strategy 기존 연구에서는 each input sentence가 모든 언어에 대해서 번역되게 했음 (Schwenk and Douze, 2017; Schwenk, 2018) 하지만 이런 방법은 scaling up할때 두가지의 명확한 단점이 있음 N-way parallel corpus가 필요함 (모든 언어에 대해서 번역하니까) language 개수에 대해 quadratic cost가 발생함 (학습도 느려짐) 제안 방법은 2개의 target langauges로도 비슷한 성능을 냄 Note that, if we had a single target language, the only way to train the encoder for that language would be auto-encoding, which we observe to work poorly. Having two target languages avoids this problem. 제안 방법은 N-way parallel corpus 조건을 각각 언어간의 alignments 조합 개수만큼만 필요하도록 완화시킴 (이 말이 정확한가) 학습 스펙 Loss: cross entropy! alternating over all combinations of the langauges involved. Optim: Adam lr: 0.001 dropout: 0.1 implementation: based on fiarseq gpus: 16 NVIDIA V100 GPUs batch size: 128,000 tokens epcohs: 17 days: 5 3.3 Training data and pre-processing{: height=”50%” width=”50%”} 3.2에서 2개의 target languages를 정하자고 했으니 English와 Spanish로 해보겠음 대부분의 데이터를 위 두가지 언어에 대해서 aligned 처리함 Note that it is not necessary that all input languages are systematically aligned with both target languages. Once we have several languages with both alignments, the joint embedding is well conditioned, and we can add more languages with one alignment only, usually English. 93개 언어에 대한 학습데이터는 the Europarl, United Nations, OpenSubtitles2018, Global Voices, Tanzil and Tatoeba corpus 를 조합해서 만듬 학습을 위해 총 223 million parallel sentences를 구성함 전처리: Moses tools 사용 (대부분의 언어) punctuation normalization removing non-printing characters and tokenization Jieba and Mecab 사용 (Chinese, Japanese) It is important to note that the joint encoder itself has no information on the language or writing script of the tokenized input texts. It is even possible to mix multiple languages in one sentence. 4. Experimental evaluation English sentence representation에 대한 evaluation frameworks는 잘되어있지만 multilingual sentence embeddings에 대해서는 스탠다드한 평가방법이 없음 그래도 가장 영향력있다고 여겨지는게 XNLI dataset임 (Conneau et al., 2018b) 영어를 14개 언어에 대해서 테스트함 BERT를 baseline으로 함 추가로 corss-lingual document classification 에 적용해봄 MLDocs, BUCC 하지만 이 데이터셋이 93개의 언어를 커버하지못하니 내가 112개의 언어에 대응되는 테스트셋 만들어서 테스트하겠음 (이런식으로 말을 풀면 자기가 만든 테스트 셋을 벤치마크로 쓸수 있구나) 4.1 XNLI: cross-lingual NLI 데이터셋 결과 Notation중에 EN -&gt; XX가 있는데, 이것 때문임. we train a classifier on top of our multilingual encoder using the English training data{: height=”50%” width=”50%”} Given two sentences, a premise and a hypothesis, the task consists in deciding whether there is an entailment, contradiction or neutral relationship between them Dataset development: 2,500 test: 5,000 translated from English into 14 languages by professional translators multilingual encoder위에 classifier하나 놓고 two sentence embedding에 대해 ($p, h, p \\cdot h$,|$p-h$|) 와 같이 feature로 바꿔서 분류함 All hyperparameters were optimized on the English XNLI development corpus only the same classifier was applied to all languages of the XNLI test set two hidden layer 사용: concat_sent_dim -&gt; 512 -&gt; 384 -&gt; 3 Swahili 같은 자원이 적은 언어에 대해서 잘나옴 BERT 는 영어에 대해서는 매우 훌륭한 점수를 냄 (transfer는 약함) Translation은 약간 다른 방법으로 테스트하는 것임 test set을 영어로 번역해서 영어로 NLI 하거나 train set을 각 언어로 번역해서 각 언어에 맞게 NLI함 이건 multilingual embedding 테스트가아니라 MT system과 monolingual model 퀄리티 평가하는 것임 (Note that we are not evaluating multilingual sentence embeddings anymore, but rather the quality of the MT system and a monolingual model) (굳이 왜 넣었나 싶긴한데 그냥 번역해서 쓰는것보다 적은 데이터에 대해선 multilingual embedding이 성능이 좋다는걸 비교해서 나타내고 싶었던게 아닐까함) {: height=”50%” width=”50%”} 4.2 MLDoc: cross-lingual classification{: height=”50%” width=”50%”} Schwenk and Li (2018) 논문에서 제안되었는데 Reuters benchmark의 개선된 버전이라고함 Dataset for each language, divided in 4 different genres training: 1,000 development: 1,000 test: 4,000 encoder의 top layer에 10 units 갖는 hidden layer 한개 쌓아서 사용 we train a classifier on top of our multilingual encoder using the English training data 4.3 BUCC: bitext mining{: height=”50%” width=”50%”} Dataset: 150K to 1.2M sentences for each langauges Given two comparable corpora in different languages, the task consists in identifying sentence pairs that are translations of each other 말이 identifying이지 extracting이라고 보면됨 (검색해서 점수 높은 것 뽑음) score sentence pairs by taking the cosine similarity of their respective embeddings parallel sentence는 threshold를 넘는 cosine similarity를 스코어로해서 nearest neighbor retrieval 로 찾아냄 (어려울듯) 이러한 방법이 scale inconsistency issues (Guo et al., 2018) 때문에 문제가 있다고 해서 Artetxe and Schwenk (2018) 논문에서 새로운 score 방법이 제안됨 $score(x, y) = margin(\\cos (x, y), \\sum_{z \\in \\mathrm{NN}{k}(x)} \\frac{\\cos (x, z)}{2 k}+\\sum{z \\in \\mathrm{NN}_{k}(y)} \\frac{\\cos (y, z)}{2 k})$ $ \\begin{array}{l}{ \\mathrm{NN}_{k}(x) \\text { denotes the } k \\text { nearest neighbors of } x} {\\text { in the other language. }}\\end{array} $ margin functions에 대해서 여러개를 테스트 해봤는데 ratio가 젤 결과가 좋았음 ratio: $ \\operatorname{margin}(a, b)=\\frac{a}{b} $ 본 논문에서는 위의 metric으로 평가했음 (결과가 저정도면 이상하다 싶을정도로 결과가 잘나온것 같긴함) 4.4 Tatoeba: similarity search 93개 언어 평가하려면 기존 데이터셋으로 못하니 저자가 만듦 112개 언어 대응 1,000 English-aligned sentence pairs for each langauge 평가는 다른언어에서 가장 비슷한 문장(nearest neighbor)을 cosine similarity로 찾고 error rate를 계산하는 것으로 함 (4.3이랑 비슷한듯){: height=”50%” width=”50%”} 5. Ablation experiments 요즘 유행(?)하고있는 것중 하나인 Ablation experiments..필요하지만 논문 정리하는 입장에서는.. 요약 인코더 깊이 쌓으면 잘됨 multitask learning으로 NLI loss를 추가하면 가중치에 따라서 더 잘 되기도함 18개보다 93개 언어에 대해서 학습할때 결과가 더 좋았음 (많은 언어에 대해서 하는데도 결과가 좋은거 보면 모델 capa가 괜찮은듯){: height=”50%” width=”50%”} 6. Conclusion 93개의 언어에 대해서 multilingual fixed-length sentence embeddings을 학습하는 모델을 제안함 Single language-agnostic BiLSTM encoder로 모든 언어를 커버함 fine-tuning 없어도 되는 모델임 새로운 테스트 데이터셋도 만들어서 제공함(112개 언어 커버) Massive 관점에서 general purpose multilingual sentence representation 을 다룬 첫번째 연구임 Future work: self-attention 쓴 encoder 쓰겠음 monolingual data 쓴 모델로 시도해보겠음 (pre-trained word embeddigns, back-translation, unsupervised MT) 전처리때 쓴 토크나이저를 SentencePiece로 바꾸고 싶음 Reference 본 논문 XNLI 데이터셋 논문 Note latex 문법중 \\operatorname, \\ 이거 두개가 latex에서 안될때가 있군..","link":"/2019-10-14-MassivelyMultilingualSentenceEmbeddigns/"},{"title":"Distilling Task-Specific Knowledge from BERT into Simple Neural Networks","text":"Author 저자: Raphael Tang∗, Yao Lu∗, Linqing Liu∗, Lili Mou, Olga Vechtomova, and Jimmy Lin (University of Waterloo) Who is an Author? ICASSP를 들고 있는 NLP 하던 분인 듯 보통은 문서분류쪽 많이 한듯 {: height=”50%” width=”50%”} 느낀점 아이디어는 간단함 Data Augmentation을 넣은건 좋았음 그러나 성능이 좋아진게 Distillation 때문인지 Data Augmentation 때문인지를 정확히 다루지 않아서.. 이 부분이 이 논문의 최대 에러임 Abstract 요즘엔 BERT, ELMo, and GPT 같은 deep language representation model이 대세임 이러한 발전은 예전에 쓰던 shallower neural networks 를 안쓰게 만듬 본 논문에서는 lightweight neural network도 구조 변경, 추가 데이터, 추가 feature 없이도 아직 쓸만하다는걸 보여주려고함 BERT에서 Knowledge distillation 해서 성능을 높여보려고함 setence classification, sentence-pair task 등으로 테스트 할 것임 ELMo보다 100배 적은 파리미터, 15배 빠른 추론속도로 비슷한 성능을 얻음 1. Introduction BERT등이 등장하면서 “first-generation” neural network가 잘 안쓰이게됨 본 논문에서는 간단하지만 효과적인 transfer 기법을 제안하고자함 (task-specific knowledge from BERT) single-layer BiLSTM을 사용할거고, BERT로부터 배울 것임 효율적인 knowledge transfer 를 위해선 데이터가 많이 필요하니, unlabled dataset으로부터 teacher output을 만들어서 student로 학습하게 할 것임 2. Related Work NLP에서 CNN, RNN 등이 발달됨 최근엔, ELMo(6가지 task SOTA 찍었음), BERT등이 등장함 Model Compression: local error-based method for pruning unimportant weights (LeCun et al. (1990)) Han et al. (2015) propose a simple compression pipeline, achieving 40 times reduction in model size without hurting accuracy. Unfortunately, these techniques induce irregular weight sparsity, which precludes highly optimized computation routines quantizing neural networks (Wu et al., 2018); in the extreme, Courbariaux et al. (2016) propose binarized networks with both binary weights and binary activations 위에서 소개된 Model Compression과는 다르게 본 논문에서는 knowledge distillation appoach (Ba and Caruana, 2014; Hinton et al., 2015) 를 사용하고자함 NLP에서 이미 이것에 대해 적용된 연구들이 있음(In the NLP literature, it has previously been used in neural machine translation (Kim and Rush, 2016) and language model- ing (Yu et al., 2018)) 3. Our Approach First, teacher model과 student model을 선택 후 학습 Second, 저자의 distillation procedure로 학습 logits-regression objective 사용 transfer dataset 구축 3.1 Model Architecture Teacher network : pretrained, fine-tuned BERT feature vector ${ \\boldsymbol{h} \\in \\mathbb{R}^{d} }$ 위에 우리가 사용할 classifier를 task에 맞게 추가해서 쓸것임 For single-sentence classification 다음과 같이 softmax 추가해서 쓸 것임(${k}$ is the number of label) ${ \\boldsymbol{y}^{(B)}=\\operatorname{softmax}(W \\boldsymbol{h}), \\text { where } W \\in \\mathbb{R}^{k \\times d} }$ For sentence-pair task 두 문장에 대한 BERT features를 concat후 softmax layer에 넣는 방식으로 함 학습시에는 BERT와 classifier에 대한 param을 둘다 업데이트하고 cross-entropy loss 사용함 Student model: single-layer BiLSTM with a non-linear classifier For classification last step의 값을 concat 후 fc layer with ReLU 에 feed해서 softmax layer로 분류함{: height=”50%” width=”50%”} For Sentence-pair tasks BiLSTM encoder weights를 share해서 siamese architecture로 사용 ${ \\text { sentence vectors } \\boldsymbol{h}{s 1} \\text { and } \\boldsymbol{h}{s 2} }$ 를 만들어냄 ${ \\begin{array}{l}{f\\left(\\boldsymbol{h}{s 1}, \\boldsymbol{h}{s 2}\\right)=\\left[\\boldsymbol{h}{s 1}, \\boldsymbol{h}{s 2}, \\boldsymbol{h}{s 1} \\odot\\right.} {\\left.\\boldsymbol{h}{s 2},\\left|\\boldsymbol{h}{s 1}-\\boldsymbol{h}{s 2}\\right|\\right], \\text { where } \\odot \\text { denotes elementwise multiplication}}\\end{array} }${: height=”50%” width=”50%”} attenion이나, layer norm 같은 스킬은 최대한 제외하고 BiLSTM의 representation Power에만 국한하는 설계를 함 3.2 Distillation Objective In addition to a one-hot predicted label, the teacher’s predicted probability is also important. In binary sentiment classifica- tion, for example, some sentences have a strong sentiment polarity, whereas others appear neutral. If we use only the teacher’s predicted one-hot label to train the student, we may lose valuable information about the prediction uncertainty. $$\\widetilde{y}{i}=\\operatorname{softmax}(\\boldsymbol{z})=\\frac{\\exp \\left{\\boldsymbol{w}{i}^{\\top} \\boldsymbol{h}\\right}}{\\sum_{j} \\exp \\left{\\boldsymbol{w}_{j}^{\\top} \\boldsymbol{h}\\right}}$$ $$\\begin{array}{l}{\\text { where } w_{i} \\text { denotes the } i^{\\mathrm{th}} \\text { row of softmax weight }} \\ {W, \\text { and } z \\text { is equivalent to } \\boldsymbol{w}^{\\top} \\boldsymbol{h} .}\\end{array}$$ Training on logits makes learning easier for the student model since the relationship learned by the teacher model across all of the targets are equally emphasized (Ba and Caruana, 2014). student network’s logits과 teacher’s logits의 MSE로 distillation objective를 만듬 (Cross entropy등도 사용하능하나, 저자의 실험에서 MSE가 좀 더 결과가 좋았다고함)$$\\mathcal{L}{\\text {distill }}=\\left|z^{(B)}-z^{(S)}\\right|{2}^{2}$$ 최종적인 Loss는 기존의 one-hot에 대한 CE loss와 distill Loss를 weighted sum해서 사용함 (${t}$는 one-hot label)$$\\begin{array}{l}{\\mathcal{L}=\\alpha \\cdot \\mathcal{L}{\\mathrm{CE}}+(1-\\alpha) \\cdot \\mathcal{L}{\\text {distill }}} \\ {=-\\alpha \\sum_{i} t_{i} \\log y_{i}^{(S)}-(1-\\alpha)\\left|z^{(B)}-z^{(S)}\\right|_{2}^{2}}\\end{array}$$ unlabeld data의 경우엔 teacher가 예측한걸 기준으로 사용함 ${ \\begin{array}{l}{\\text { i.e., } t_{i}=1 \\text { if } i=\\arg \\max y^{(B)} \\text { and } 0 \\text { otherwise. }}\\end{array} }$ 3.3 Data Augmentation for Distillation 데이터 셋 작으면 티쳐한테 배울때 효과가 별로 없음 그렇기 때문에 데이터셋 키우기로함 (with pseudo-labels provided by the teacher) 하지만.. NLP에서의 Data augmentation은 Computer vision에 비해 어려움 CV는 비슷한 이미지들이 매우 많음 (CIFAR-10 is a subset of the 80 million tiny images dataset) CV는 이미지 회전이나 노이즈 추가등.. 방법이 많음 (Second, it is possible to synthesize a near-natural image by rotating, adding noise, and other distortions) NLP에서 이 방법 쓰면 not be fluent되기 때문에 쓸 수 없음 ㅠ 본 논문에서는 약간의 휴리스틱으로 task-agnostic data augmentation을 하려고함 (image distortion과 같진 않고, 비슷하다고 생각하면됨) Masking: ${p_{mask}}$ 의 확률로 랜덤하게 단어를 [MASK]로 바꿈 Intuitively, this rule helps to clarify the contribution of each word toward the label (각 단어의 contribution을 파악하는데 도움이 된다고 주장) e.g., the teacher network produces less confident logits for “I [MASK] the comedy” than for “I loved the comedy.” POS-guided word replacement: ${p_{pos}}$ 의 확률로 단어를 같은 pos 태그를 갖는 다른 단어로 교체함 (허허 요상한 방법일세) 이러한 룰은 semantic을 방해하기도 함 (This rule perturbs the semantics of each example, e.g., “What do pigs eat?” is different from “How do pigs eat?”) n-gram sampling: ${p_{ng}}$ 의 확률로 예시 문장에서 n-gram을 샘플링함 (n is randomly selected from {1,2,…,5}) n-gram 외의 단어는 드랍해버리는것과 비슷한 효과고 마스킹보다 더 공격적인 방법임 (This rule is conceptually equivalent to dropping out all other words in the example, which is a more aggressive form of masking) Data augmentation procedure: training example ${w_{1}, …, w_{n}}$ 단어에 대해서 iteration 하면서 각 단어에 대해서 유니폼 분포로 확률을 계산함 ${ X_{i} \\sim \\text { UNIFORM }[0,1] \\text { for each } w_{i} }$ ${ \\text{if } X_{i}&lt;p_{\\text {mask }}, \\text { we apply masking to } w_{i} }$ ${ \\text{if } p_{\\text {mask}}&lt;X_{i}&lt;p_{\\text {mask }} + p_{\\text {pos }}, \\text { we apply POS-guided word replace to } w_{i} }$ masking과 POS-guided swapping은 mutuially exclusive하게 진행해서 한개가 적용되면 나머지는 적용안함 iteration이 끝나면, ${p_{ng}}$의 확률로 n-gram sampling을 synthetic example(위에서 만든 문장)에 적용하면 final synthetic example이 완성됨 이러한 전체 프로세스를 한 문장당 $ n_{iter} $번 적용해서 총 $ n_{iter} $개의 문장을 만듬 (중복은 제거) For sentence-pair datasets, we cycle through augmenting the first sentence only (holding the second fixed), the second sentence only (holding the first fixed), and both sentences. 4. Experimental Setup Teacher Network으로써의 BERT는 large 버전 사용 BERT fine-tuning 할 땐 Adam opt with lr {2,3,4,5} X ${10^{-5}}$ 적용 val set 기준 best model 선택 여기선 data augmentation 안씀 Student model 학습할 땐 data augmentation 사용 soft logit target을 사용한 모델을 ${\\text BiLSTM_{SOFT}}$ 로 표기하겠음 3.2 세션에서 weighted sum으로 기존 CE와 distillation Loss를 추가해서 만들었는데 ${\\alpha = 0}$으로 셋팅해서 distillation objective만 사용한게 젤 잘나왔음 4.1 Datasets GLUE에서 3개 뽑아서 씀 SST-2: movie reviews for binary sentiment classification (positive vs. negative) MNLI: to predict the relationship between a pair of sentences as one of entailment, neutrality, or contradiction QQP: binary label of each question pair indicates redundancy 4.2 Hyperparameters Student Model BiLSTM hidden: 150 or 300 RelLU activated hidden: 200 or 400 Optim: AdaDelta (lr: 1.0 ${\\rho}$: 0.95) Batch size: 50 (SST2), 256 (MNLI, QQP) Data Augmentation ${p_{mask} = p_{pos} = 0.1 \\text{ and } p_{ng} = 0.25}$ ${n_{iter}} = 20$ (SST), ${n_{iter}} = 10$ (MNLI, QQP) 4.3 Baseline Models BERT OpenAI GPT GLUE ELMo baselines 5. Results and Discussion5.1 Model Quality{: height=”50%” width=”50%”} our distillation approach of matching logits using the augmented training dataset, and achieve an absolute improvement of 1.9– 4.5 points against our base BiLSTM. data augmentation 없이 distillation한 것도 보여줘야.. 설득력이 더 있을텐데 음.. 5.2 Inference Efficiency On a single NVIDIA V100 GPU with a batch size of 512 on all 67350 sentences of the SST-2 training set our single-sentence model uses 98 and 349 times fewer parameters than ELMo and BERTLARGE, respectively 15 and 434 times faster {: height=”50%” width=”50%”} 6. Conclusion and Future Work Explore distilling the knowledge from BERT into a simple BiLSTM-based model The distilled model achieves comparable results with ELMo, while using much fewer parameters and less inference time Future work로는 더 단순한 모델로 KD하거나 더 복잡한 모델로 KD하거나..(이거 넘 당연한 발상아닌가..) Coderef: https://github.com/qiangsiwei/bert_distill Note: marp syntax 변경: https://marpit.marp.app/image-syntax","link":"/2019-12-10-Distilling_Task-Specific_Knowledge_from_BERT_into_Simple_Neural_Networks/"},{"title":"Unified Language Model Pre-training for Natural Language Understanding and Generation","text":"Author 저자: Li Dong∗ Nan Yang∗ Wenhui Wang∗ Furu Wei∗ † Xiaodong Liu Yu Wang Jianfeng Gao Ming Zhou Hsiao-Wuen Hon (Microsoft Research) Who is an Author? 일단 쓴 논문들에 대한 기본 인용수가 높다 감성분석, MRC, Summarization 등 태스크를 가리지 않고, EMNLP, AAAI, ACL 등에 논문을 엄청 많이 냄.. 그냥 고수 이 논문은 NeurIPS 2019 191219 기준으로 인용수 26회 {: height=”50%” width=”50%”} 느낀점 NLG에서 SOTA를 꽤 찍었는데 방식이 좀 신기 shared param (같은 모델)로 NLU와 NLG를 할 수 있다는게 가장 큰 장점 masking으로 장난치면서(?) 모델을 발전시킨건 어쩌면 자연스러운 수순인듯 1st segment에서 passage와 answer를 concat하거나 conversation history를 concat 방식으로 집어넣는데, 잘되는게 좀 신기하긴함 T5가 살아남을지 이 친구가 더 개량되서 살아남을지 궁금 seq2seq LM을 fine-tuning하는 방법이 좀 신선했음 당연히 left-to-right 방식으로 teacher forcing할줄 알았는데.. ㅎㅎ Abstract UNIfied pre-trained Language Model (UNILM) 이라는 모델을 제안함 NLU와 NLG를 모두 할 수 있게 fine-tune이 가능한 모델임 3가지 LM task로 pretraining함 unidirectional bidirectional sequence-to-sequence prediction shared Transformer network와 specific self-attention masks(to control what context the prediction conditions on)를 통해서 unified modeling을 함 UNILM은 GLUE, SQuAD 2.0, CoQA task도 좋은 성능을 낼 수 있고 NLG dataset에서도 5개 부분에서 SOTA를 기록함 CNN/DailyMail abstractive summarization ROUGE-L 값은 40.51을 기록함 (2.04 개선) Gigaword abstractive summarization ROUGE-L은 35.75 기록함 (0.86 개선) CoQA generative question answering F1 score는 82.5 기록함 (37.1 개선) SQuAD question generation BLEU-4는 22.12 기록함 (3.75 개선) DSTC7 document-grounded dialog response generation NIST-4는 2.67 기록함 (사람이 한 점수는 2.65) code &amp; pretrained models: https://github.com/microsoft/unilm 1. Introduction LM pre-training은 다양한 NLP task에서 SOTA를 찍을 수 있게 해줌 (substantially advanced) Pre-trained LMs은 contextualizaed text representations을 단어 주변의 context를 활용해서 단어를 예측함으로써 학습하고 이때 대량의 text 데이터를 사용함 Pre-trained LMs은 Downstream task에 대해서 fine-tune 해서 쓸수 있음 pre-training LMs의 타입에 따라 다양한 prediction task와 training objectives가 사용되왔음 ELMo의 경우엔 2가지의 unidirectional LMs을 사용함 left-to-right와 right-to-left로 배우기 때문임 GPT의 경우엔 left-to-right 임 BERT의 경우는 bidrectional LM임 {: height=”50%” width=”50%”} BERT가 성능이 매우 좋은 모델이지만 특성상 NLG task에 적용이 어려움 본 연구에서는 UNIfied pre-trained Language Model (UNILM)을 제안하면서 모델을 NLU와 NLG task에 모두 적용하고자함 UNILM은 multi-layer Transformer network이고 pre-train을 하면서 동시에 3가지 타입의 unsupervised language modeling objectives에 대해 학습함 {: height=”50%” width=”50%”} 특별히 몇가지의 cloze tasks(빈칸 채우기)를 디자인했고 거기서 보는 context는 다음같음 unidirectional LM left-to-right unidirectional LM context는 왼쪽에 있는 모든 단어들이 됨 right-to-left unidirectional LM 반대로 오른쪽에 있는 모든 단어들이 됨 bidirectional LM context는 왼쪽 오른쪽 방향을 모두 포함하는 단어 주변의 모든 단어들 sequence-to-sequence LM context는 encoder의 정보와 target sequence에서 예측해야되는 단어의 앞에 있는 모든 단어들 BERT와 비슷하게 pre-trained UNILM은 fine-tuning이 가능하지만(with additional task-specific layers if necessary), NLU task가 메인인 BERT와 다르게 UNILM은 다른 종류의 LMs의 context를 결합하기 위해서 different self-attention masks를 사용하는 것으로 설계되었고 이는 NLU와 NLG task 모두를 가능하게 해줌 제안하는 UNILM은 3가지 장점이 있음 the unified pre-training procedure는 single Transformer LM이 다양한 타입의 LMs을 위한 모델의 parameters와 architecture를 공유할 수 있게 해줌 (alleviating the need of separately training and hosting multiple LMs) context를 다르게 잡아내는 different LM objective를 학습하면 any sing LM task에서 발생할 수 있는 overfitting을 막아주기 때문에, 이러한 parameter sharing은 학습된 text representations을 더 general하게 해줌 UNILM은 sequence-to-sequence LM을 사용하는데, 이는 NLG를 위한 자연스러운 선택이됨 (such as abstractive summarization and question generation) 실험결과를 보면, bidirectional encoder를 사용한 제안모델이 GLUE에서 BERT와 비교할만하고 two extractive QA task에서도 좋은 결과를 냄 (NLU, NLG 둘다 잘한다) 2. Unified Language Model Pre-training 주어진 input sequence ${ x=x_{1} \\cdot \\cdot \\cdot x_{n} }$에 대해서 UNILM은 각 token에 대해서 contextualized vector representation을 얻음 pre-training 단계에서 shared Transformer network를 unidirectional LM, bidirectional LM, and sequence-to-sequence LM 라는 LM objectives로 학습함 이를 위해서 self-attention에 대해 different masks 를 도입함 (use masking to control how much context the token should attend ) pre-training 끝나면 downstream task를 위해 task-specific data로 fine-tuning해서 쓸 수 있음 {: height=”50%” width=”50%”} 2.1 Input Representation Special token 추가함 [SOS]: start-of-sequence [EOS]: end-of-sequence input representation은 BERT 형식을 따름 WordPiece로 토큰화됨 LM 종류에 따라 segment가 달라짐 (Figure 1 참고) 2.2 Backbone Network: Multi-Layer Transformer input vectors를 ${ H^{0}=[x_{1}, \\cdots, x_{n}] }$ 로 나타낼 수 있고 L-layer의 Transformer를 통해 different levels에서의 contextual representation으로 인코딩하면 ${ H^{l}=[h_{1}^{l}, \\cdots, h_{n}^{l}] }$ 으로 나타낼 수 있음 $\\mathbf{H}^{l}=\\operatorname{Transformer}_{l}(\\mathbf{H}^{l-1}), l \\in[1, L]$ 로 표현 가능함 $l$ 번째 layer에서 self-attention Head $\\mathbf{A}_{l}$ 의 output은 다음과 같이 계산됨 $$\\begin{aligned} \\mathbf{Q} &amp;=\\mathbf{H}^{l-1} \\mathbf{W}{l}^{Q}, \\quad \\mathbf{K}=\\mathbf{H}^{l-1} \\mathbf{W}{l}^{K}, \\quad \\mathbf{V}=\\mathbf{H}^{l-1} \\mathbf{W}{l}^{V} \\ \\mathbf{M}{i j} &amp;=\\left{\\begin{array}{ll}{0,} &amp; {\\text { allow to attend }} \\ {-\\infty,} &amp; {\\text { prevent from attending }}\\end{array}\\right.\\ \\mathbf{A}{l} &amp;=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{d{k}}}+\\mathbf{M}\\right) \\mathbf{V}_{l} \\end{aligned}$$ 이전 layer의 output인 ${ H^{l-1} \\in R^{n \\times d_{h}} }$ 은 parameter matrices ${ W_{l}^{Q}, W_{l}^{K}, W_{l}^{V} \\in R^{d_{h} \\times d_{k}} }$에 의해 queries, keys, vlaues로 linearly projected 됨 mask matrix ${ \\mathbf{M} \\in \\mathbb{R}^{n \\times n} }$ 는 token의 contextualized representation을 계산하기 위해 어떤 token들에 attention할지를 결정하기 위해 사용됨 2.3 Pre-training Objectives The parameters of UNILM are learned to minimize the cross-entropy loss computed using the predicted tokens and the original tokens LM 종류 Unidirectional LM: use both left-to-right and right-to-left LM objectives For instance, to predict the masked token of “${x_{1}x_{2}}$ [MASK] ${x_{4}}$”, only tokens ${x_{1}, x_{2}}$ and itself can be used. This is done by using a triangular matrix for the self-attention mask ${M}$ Bidirectional LM: the self-attention mask $M$ is a zero matrix, so that every token is allowed to attend across all positions in the input sequence. Sequence-to-Sequence LM: the tokens in the first (source) segment can attend to each other from both directions within the segment, while the tokens of the second (target) segment can only attend to the leftward context in the target segment and itself, as well as all the tokens in the source segment “[SOS] ${t_{1} t_{2}}$ [EOS] ${t_{3} t_{4} t_{5}}$ [EOS]” into the model. While both t1 and t2 have access to the first four tokens, including [SOS] and [EOS], t4 can only attend to the first six tokens sequence-to-sequence LM의 경우 bidirectional encoder와 unidirectional decoder를 학습한다고 보면 됨 Next Sentence Prediction: Bidirectional LM에 대해서는 NSP를 적용함 2.4 Pre-training Setup one training batch당, 1/3은 bidrectional LM objective, 1/3은 seq2seq LM objective, 나머지 1/3은 unidirectional LM objective (left-to-right, right-to-left)를 사용함 모델의 구조는 ${BERT_{LARGE}}$와 같음 gelu activation 24-layer transformer (340M params) with 1,024 hidden size 16 attention heads weight matrix of the softmax classifier is tied wtih token embeddings ${BERT_{LARGE}}$의 weight로 initialize함 Corpus는 English Wikipedia와 BookCorpus 사용 Vocab size: 28,996 Maximum lengths of input seq: 512 Masking Prob: 15% 80%: [MASK] 10%: random token 10%: original token 마스킹할때 80%는 one token으로 나머지 20%는 bigram or trigram으로 마스킹함 Optimizer: Adam: ${\\beta_{1}=0.9, \\beta_{2}=0.999}$ lr: 3e-5 warm up: first 40,000 steps (and linear decay) weight decay: 0.01 Dropout rate: 0.1 Batch size: 330 (특이하네) pre-training procedure runs: 770,000 steps time: 7 hours for 10,000 steps GPUs: 8 Nvidia Telsa V100 32GB 2.5 Fine-tuning on Downstream NLU and NLG Tasks NLU task에 대해서는 BERT처럼 fine-tuning하면 됨 [SOS] 토큰에 대한 vector $ \\mathbf{h}_{1}^{L} $에 randomly initialized softmax classifier를 붙임 ${ softmax(h_{1}^{L} W^{C}), \\text { where } W^{C} \\in R^{d_{h} \\times C} }$ (C는 카테고리 개수(클래스 개수)임) NLG task에 대해서는 seq2seq task와 비슷함 Notation S1: source sequence S2: target sequence 하나로 합침(pack) “[SOS] S1 [EOS] S2 [EOS]” fine-tuning 방법: target sequence에 있는 토큰을 특정 비율로 랜덤하게 마스킹한 후에 맞추도록 학습함(masking some percentage of tokens in the target sequence at random, and learning to recover the masked words.) The training objective is to maximize the likelihood of masked tokens given context 생성을 끝내는 의미로도 사용되는 [EOS]에 대해서도 마스킹을 하는게 좋은데, 그 이유는 모델이 언제 generation process를 끝내야되는지도 학습할 수 있기 때문임(It is worth noting that [EOS], which marks the end of the target sequence, can also be masked during fine-tuning, thus when this happens, the model learns when to emit [EOS] to terminate the generation process of the target sequence) (근데 이렇게 finetuning하면 fully generation하는게 아닌데 잘 되나..) 3. Experiments NLU는 GLUE, extractive question answering으로 평가 NLG는 abstractive summarization, question generation, generative question answering, and dialog response generation등으로 평가 3.1 Abstractive Summarization Dataset: non-anonymized version of the CNN/DailyMail dataset Gigaword for model fine-tuning and evaluation Input representation: by concatenating document (the first segment) and summary (the second segment) Finetune process: fine-tune our model on the training set for 30 epochs reuse most hyper-parameters from pre-training Masking prob: 0.7 (되게 높아졌기 때문에 generation이 가능한거군..!) label smoothing with rate of 0.1 For CNN/DailyMail: batch size to 32, and maximum length to 768 For Gigaword: batch size to 64, and maximum length to 256 Decoding: beam search with beam size of 5 remove duplicated trigrams in beam search The input document is truncated first 640 for CNN/DailyMail first 192 tokens for Gigaword {: height=”50%” width=”50%”} Evaluation Metric: F1 version of ROUGE Table 3는 CNN/DailyMail 에 대한 평가이고 Table 4는 Gigaword에 대한 평가임 Other Models LEAD-3 (Baseline): 첫 3문장을 문서의 summary로 보는 것 PGNet: Pointer-generator network 기반의 seq2seq 모델 (copy mechanism) S2S-ELMo: pre-trained ELMo representation을 통해 seq2seq 모델을 개량한 것 Bottom-Up: salient phrases를 선택하는 content selector를 사용 3.2 Question Answering (QA){: height=”50%” width=”50%”} Extractive QA: 답이 passage안의 text span라고 가정 bidrectional encoder를 사용해서 접근함 experiments SQuAD 2.0 (Stanford Question Answering Dataset) hyper params epoch: 3 batch size: 24 max len: 384 CoQA (Conversational Question Answering) SQuAD랑은 좀 다른데, 대화 내역에 기반한 답변을줘야함 답변은 free-form texts 형태임 (yes/no answer 포함) concatenate the question-answer histories to the first segment for yes/no questions, we use the final hidden vector of the [SOS] token to predict whether the input is a yes/no question, and whether the answer is yes or no for other examples, we select a passage subspan hyper params epoch: 2 batch size: 16 max len: 512 결과를 보면 EM (Exact Match)이나 F1 모두 UNILM이 젤 높음 Generative QA: 답을 즉석으로 생성해야함 seq2seq model 방법 채택 기존 vanilla seq2seq model은 extractive method 보다 성능이 낮았음 (Reddy et al. [2019]) 첫번째 segment에는 대화 이력을 concat해서 넣음(the input question and the passage) 두번째 segment에서는 답변을 출력 experiments CoQA 데이터셋에 대해서 fine-tuning epoch: 10 batch size: 32 mask prob: 0.5 max len: 512 label smoothing: 0.1 decoding에 beam search 적용 (with 3 beam size) 3.3 Question Generation{: height=”50%” width=”50%”} passage와 answer가 주어졌을 때, question을 생성하는 것 seq2seq 문제로 보고 풀겠음 1st seg: input passage + answer 2nd seg: generated question SQuAD 1.1 dataset을 평가셋으로 사용함 선행 연구에서와 같이 original training set을 training과 test sets으로 쪼개서 사용하기로하고 original dev set은 그대로둠 hyper params: epoch: 10 batch size: 32 mask prob: 0.7 lr: 2e-5 label smoothing: 0.1 Generated Questions Improve QA{: height=”50%” width=”50%”} Question generation model로 질문을 만들어서(data augmentation) 다시 학습시키면 기존의 question answering model의 성능이 올라감 3.4 Response Generation{: height=”50%” width=”50%”} document-grounded dialog response generation task로 UNILM을 평가해봄 multi-turn conversation history와 a web document as the knowledge source가 주어진 상태에서 시스템은 대화에도 알맞고, web document contents도 반영하는 답변을 해야함 UNILM을 seq2seq model로 사용함 1st seg: web document + conversation history 2nd seg: response dataset: DSTC7 hyper params: epoch: 20 batch size: 64 masking prob: 0.5 max len: 512 decoding에 beam search 적용 (with 10 beam size) 3.5 GLUE Benchmark{: height=”50%” width=”50%”} (버트보다 좋은 성능 가진 모델이 많이 나왔는데 버트랑만 비교하는건 좀 아쉽다) 4. Conclusion and Future Work several LM objectives를 shared parameters로 학습하는 unified pre-training model인 UNILM을 제안함 NLU와 NLG 둘다 가능함 BERT와 GLUE 벤치마크에서 비교할만했음 5가지 NLG dataset에서 SOTA를 달성함 (CNN/DailyMail and Gigaword abstractive summarization, SQuAD question generation, CoQA generative question answering, and DSTC7 dialog response generation) Future works: training more epochs and larger models on web scale text corpora + ablation experiments support cross-lingual tasks multi-task fine-tuning on both NLU and NLG tasks (MT-DNN의 extension) Codehttps://github.com/microsoft/unilm","link":"/2019-12-19-Unified%20Language%20Model%20Pre-training%20for%20Natural%20Language%20Understanding%20and%20Generation/"},{"title":"Linux, Unix 정리","text":"Shorcut자주 쓰는 명령어 모음 lshw: 하드웨어 스펙보기 예약변수: HOME, PATH, PWD, LANG, 등등 위치 매개 변수(Positional Parameters) 문자 설명 $0 실행된 스크립트 이름 $1 $1 $2 $3…${10}인자 순서대로 번호가 부여된다. 10번째부터는 “{}”감싸줘야 함 $* 전체 인자 값 $@ 전체 인자 값($* 동일하지만 쌍따옴표로 변수를 감싸면 다른 결과 나옴) $# 매개 변수의 총 개수 특수 매개 변수(Special Parameters) 문자 설명 $$ 현재 스크립트의 PID $? 최근에 실행된 명령어, 함수, 스크립트 자식의 종료 상태 $! 최근에 실행한 백그라운드(비동기) 명령의 PID $- 현재 옵션 플래그 $_ 지난 명령의 마지막 인자로 설정된 특수 변수 디버깅(Debugging) 간단하게는 echo, exit 명령나 tee 명령어로 디버깅한다. 다른 방법으로 실행 시 옵션을 주거나 코드에 한줄만 추가하면 해볼수 있다. Bash 옵션(스크립트 실행 시) set 옵션(스크립트 코드 삽입) 설명 bash -n set -n, set -o noexec 스크립트 실행없이 단순 문법 오류만 검사(찾지 못하는 문법 오류가 있을수 있음) bash -v set -v, set -o verbose 명령어 실행전 해당 명령어 출력(echo) bash -x set -x, set -o xtrace 명령어 실행후 해당 명령어 출력(echo) set -u, set -o nounset 미선언된 변수 발견시 “unbound variable” 메시지 출력 배열(Array Variable) 배열 변수 사용은 반드시 괄호를 사용해야 한다.(예: ${array[1]}) 참고: 1차원 배열만 지원함 123456789101112131415161718192021222324252627# 배열의 크기 지정없이 배열 변수로 선언# 참고: 'declare -a' 명령으로 선언하지 않아도 배열 변수 사용 가능함declare -a array# 4개의 배열 값 지정array=(&quot;hello&quot; &quot;test&quot; &quot;array&quot; &quot;world&quot;)# 기존 배열에 1개의 배열 값 추가(순차적으로 입력할 필요 없음)array[4]=&quot;variable&quot;# 기존 배열 전체에 1개의 배열 값을 추가하여 배열 저장(배열 복사 시 사용)array=(${array[@]} &quot;string&quot;)# 위에서 지정한 배열 출력echo &quot;hello world 출력: ${array[0]} ${array[3]}&quot;echo &quot;배열 전체 출력: ${array[@]}&quot;echo &quot;배열 전체 개수 출력: ${#array[@]}&quot;printf &quot;배열 출력: %s\\n&quot; ${array[@]}# 배열 특정 요소만 지우기unset array[4]echo &quot;배열 전체 출력: ${array[@]}&quot;# 배열 전체 지우기unset arrayecho &quot;배열 전체 출력: ${array[@]}&quot; 반복문(for, while, until) 반목문 작성 시 아래 명령어(흐름제어)을 알아두면 좋다. 반복문을 빠져 나갈때: break 현재 반복문이나 조건을 건너 뛸때: continue 123456789101112131415161718# 지정된 범위 안에서 반복문 필요 시 좋음for string in &quot;hello&quot; &quot;world&quot; &quot;...&quot;; do; echo ${string};done# 수행 조건이 true 일때 실행됨 (실행 횟수 지정이 필요하지 않은 반복문 필요 시 좋음)count=0while [ ${count} -le 5 ]; do echo ${count} count=$(( ${count}+1 ))done# 수행 조건이 false 일때 실행됨 (실행 횟수 지정이 필요하지 않은 반복문 필요 시 좋음)count2=10until [ ${count2} -le 5 ]; do echo ${count2} count2=$(( ${count2}-1 ))done 조건문(if…elif…else…fi) 조건문 작성 시 주의해야될 부분은 실행 문장이 없으면 오류 발생함 1234567891011121314151617181920212223string1=&quot;hello&quot;string2=&quot;world&quot;if [ ${string1} == ${string2} ]; then # 실행 문장이 없으면 오류 발생함 # 아래 echo 문장을 주석처리하면 확인 가능함 echo &quot;hello world&quot;elif [ ${string1} == ${string3} ]; then echo &quot;hello world 2&quot;else echo &quot;hello world 3&quot;fi# ANDif [ ${string1} == ${string2} ] &amp;&amp; [ ${string3} == ${string4} ]..생략# ORif [ ${string1} == ${string2} ] || [ ${string3} == ${string4} ]..생략# 다중 조건if [[ ${string1} == ${string2} || ${string3} == ${string4} ]] &amp;&amp; [ ${string5} == ${string6} ]..생략 문자열 인덱싱: https://superuser.com/questions/1033273/bash-4-3-substring-negative-length-on-os-x 맥에서는 -1 이런게 잘 안먹혀서 ${STR:6:$#-1} 이런식으로 해야하는듯 문자열 짜르기: https://www.tutorialkart.com/bash-shell-scripting/bash-split-string/ 파일 합치기: 1cat part-000 part_blog_normal_doc_20030317 &gt; part_blog_normal_concat 0-9 사이 숫자만 변경되는 패턴 찾기: https://recipes4dev.tistory.com/157 grep &quot;STR[0-9]&quot; * 문자열 뒤에서 짜르기 rev 두번쓰는건 좀 느림.. parameter expansion을 쓰자 참고: https://blog.gaerae.com/2015/01/bash-hello-world.html 1234567891011# 뒤에꺼만 짜르기last=${data%,*}d3b028:~/test_200225$ echo $last# 결과: foo,bar,baz# 뒤에꺼만 냅두기data=foo,bar,baz,quxlast=${data##*,}# 결과: qux 문자열 중간 짜르기 ref: https://stackoverflow.com/questions/5683367/how-to-cropcut-text-files-based-on-starting-and-ending-line-numbers-in-cygwin 큰 파일 짜르기 ref: https://linoxide.com/linux-how-to/split-large-text-file-smaller-files-linux/ https://askubuntu.com/questions/54579/how-to-split-larger-files-into-smaller-parts 1split -l 라인수 파일 (옵션은 추후 참고) tab으로 짜르기 https://linuxhint.com/20_awk_examples/#a4 파일내의 단어 문자수 세기 ref: https://leeahnlee.tistory.com/14 -c : 전체 문자의 수를 출력 -l : 전체 라인의 수를 출력 -w : 전체 단어의 수를 출력 1wc -l 파일명 큰 파일 label 기준으로 짜르기 1234# label 2 기준으로 몇라인인지 숫자세기cat _blog_all_title_total.txt | grep -w $'\\t2' | wc -l# 또는 # label 8192 기준으로 몇라인인지 숫자세기cat _blog_all_title_total.txt | grep -w $'\\t8192’ | wc -l ref: https://recipes4dev.tistory.com/157 https://stackoverflow.com/questions/9954515/grep-find-lines-that-contains-t 기타 Notation $# csh, sh 스크립트 할 arguments의 갯수-ref: https://idchowto.com/?p=11930 grep grep엔 메타문자가 있음 ^: 라인의 시작 블로그 파일을 gdid grep으로 분류하고 싶다면 다음과 같이하면 됨1cat imp_log.txt | grep ^90000003_ &gt; blog_imp_log.txt sort &amp; join파일을 join 하려면 우선 sort 를 해야됨 sort: https://goitgo.tistory.com/15 sort하려는데 저장공간이 없다고 뜰때: https://www.linuxquestions.org/questions/linux-newbie-8/sort-big-files-tmp-sorta3aljf-no-space-left-on-device-823971/ join: https://m.blog.naver.com/PostView.nhn?blogId=zzeun&amp;logNo=130186967794&amp;proxyReferer=https:%2F%2Fwww.google.com%2F 1sort 파일명 &gt; sorted_파일명.txt join 할땐 여러가지 옵션이 있음우선 key값은 sep 기준으로 맨 앞에 있는게 보통임 (비교할 key 값은 옵션으로 지정가능) 1join 파일1 파일2 &gt; joined_파일3.txt 만약 join되지 않는 라인만 출력하고자 한다면 다음과 같이하면 됨 (파일1에서 join되지 않는 라인 출력) 1join -v 1 file1 file2 &gt; not_joined_file3.txt rsync Rsync(Remoe Sync)는 원격에 있는 파일과 디렉토리를 복사하고 동기화 하기 위해서 사용하는 툴이며 동시에 네트워크 프로토콜임 Rsync는 CLI툴로, 커맨드 라인의 옵션들을 이용해서 배치 프로그램을 개발하기 쉬다는 장점이 있다. 이 스크립트를 cron 등에 올리는 걸로 간단하게 백업 혹은 미러(mirror) 시스템을 구축할 수 있음 장점 원격 시스템으로 부터 파일을 효율적으로 복사하거나 동기화 할 수 있음 Link, device, 파일의 소유자와 그룹 권한(permissions)등 파일의 부가정보도 복사할 수 있음 scp보다 빠름 (rsync는 remote-update 프로토콜을 이용해서 차이가 있는 파일만 복사함. 처음엔 다 복사하니 비슷할수도) 데이터를 압축해서 송/수신하기 때문에 더 적은 대역폭을 사용 알고리즘 기본적으로 rsync는 파일의 크기와 수정 시간(modification)을 비교하는 것으로 파일을 전송할지 말지를 결정함 –checksum 옵션을 이용하면 비교 방법을 개선할 수 있다. 이 옵션을 켜면, 파일의 checksum을 비교한다. 크기/시간을 이용한 비교 방법보다 안전하지만 더 느리고 더 많은 자원을 사용 Rsync는 파일을 고정 크기를 가지는 청크(chunk)로 나눈다음에 checksum을 계산한다. 이 checksum을 서로 계산해서, 다를 경우 해당 부분의 청크만을 복사한다.만약 파일의 앞 부분의 정보가 수정돼서 정보들이 밀린다면 모든 청크의 checksum이 어긋날 것이다. 이 문제를 피하기 위해서 “Rolling hash”를 사용 출처: https://skibis.tistory.com/16 rsync 사용법 시나리오 https://blueyikim.tistory.com/562 rsync 속도제한 실 서비스에서 무언가를 가져오거나 적용할 땐 느린속도로 해주는게 좋음 -bwlimit 옵션이 있고 단위는 kb/s 임 (아래 예는 600KB/s) (원격에서 로컬로 (rsync from to 로 보면됨)) rsync -avz --progress --bwlimit=600 원격계정@원격ip:원격경로 로컬경로 파일 복사하지 않고 디렉토리 구조를 속도 제한해서 복사rsync -av --progress --bwlimit=600 -f&quot;+ */&quot; -f&quot;- *&quot; 복사주는곳 복사받는곳 Practical Unix프로젝트를 하다보니 얼추 쓸수는 있게 되었는데 뭔가 디테일이 부족하다고 느끼던 차, 스탠포드에서 강의가 있는걸 알게되서 한번 빠르게 정주행해야곘다 생각했습니다. 아래에 간단하게 정리했습니다. command set: https://www.tjhsst.edu/~dhyatt/superap/unixcmd.html video:https://practicalunix.org/video-schedule week 2: Intro 사용할 shell (bash, zsh 등등)정하기 사용할 eiditor (vim, emacs) 배우기 shell과 editor 커스터마이징하기 (dot file! ex .zshrc) github에 올려놓고 자유롭게 저장, 바꿔쓰기 참고로 난 macOS / iTerm / zsh 환경에서 작업 중https://practicalunix.org/content/week-2-intro week 3: Pipelines - Input/Output Redirection 입출력 대상을 표준 입력(stdin), 표준 출력(stdout), 표준 오류(sterr)를 쓰지 않고 다른 경로인 파일로 재지정 하는 것 표준 입력 재지정(Input Redirection) 키보드 입력(표준 입력)을 파일에서 받도록 대체하는 것 “&lt;” 연산자를 사용해서 키보드로 연결된 표준 입력 방향으로 파일로 변경(명시적) cat 명령어를 사용하는 것과 동일한 결과 표준 출력 재지정(Output Redirection) 명령의 실행 결과나 에러 상황을 화면에 출력하지 않고 바로 파일로 저장 “&gt;” 연산자를 파일명 앞에 지정하여 사용함 “&gt;” 연산자로 출력방향을 지정할 때 목적 파일은 항상 처음부터 다시 작성됨(파일 덮어씀) “&gt;&gt;” 연산자를 사용하면, 존재하지 않는 파일이면 “&gt;”과 마찬가지로 파일이 생성되고, 파일이 있는 경우에는 이어서 작성 됨 표준 오류 재지정(Error Redirection) 리다이렉션 연산자가 필요없음 (이부분은 아직 잘 모르겠음) 파이프(Pipe), 파이프라인(Pipeline) 둘 이상의 명령을 함께 묶어 출력의 결곽 다른 프로그램의 입력으로 전환하는 기능임 즉, 명령어의 표준출력을 또 다른 명령어의 표준 입력과 연결 시킬 수 있음 명령어와 명령어의 연결은 &quot;|&quot; 기호를 사용함 &quot;|&quot; 기호 앞의 명령 결과가 &quot;|&quot; 기호 뒤의 명령에 입력 데이터로 사용됨 1ls /bin /usr/bin | sort | uniq | grep zip 1python read-input.py &lt; nums-0-999 &gt;&gt; result.txt 기타 명령어 ‘head’, ‘tail’ : 파일의 시작, 끝을 보여줌 ‘tr’ ‘sort’ ‘uniq’ ‘cut’ ‘join’ ‘sed’ ‘awk’ ‘tee’ http://eunguru.tistory.com/89https://practicalunix.org/content/week-3-pipelines week 4: Grep and Regular ExpressionsReference https://blog.gaerae.com/2015/01/bash-hello-world.html","link":"/2020-02-10-Practical-Linux-Unix/"},{"title":"Towards a Human-like Open-Domain Chatbot","text":"Author 저자: Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, Quoc V. Le (Google Research, Brain Team) Who is an Author? 구글스칼라나 다른 곳에 딱히 프로필이 없음 그의 행적은 트위터에.. https://twitter.com/xpearhead 미디엄도.. https://medium.com/@dmail07 {: height=”50%” width=”50%”} 느낀점 일단 논문이 꽤 길다 모델쪽보단 automatic evaluation metric을 제안했다는것에 은근 더 중점을 맞추는 느낌 모델쪽 얘기는 Evolved Transformer논문을 더 봐야할듯 뭐랄까.. 설명이 많고 장황한 논문이다. 새로운 개념을 정의하는게 많은 논문임. 제안하는 개념이 필요한 이유등을 주로 설명함. Metric + large scale + tip이 본 논문의 주요 contribution인듯 modeling적인 부분은 별로 기술되어있지 않음 Abstract Meena라는 이름을 가진, multi-turn open-domain chatbot을 제안함 public domain social media conversation으로부터 데이터를 정제해서 end-to-end로 학습시킴 (데이터 공수가 꽤 중요했을듯) 2.6B params의 neural net으로 이루어진 모델은 단순하게 next token의 perplexity를 최소화시키는 방법으로 학습함 Sensibleness and Specifity Average (SSA)라는 Human evaluation과 연관성을 가진 metric도 제안함 (human-like multi-turn conversation의 key element를 평가) 실험 결과로 perplexity와 SSA간에는 strong correlation이 있는 것으로 나타남 best perplexity를 갖는 모델은 SSA도 높았음 The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evalu- ation) suggests that a human-level SSA of 86% is potentially within reach if we can better op- timize perplexity full version of Meena (filtering mechanism + tuned decoding)의 경우 79% SSA점수를 받았음 (기존에 존재하는 챗봇들보다 23% 높았음) 1. Introduction 자연어로 자유롭게 대화하는 능력은 human intelligence의 증표 같은 것이며 진정한 인공지능에게 요구되는 것임 closed-domain 챗봇은 특정 태스크를 위해 키워드나 인텐트등에 답변하지만 open-domain 챗봇의 경우엔 any topic에 대해서도 답변이 가능해야함 기존 연구에서 MILABOT (Serban et al., 2017), XiaoIce (Zhou et al., 2018)1, Gunrock (Chen et al., 2018), Mitsuku (Worswick, 2018)2 and Cleverbot3 (by Rollo Carpenter) 등이 제안되었지만, complex frameworks (dialog managers with knowledge-based, retrieval-based, or rule-based systems)에 의존적이었음 (어떻게 보면 당연한거같기도 한데 여기서는 지적함) End-to-End NeuralNet 접근방법도 있었지만 하나의 학습 모델로 평이한 결과를 냄 많은 연구에도 불구하고 open-domain chatbot은 여전히 약점들을 갖고 있음 open-ended input에 말이 안되는 답변을 하곤함 혹은 모호하거나 일반적인 답변을 함 본 연구에서는 generative chatbot인 Meena를 제안함 40B words mined and filtered from public domain social media conversation 으로 end-to-end 학습함 end-to-end 접근방법을 한계까지 밀어붙임 large scale low-perplexity model이 대화 잘하는지 확인함 Meena에서는 Evolved Transformer (So et al., 2019) 기반의 seq2seq model을 사용함 모델은 multi-turn conversation 으로 학습함 input sequence 형태는 all turn of the context 임 (최대 7개까지 사용) (핑퐁팀의 접근과 비슷한듯) output sequence 형태는 response임 best model은 2.6B params을 사용하고 test perplexity로 10.2를 기록함 Meena와 다른 챗봇의 quality를 평가하기 위해 Sensibleness and Specificity Average (SSA)를 제안함 모델의 모든 답변에 대해 사람이 label을 달 때, 저 두가지 기준을 사용함 sensible은 context 안에서 말이 되는지를 판단함 사람이 만든 대화에서는 97%정도가 이 기준에 부합했음 하지만 이것만 가지고 판단할 경우 답변이 다소 모호하거나 지루할 수 있음(vague and boring) 예를 들면, “I don’t know” 답변이 계속 나온다든지 하는 문제가 발생함 그렇기 때문에 또 다른 평가 기준이 필요함 specificity는 주어진 context안에서 답변이 얼마나 구체적인지를 평가함 Meena, humans 그리고 다른 오픈도메인 챗봇을 SSA metric으로 비교하기 위해 2가지 타입의 human evaluation을 사용함 static: 1,477 multi-turn conversations을 큐레이션해서 데이터셋을 만듬 interactive: 사람이 시스템에 원하는건 무엇이든지 입력함 surprised but pleased, static, interactive evaluation에서 모두 SSA metric이 Meena’s perplexity와 strong correlation이 있음을 발견했음 (In other words, the better that Meena fit its training data, the more sensible and specific its chat responses became.) 어찌보면 당연해보이는게 왜 놀랍냐면, 최근 연구들에서는 human evaluation과 BLEU같은 automatic metrics이 poor correlation을 갖는게 밝혀졌기 때문임 (사실 BLEU를 생성 모델의 평가지표로 쓰는거 자체가 말이 안되긴 함) best end-to-end learned model은 avg 72% SSA를 기록했고 full version of Meena (+filtering mechanism and tuned decoding)는 79%의 SSA를 기록함 (사람 간의 대화 점수는 avg 86%임, 사람의 경우 sensibleness가 매우 높고, specificity는 눈에 띄게 낮은 경향이 있었음) 평가방법에 단점이 있다면, static eval의 경우 데이터셋이 제한되어있으므로 human conversations을 모두 커버한다 할 수 없음 그럼에도 불구하고 SSA score와 perplexity간에 발견한 correlation을 통해 perplexity를 개선하는게 human-like chatbot에 도움이 된다걸 발견했다는데 의의가 있음 본 논문의 contribution은 proposing a simple human evaluation metric for multi-turn open-domain chatbots showing evidence that perplexity is an automatic metric that correlates with human judgment demonstrating that an end-to-end neural model with sufficiently low perplexity can surpass the sensibleness and specificity of existing chatbots that rely on complex, handcrafted frameworks developed over many years 2. Evaluating chatbots 챗봇과 NLG를 평가하는건 잘알려진 challenge임 (Liu et al., 2016; Lowe et al., 2017; Novikova et al., 2017; Hashimoto et al., 2019) 본 논문에서는 사람과 같은 답변을 하는지를 평가하기위한 human evaluation metric을 제안함 2가지 setup인 static (fixed set of multi-turn contexts to generate responses), interactive(chat freely with chatbots) 으로 나눠서 진행함 2.1 Measuring Human Likeness 주어진 context에서 말이 되는지를 평가 (given the context, makes sense) Sensibleness 평가 요소 common sense logical coherence consistency 약점 However, being sensible is not enough. A generic response (e.g., I don’t know) can be sensible, but it is also boring and unspecific GenericBot(question에는 항상 “I don’t know”를, statement에는 항상 “ok” 시전)을 만들어서 평가해보니 static evaluation에서 DialoGPT (62%) 보다 높은 70%의 Sensible 점수를 받음 (DialoGPT 대답이 더 human-like 했음에도!) 이러한 문제를 개선하기 위해 답변이 sensible로 label되면, crowd worker에게 이 답변이 context에 맞게 충분히 specific한지를 평가함 Specificity 일단 sensible label이 된 상태여야 specificity label 할지를 고려함 반례: A says: “I love tennis,” B responds: “That’s nice,” label: “not specific” 올바른 예: A says: “I love tennis,” B responds: “Me too, I can’t get enough of Roger Federer!” label: “specific” GenericBot의 경우 어떠한 답변도 “specific” label을 받지 못했지만 DialoGPT의 경우 39%정도 “specific” label을 받음 평가할떄 들어가는 주관의 정도는 crowd worker의 agreement로 정량화 할수 있음 (The degree of subjectivity is somewhat quantified in the crowd worker agreement) 모든 모델 성능평가에 대한 crowd worker의 consistency를 측정하기 위해 agreement와 Krippendorff’s alpha (Krippendorff, 2011)를 사용함 (https://en.wikipedia.org/wiki/Krippendorff%27s_alpha) {: height=”50%” width=”50%”} Sensibleness와 Specificity를 하나의 metric으로 사용하기 위해 간단히 두 값을 평균냈고 이를 SSA (Sensibleness and Specificity Average)라 표현 점수: GenericBot: 35% DialoGPT: 51%{: height=”50%” width=”50%”} 2.2 Static Evaluation 모델 비교를 편하게 하기 위한 common benchmark가 필요했음 1,477 conversational contexts (with 1~3 conversation turn) 위의 조건을 만족하는 대화 데이터셋을 Mini-Turing Benchmark (MTB)라 정함 turn 개수에 따른 데이터 수 single-turn context (e.g., “How are you?”): 315 two-turn: 500 three-turn: 662 MTB에는 personality question도 포함됨 (e.g. “Do you like cats?”) 이때 personality는 일관성이 있어야함 For example, the context “A: Do you like movies?; B: Yeah. I like sci-fi mostly; A: Really? Which is your favorite?” expects a consistent response such as I love Back to the Future. On the other hand, a response like I don’t like movies would be a contradiction (context, response) pair에 대한 결과를 crowd workers에 보여준뒤 sensible and specific을 평가함 이때 static이라는 단어가 왜 붙냐면 contexts가 고정되어 있기 때문임 2.3 Interactive Evaluation static eval이 모델 비교하기엔 편하나, dataset에 따라 biased 한 경향이 있음 crowd workers가 1:1로 chatbot에게 아무거나 물어보고 싶은걸 물어보는 평가 모드로 하나 더 추가함 대화는 최소 14턴이 요구되고 (7턴은 챗봇), 최대로는 28턴까지 실험함 각 모델당 100개의 conversations을 수집함 (즉 최소, 700개의 labeled turns이 모델당 있는 것임) 2.4 Estimate of Human Performance internal company volunteers의 도움을 받아 100개의 human-human conversations을 수집했음 SSA를 위한 labeling은 위의 volunteers와는 상관없는 5명의 crowd workers의 majority voting으로 매 human turn마다 매겨짐 2.5 Evaluation of Cleverbot and DialoGPT Cleverbot은 API 사용했음 DialoGPT는 762M params짜리 공개된 모델을 사용함 (345M짜리 모델이 single-turn에서 가장 좋다고 알려져서 먼저 사용했었는데 762M 모델에 비해 multi-turn에서는 성능이 현저히 성능이 나빠서 762M 사용) DialoGPT authors가 decoding script를 공개하지 않아서, top-K (K=10) decoding을 적용했음 (코드는 huggingface의 Wolf가 공개한 구현체 씀) 위의 두 모델도 Meena와 같이 crowd sourcing으로 평가됨 2.6 Evaluation of Mitsuku and XiaoIce Mitsuku는 webapp을 사용해야했고 XiaoIce는 public API 없어서 interactive evaluation을 수행 자원봉사자들이 Mitsuku는 100개의 대화를, XiaoIce에서는 119개의 대화를 수집함 2.7 Automatic Evaluation quick research iterations을 위해, perplexity에 집중했음 기존의 평가방법과 달리, perplexity가 automatic metric가 됨 A seq2seq model outputs a probability distribution over possible next response tokens Perplexity measures how well the model predicts the test set data; in other words, how accurately it anticipates what people will say next. When interpreting perplexity scores, bear in mind that lower is better and that the theoretical minimum is one 3. Meena chatbot 기존의 e2e dialog models은 크게 두가지 종류로 나뉘었음 (1) complex models with human-designed components (2) large neural network models (known as end-to-end models) 핵심 질문!! (open research question) in order to reach a point where a model can carry out high-quality, multi-turn conversations with humans, could we simply take an end-to-end model and make it bigger—by adding more training data and increasing its parameter count—or is it necessary to combine such a model with other components? 본 논문이 제안하는 Meena Model (the largest end-to-end model)이 humanlike chat responses 를 open domain에서 할 수 있다는걸 보여줌으로써 위의 open research question에 답변이 가능할 것이라고 주장 3.1 Training Data Data는 public domain social media conversations을 정제해서 만듬 source data는 multiple speakers를 포함한 message trees 형태임 첫번째 메세지를 root로 삼고, 거기에 대한 답변이 child 노드가 됨 이런식으로 하면 대화할때 각 메세지의 conversation turn을 알 수 있음 대화속의 각 턴을 답변으로 하고 이전 턴들을 context로 하면 (context, response) pair training dataset을 만들 수 있음 generation quality를 높이기 위해, 다음과 같은 조건을 만족하면 메세지를 삭제함 subword의 개수가 2개보다 적거나 128개보다 많은 경우 alphabetic characters의 percentage가 70% 이하인 경우 message가 URL을 갖고 있는 경우 author’s username에 “bot”이 포함된 경우 메세지가 100번 이상 반복된 경우 parent’s text와 high n-gram이 겹치는 경우 commercial text classifier가 메세지가 안전하지 않거나 공격적이라고 분류한 경우 추가로 메세지 내에서 parent’s text를 인용한건 따로 제거했음 메세지가 제거된 경우 그 메세지 기준 sub-tree는 다 drop시킴 필터링 후에 남은 (context, response) pair의 개수: 867M tokenization: BPE with sentencepiece vocab size: 8K BPE subwords (이전 실험들에서 이정도 사이즈로도 답변을 생성하기에 충분하다는걸 확인함) Final Meena dataset 크기: 341GB of text (40B words) (GPT-2의 경우 40GB of internet text(8 million web pages) 사용) 3.2 Model Architecture{: height=”50%” width=”50%”} Meena model에서 가장 성능이 좋았던건, Evolved Transforemr (ET) (So et al., 2019) seq2seq model임 (with 2.6B parameters) 1 ET encoder block + 13 ET decoder blocks 사용 ET는 transformer를 기반으로한 evolutionary NAS architecture임 largest ET 모델은 10.2 perplexity를 기록했고 largest vanilla transformer는 10.7 perplexity를 기록함 (모두 738k step으로 고정했을 때) vanilla Transformer는 32 decoder layer를 사용했고 나머지 hyper params는 동일함 (transformer 논문에선 6개, 버트에선 base가 12개 large가 24개임) (이러한 이유 때문에 이렇게 맞춰준듯 An Evolved Transformer block is about twice as deep as a Transformer layer) 다른 모델과 비교하자면, extra-large GPT-2는 1.5B params을 갖고, LM 기반임(decoder only), DialoGPT의 경우엔 대화모델이고 762M params을 가짐 Meena’s hyper Params hidden size: 2,560 attention head: 32 share embeddings across the encoder and decoder encoder, decoder maxlen: 128 tokens (256 combined) optim 방법: manual coordinate-descent search 3.3 Training Details TPU-v3 Pod (2,048 TPU cores)에서 30일간 (또 이렇게 돈을 태웠..) 학습함 dataset: 40B words (or 61B BPE tokens) 2.6B-param model이 61B-token dataset에도 overfit됨 (모델 capa가 엄청 크다는걸 보여줌) 이러한 이유로 add a small amount of 0.1 attention and feed-forward layer dropout (0.1 attention을 더했다는게 뭐지..) 메모리를 아끼기 위해, Adafactor optimizer (Shazeer and Stern, 2018)를 사용했음 init lr: 0.01 (0~10k steps) decaying lr with the inverse square root of number of steps(10k step 이후 적용) Tensor2Tensor codebase 사용함 TPU 셋팅 TPU-v3의 core는 16GB of high-bandwidth memory를 가짐 메모리 사용량 최대로하고 각 코어당 8 training examples를 할당시킴 그 결과 1 step에 1초 정도 걸림 full TPU-v3 Pod에서 4M tokens를 1초에 학습시킬 수 있음 학습이 다 끝났을 때 모델은 164 epoch을 돌고 10T tokens을 봄(repeated tokens 포함) 3.4 Decoding generic and bland response를 생성하는건 neural conversational models에게 항상 문제였음 이러한 문제를 해결하기위한 common approach는 더 좋은 decoding algorithm을 사용하는 것 (reranking, conditioning, adversarial learning, variational autoencoding 등등) 제안 모델의 경우엔 충분히 low perplexity를 가지기 때문에 a simple sample-and-rank decoding strategy로도 diverse and high-quality responses 를 만들어낼 수 있음 Sample-and-rank Sample N independent candidate response (using plain random sampling with temperature T) candidate response중에서 highest probability를 갖는 걸 final output으로 선택 Temperature T는 hyper param이고 the next token에 대한 확률분포 $ p_{i} $를 regulate함 Hinton et al. (2015)이 했던것 처럼 logits $ z_{i} $ 를 softmax 계산하기 전에 T 나눴음$$p_{i}=\\frac{\\exp \\left(z_{i} / T\\right)}{\\sum_{j} \\exp \\left(z_{j} / T\\right)}$$ T 값에 따른 변화 T = 1 이면, unmodified distribution임 T가 커지면, contextually rare tokens 더 볼 수 있음 (relevant entity names) T가 작아지면, common words가 더 많이 나옴, 안정적이나 specific은 떨어짐 (예: 관사, 전치사) “Why do you like the ocean?” 이라는 질문이 input으로 있을 때 결과 beam search는 반복적인 답변, uninteresting 답변이 많지만 sample-and-rank의 경우 다양한 답변 및 context-rich 답변이 가능 예시{: height=”50%” width=”50%”}{: height=”50%” width=”50%”} Key point! with low perplexity so samples can be taken at high temperature to produce human-like content sample-and-rank에서 N = 20, T = 0.88로 셋팅 Figure 1을 보면, decoding strategy를 고정했을때 perplexity를 개선하면 SSA가 높아지는걸 볼 수 있음{: height=”50%” width=”50%”} 3.5 Sample conversations Human과 Meena간의 대화 샘플 (체리피킹이라고 저자가 직접 써놓았음) 대화 생성때는 sample-and-rank를 사용했음 Meena가 open-domain에서 대화를 그럭저럭 잘하지만 “Is it indoors or outdoors?”라고 묻는 부분을 보면 not sensible한 부분도 있음을 확인할 수 있음 첫번째 예제:{: height=”50%” width=”50%”} 두번째 예시: context 를 사용해서 대화하기도함 (내용이 실제로 맞음..){: height=”50%” width=”50%”} 세번째 예시: 철학얘기하는 챗봇{: height=”50%” width=”50%”} 네번째 예시: 멀티턴 환경에서 농담하는 챗봇{: height=”50%” width=”50%”} 자세한 대화 데이터셋은 Github 참고: https://github.com/google-research/google-research/tree/master/meena/ 4. Results test perplexity와 human evaluation metric, SSA 간의 correlation에 대해 다루려함 4.1 SSA-perplexity correlation the number of layers, attention heads, total training steps 등을 바꾸기도 하고 ET쓸지 regular Transformer 쓸지, hard labels 쓸지 soft labels 등등 고민하며 실험했음 static eval에서 correlation이 거의 선형으로 보였지만 lower values of perplexity에서도 잘되는지 확인하고 싶어서 interactive eval도 수행했고 잘나온걸 확인 (dataset으로 인한 bias 문제는 없다고 주장할 수 있게 됨){: height=”100%” width=”100%”}{: height=”50%” width=”50%”} the lowest perplexity model was evaluated 7 times with static evaluations and also 7 times with interactive evaluations. 5. Further Advancing SSA 72% ± 1%, for Meena (base)의 성능을 얻었지만 decoding strategy와 rule 추가로 성능을 79% ± 1%, for Meena (full)까지 개선해보고자 함 5.1 Advancing Decoding temperature T와 top-k를 다르게 주면서 디코딩 영향을 평가해봄 top-k = 40, T = 1.0, N = 20: SSA 72% top-k = 40, T = 0.88, N = 20: SSA 74% sample-and-rank에서 N을 {1,20,400}으로 바꿔가며 평가해봄 N = 1일 때보단 N = 20일때 유의미하게 좋아짐 (SSA +10%) N = 400 일땐 오히려 sensibleness가 안좋아짐 5.2 Addressing Cross-turn Repetitions Cross-turn Repetitions이란 특정 턴에서 이전 턴의 결과를 반복하는걸 의미함 이전껄 반복하기도하고, 답변안에서 모순이 있기도함 (“I like pizza, but I don’t like it”) perplexities가 안좋은 Meena 버전에서 잘 발견되는 현상임 (base 모델에서는 잘 안보이는 현상이긴 함) 이를 해결하기 위해 rule을 도입했고 대화 중 2개의 턴에서 long comon sub-sequences를 갖고 있으면 해당 candidates를 제거하게 함 이를 통해 SSA 성능이 74% -&gt; 79%로 올라감{: height=”50%” width=”50%”} 5.3 Safty Layer full Meena 버전의 경우, 내용적으로 민감한 내용들은 filtering mechanism을 적용한 classifier layer를 추가해서 걸렀음 (evaluation 및 실제 대화할 때) 6. Related Work Human evaluation과 correlation을 갖는 automatic metric을 찾는건 open-domain conversational modeling에서 매우 중요한 goal이었음 BLEU, ROUGE, 기타 등등이 있었지만 dialog에는 맞지 않음이 밝혀졌었음 learnable metric을 구축하려는 시도도 있었지만 human labels이 필요하거나 unsupervised approaches를 사용했고 이는 더 복잡하거나 따로 training이 필요했음 (e.g. ranking system) 본 연구에서는 any neural seq2seq model에서 사용 가능한 perplexity가 human evaluation과 강한 correlation을 갖는다는 걸 확인함 DialoGPT 등에서 했던 evaluation setting은 single-turn dialog였지만 본 연구에서는 3 turns까지 가능한 static MTB benchmark와 14 turn까지 가능한 interactive setup으로 평가함 7. Discussion public domain social media conversations에 대한 perplexity가 good automatic proxy for human judgement (sensibleness and specificity 관점에서)가 될 수 있다는 걸 제안함 MTB (one to three-turn context)는 first turn에 biased 될 수 있고 context도 많지 않음 주로 다루는 주제도 다음과 같음 common sense, basic knowledge, asking/sharing about personality, likes/dislikes, opinions, feelings, hobbies, pleasantries, etc deeper question answering은 불가능 (e.g., how fast is a cheetah) Human-likeness란 incredibly broad and abstract concept임 이러한 이유로 interactive evaluation을 함 (14 to 28 turns) Sensible and specificity 외에 human-like conversation attribute를 확장할 필요가 있음 humor empathy deep reasoning question Answering knowledge discussion skills Sensible and specificity 경우 sub-component로 쪼갤 수 있음 logical personality consistency common sense relevance basic factual correctness Future work는 explore the continued optimization of sensibleness via the optimization of test set perplexity","link":"/2020-02-06-Towards_a_Human_like_Open_Domain_Chatbot/"},{"title":"Elastic Search 정리","text":"아파치 루씬 기반의 Java 오픈소스 분산 검색엔진임 ES를 통해 루씬 라이브러리를 단독으로 사용할 수 있게됨 많은 양의 데이터를 빠르게, 거의 실시간(NRT, Near Real Time)으로 저장, 검색, 분석할 수 있음 ELK 스택이란 다음과 같음 Logstash 다양한 소스(DB, csv, …)의 로그 또는 트랸쟉션 데이터를 수집, 집계, 파싱하여 ES로 전달 Elasticsearch Logstash 로 전달 받은 데이터를 검색 및 집계해서 필요한 관심 정보를 획득 http://localhost:9200/ Kibana ES의 빠른 검색을 통해 데이터를 시각화 및 모니터링함 키바나는 JVM에서 실행되는 엘라스틱서치와 로그스태시와 달리 node.js로 실행하는 웹애플리케이션임 http://localhost:5601/ {: height=”50%” width=”50%”} RDB와 Es 비교 Database -&gt; Index Table -&gt; Type column -&gt; Field row -&gt; Document{: height=”50%” width=”50%”}{: height=”50%” width=”50%”} ES 아키텍쳐 / 용어 정리{: height=”50%” width=”50%”} 클러스터 노드들의 집합 서로 다른 클러스터는 데이터의 접근, 교환이 불가 노드 ES를 구성하는 하나이 단위 프로세스임 역할에 따라 Master-eligible, Data, Ingest, Tribe 등으로 구분 가능 Master-eligible: 클러스터 제어하는 마스터 노드 (인덱스 생상, 삭제 / 클러스터 노드 추적, 관리 / 데이터 입력시 어느 샤드에 할당할지 결정) Data node: CRUD 작업과 관련있는 노드 (CPU, 메모리를 많이 써서 모니터링 필요함, Master node와 분리되는 것이 좋음) Ingest node: 데이터 변환, 사전 처리 파이프라인 Coordination only node: 로드밸런서와 비슷한 역할 인덱스 (index), 샤드 (Shard), 복제 (Replica) 인덱스: RDB의 DB와 대응됨 샤드: 데이터 분산해서 저장하는 방법임. scale out을 위해 index를 여러 shard로 쪼갬. 기본적으로는 1개 존재하고 나중에 개수 조정가능 복제: 또 다른 형태의 shard라 할 수 있음. 노드를 손실했을 경우 데이터 신뢰성을 위해 샤드들 복제하는 것. 그러므로 replica는 서로 다른 노드에 존재하는 것이 좋음 {: height=”50%” width=”50%”} ES 특징 Scale out: 샤드를 통해 규모가 수평적으로 늘어날 수 있음 고가용성: replica를 통해 데이터 안정성 보장 Schema Free: json 문서를 통해 데이터 검색을 수행하므로 스키마 개념이 없음 Restful: 데이터 CRUD 작업은 HTTP Restful API를 통해 수행함 {: height=”50%” width=”50%”} 예시 (document (row) 생성) 1# curl -XPOST 'localhost:9200/victolee/blog/1?pretty' -d '{&quot;postName&quot; : &quot;elasticsearch&quot;, &quot;category&quot; : &quot;IT&quot;}' -H 'Content-Type: application/json' -d 옵션 추가할 데이터를 json 포맷으로 전달합니다. -H 옵션 헤더를 명시합니다. 예제에서는 json으로 전달하기 위해서 application/json으로 작성했습니다. ?pretty 결과를 예쁘게 보여주도록 요청 결과: 이렇게 curl 요청을 하면, victolee 인덱스에, blog 타입으로 id 값이 1인 document가 저장됨 {: height=”50%” width=”50%”} 역색인 (Inverted Index) https://www.slideshare.net/kjmorc/ss-80803233 키바나에서 데이터 삽입 예제 1234567PUT /my_playlist/song/6{&quot;title&quot; : &quot;1000 years&quot;,&quot;artist&quot; : &quot;Christina Perri&quot;,&quot;album&quot; : &quot;Breaking Dawn&quot;,&quot;year&quot; : 2011} 명령어 설명 my_playlist : 여러분의 데이터가 들어갈 인덱스의 이름입니다. song : 만들어질 document의 이름입니다. 6 : 엘리먼트 인스턴스의 아이디입니다. 이 경우에는 song id입니다. 만일 my_playlist가 존재하지 않았다면, 새로운 인덱스인 my_playlist가 만들어짐. document인 song과 id인 6도 똑같이 만들어짐. 값을 업데이트 하기 위해서는 PUT 명령어를 동일한 document에 사용하면 됨. 새로운 필드도 추가 가능함 {: height=”50%” width=”50%”} GET 명령어 쓰면 값 불러옴 1GET /my_playlist/song/6 {: height=”50%” width=”50%”} 데이터 선택하는 조건문 예시 1234567891011121314# state가 UT인 데이터 가져오기GET /bank/_search?q=state:UT# state가 UT이거나 CA인 데이터 가져오기GET /bank/_search?q=state:UT OR CA# state가 TN이면서 여성(female)인 데이터 가져오기GET /bank/_search?q=state:TN AND gender:F# 20살보다 많은 나이를 가진 사람들 가져오기GET /bank/_search?q=age:&gt;20# 20살과 25살 사이의 데이터 가져오기GET /bank/_search?q=age:(&gt;=20 AND &lt;=25) 좀 더 복잡한 질의 1234567891011121314151617# address 필드에서 Street이라는 단어가 포함되어야 함# gender 필드에서 f가 정확히 일치하여야 함# age 필드에서 숫자는 25보다 크거나 같아야 함GET /_search{&quot;query&quot;: { //1 &quot;bool&quot;: { //2 &quot;must&quot;: [ { &quot;match&quot;:{&quot;address&quot;:&quot;Street&quot;}} //3 ], &quot;filter&quot;: [ //4 { &quot;term&quot;:{&quot;gender&quot;:&quot;f&quot;}}, //5 { &quot;range&quot;: { &quot;age&quot;: { &quot;gte&quot;: 25 }}} //6 ] }}} Kibana 데이터는 ES에 올라가 있어야함 ES 인덱스(DB)에 저자된 데이터를 키바나가 인식할 수 있도록 인덱스를 설정해야함 데이터 복구 스냅샷을 이용해야함~! https://kay0426.tistory.com/46 출처: https://victorydntmd.tistory.com/308 https://velog.io/@jakeseo_me/%EB%B2%88%EC%97%AD-%EC%97%98%EB%9D%BC%EC%8A%A4%ED%8B%B1%EC%84%9C%EC%B9%98%EC%99%80-%ED%82%A4%EB%B0%94%EB%82%98-%EC%8B%A4%EC%9A%A9%EC%A0%81%EC%9D%B8-%EC%86%8C%EA%B0%9C%EC%84%9C https://12bme.tistory.com/486","link":"/2020-05-27-elastic_search/"},{"title":"Deeper Text Understanding for IR with Contextual Neural Language Modeling&quot;","text":"목차 Author Abstract Introduction Related Work Document Search with BERT Experimental Setup Results and Discussion Conclusion Author CMU 박사괴정 (https://www.cs.cmu.edu/~zhuyund/) IR에 적용하는 Language Understanding쪽 연구 Three papers in deep retrieval and conversational search got accepted into SIGIR 2020! {: height=”50%” width=”50%”} Abstract 뉴럴넷은 복잡한 언어 패턴과 query-document relation을 자동으로 학습할 수 있는 새로운 가능성을 제공하고 있음 Neural IR models은 query-document relevance pattern을 학습하는데 좋은 결과를 보여주지만, query 또는 document의 text content를 이해하는 것에 대한 연구는 많지 않았음 (?) 본 논문에서는 최근에 제안되었던 contextual neural LM, BERT 등이 IR에서 deeper text understanding에 얼마나 효과 있는지를 알아보고함 실험 결과는 전통적인 word embedding보다 BERT가 제공하는 contextual text representations이 더 효과있음을 보여주었음 BoW retrieval 모델에 비해 contextual LM은 더 나은 language structure를 사용하고, 자연어 형태의 query에 대해 큰 성능향상을 가져올 수 있음 text understanding ability를 search knowledge와 결합시키는 것은 제한적인 학습셋을 갖는 조건에서 search task를 Ptr BERT가 더 잘할 수 있게 해줌 (정확한해석은 아닌데 대략 이런의미) Introduction Text retrieval은 문서의 의미를 이해하고 search task를 이해하는게 요구됨 뉴럴넷은 raw document text와 학습셋으로부터 understanding을 얻어내기 때문에 매력적인 솔루션임 대부분의 뉴럴 IR 방법은 query-document relevance patterns을 학습하는데 초점을 맞춤 (다른 말론 search task에 대한 knowledge) 하지만 relevance patterns만을 학습한다는 것은 많은양의 학습 데이터를 필요로 한다는 의미이고, 여전히 tail queries나 new search domain에 generalize되기 어려움 Ptr word representation (such as word2vec) 등은 뉴럴 IR에서 많이 사용되어왔음 하지만 이런 word co-occurrence 방법론은 text에 대해 shallow bag-of-words 정도의 정보임 최근엔 ELMo, BERT 같은 Ptr neural LM 등의 발전이 있었고 기존의 전통적인 word embeddings과 달리 contextual representation을 제공함 이런 contextual LM은 전통적인 word embeddings들의 성능보다 뛰어남을 여러 NLP task에서 보여줌 이런 모델은 IR에 새로운 가능성을 가져다줌 본 논문에서는 BERT를 이용해서 ad-hoc document retrieval 에 적용해봄 (two ad-hoc retrieval datasets에 적용) 적은 데이터로 finetuning해도 기존 baseline을 크게 뛰어넘음을 보여줌 전통 retrieval models과 다르게 longer natural language queries가 short keywords queires보다 좋은 성능을 보여줄 수도 있었음(by large margines with BERT) 더 분석해본 결과, stopwords, punctuation등 전통 IR 방법에선 무시했던 것들이 문법적 요소와 단어 의존성으로 인해 natural langauge queries를 이해하는데 핵심 역할을 한다는 것도 드러남 최종적으로, BERT를 search knowledge from a large search log 로 개선(?)해서 text understanding도 하고 search task도 하게 만듬 (labeled data가 제한적인 경우에 도움됨) Related Work query-document relevance patterns approach 1 text presentations tailored for the search task [1, 2, 9] with search signals from click logs pseudo-relevance feedback approach 2 neural architecture로 다양한 matching feature를 잡아내는 것 exact match passage-level signals 위와 다르게, query/document의 text content를 어떻게 이해하는가에 대한 연구는 많이 없는 상태임 사용해도 word2vec 정도였음 BERT가 잘되니 적용해보겠음 (open-domain document에 학습하다보니 general pattern 학습함) Document Search with BERT{: height=”50%” width=”50%”} 본 논문에서는 off-the-shelf BERT architecture를 사용 sentence pair classification architecture를 의미 마지막 레이어에서 binary classification을 통해 relevance의 확률을 예측함 Passage-Level Evidence BERT를 긴 문서들에 적용하면 메모리와 복잡도 등이 증가하게됨 senence-trained model이니 long text에 효과가 덜할 수도 있음 이 때문에 문서를 overlapping passages로 나눔 neural ranker는 각각의 passage에 대해 독립적으로 relevance를 예측함 (같은 문서지만 쪼갰으니 여러번 계산하는 듯..?!) document score is the score of the first passage (BERT-FirstP), the best passage (BERT-MaxP), or the sum of all passage scores (BERT-SumP) Augmenting BERT with Search Knowledge search task는 다음 두가지를 모두 요구함 general text understanding e.g. Honda is a motor company more-specific search knowledge e.g. people want to see special offers about Honda BERT는 genral langauge patterns을 배우긴 했지만, search knowledge는 labeld search data로부터 학습해야만함 이런 종류의 데이터는 매우 expensive하고 모으는데 시간이 걸림 이는 pre-trained ranking model (언어 이해지식과 검색 지식 모두 갖고 있는) 을 요구하게함 BERT를 large search log 를 통해 튜닝해서 search knowledge를 포함하도록 augmentation함 이렇게하면 데이터가 검색에서 적은 케이스에 도움이 될 것으로 기대함 Experimental Setup Datasets Robust04 news corpus (article, ptr corpus에 가까움) 0.5M documents and 249 queres 두가지 버전의 queries로 구성됨 short keyword query (title) longer natural language query (description) relevance assessment에 대한 narrative 포함 ClueWeb09-B web pages (tables, navigation bars, discontinuous text) 50M web pages and 200 queries For augmenting BERT with search data, we follow the domain adaptation setting from Dai et al. [1] and use the same Bing search log sample. The sample contains 0.1M queries and 5M query-document pairs.{: height=”50%” width=”50%”} Baselines and Implementations Unsupervised baselines Indri’s bag of words (BOW) sequential dependency model queries (SDM) Learning-to-rank baselines RankSVM Coor-Ascent with bag-of-words features Neural baselines DRMM word2vec 사용함 2개의 데이터셋에선 성능이 젤 잘 나왔던 neural models임 Conv-KNRM n-gram embeddings for search task large search log로 학습할때 좋은 성능 나옴 Bing search log 이용해서 만들면 SOTA임 (근데 왜 표엔 없지) baseline들은 stopword 지우고 stemming 했지만 BERT는 raw text 사용함 Supervised models은 BOW with 5-fold cross-validation을 사용해서 검색된 top 100 documents를 re-rank함 (정확히 어떻게 한다는거지) Results and DiscussionPre-trained BERT for Document Retrieval Robust04에서 BERT는 지속적으로 베이스라인보다 tite query에 대해서는 10% margin으로 description query에 대해서는 20% margin으로 더 나은 성능을 보여줌 ClueWeb09-B에서는 BERT는 Coor-Ascent와 title query에서는 비슷하지만 description query에서는 더 좋은 성능을 보여줌 위 결과를 종합하면 description queries에서는 BERT가 효과가 있음 {: height=”50%” width=”50%”} Sources of effectiveness two layers from the BERT-MaxP model when predicting the relevance between a description query ‘Where are wind power installations located?’ and a sentence ‘There were 1,200 wind power installations in Germany’ layer에서 exact match, Bigram (prev, next) 등을 학습한 걸 볼 수 있음 where-in 매칭은 context를 고려한다고 할 수 있음 (전통 IR에서는 이런 단어들은 무시함 (IDF가 낮아서)) 이런걸 보면 stopwords도 사실 relevance에 중요한 단서가 될 수 있음을 보여줌 {: height=”50%” width=”50%”} Title queries vs. description queries 정리하면, description queries가 title queries를 이번 연구처럼 large margin을 갖고 이긴게 거의 처음임 On Robust04, using description queries with BERT-MaxP brings a 23% improvement over the best title query baseline (SDM) Most other ranking methods only get similar or worse performance on descriptions compared to titles. To the best of our knowledge, this is the first time we see that description queries outperform title queries with such a large margin Understanding Natural Language Queries 3가지 종류의 질의로 text understanding을 검사함 title description narrative (removing stopwords and punctuation) BERT-MaxP makes large improvement on longer queries by modeling word meaning and context. {: height=”50%” width=”50%”} Understanding the Search Task Corpus-trained text representation이 꼭 search task와 align되는건 아님 Its pre-trained language model encodes general word associations like (‘Honda’, ‘car’), but lacks search-specifc knowledge like (‘Honda’, ‘special offer’) search specific knowledge가 요구됨 데이터가 부족할 수 있음 이걸 해결해야함 if BERT’s language modeling knowledge can be stacked with additional search knowledge to build a better ranker, and if the search knowledge can be learned in a domain-adaptation manner to alleviate cold-start problems BERT를 Bing search log with 0.1M queries 샘플에서 학습시키고 ClueWeb09-B에 finetuning시킴 결과적으로 Bing search log로 학습하면 성능이 더 개선됨 {: height=”50%” width=”50%”} Conclusion Text understanding is a long-desired feature for text retrieval Contextual neural language models open new possibilities for understanding word context and modeling language structures BERT가 search task에서 적용 잘되고 성능도 높여줌 We found that queries written in natural language actually enable better search results when the system can model language structures","link":"/2020-05-26-Deeper%20Text%20Understanding%20for%20IR%20with%20Contextual%20Neural%20Language%20Modeling/"},{"title":"BERT-based Lexical Substitution","text":"Author 논문 매우 많이 씀 AAAI, ACL, ICLR 등 탑티어 컨퍼런스 논문냄 마소에서 인턴했고 2021 fall 박사과정 자리 구하는중 (아직 석사라는 뜻) 개인블로그 운영: https://michaelzhouwang.github.io/ {: height=”50%” width=”50%”} {: height=”30%” width=”30%”} 저자: Wangchunshu Zhou, Ke Xu (Beihang University) Tao Ge, Furu Wei, Ming Zhou (Microsoft Research Asia) Abstract 이전 연구들은 lexical resources (e.g. WordNet)으로 부터 타겟의 동의어를 찾아서 substitute candidates를 얻어서 context를 보고 랭킹하는 식의 연구였음 이런 연구들은 두가지 한계점이 있음 타겟 단어의 synonyms 사전에 없는 good substitute candidates를 찾아내지 못함 substitution이 문장의 global context에 주는 영향을 고려하지 못함 이 문제를 해결하기 위해, end-to-end BERT-based lexical substitution approach를 제안함 annotated data or manually curated resources 없이 만든 substitute candidates 제안하고 검증함 target word’s embedding 에 dropout 적용해서 target word’s semantics and contexts for proposing substitute candidates를 고려할 수 있게함 SOTA 찍음 (LS07, LS14 benchmark) Introduction Lexical substitution은 문장의 의미를 바꾸지 않고 단어를 바꿔주는 task를 말함 text simplication and paraphrase generation task와 비슷함 비슷한 단어 찾기와 맥락 의미 유지 두가지가 중요한데 대부분의 선행 연구는 첫번째 (동의어로 교체)에 초점이 맞춰짐 {: height=”40%” width=”40%”} 하지만 사전같은 리소스는 제한되어 있고, 바꾼 단어가 문장에 어떤 영향 주는지를 고려못함 BERT-based Lexical SubstitutionSubstitute Candidate Proposal 마스킹해버리면, 문장의 맥락상으론 맞지만 단어적으로는 틀린 단어 생성함 반대로 마스킹 안하면 거의 99.99% 의 확률로 original target word를 예측함 위 두개 방법의 trade-off를 고려해서 embedding dropout 하는걸 선택 문장이 있을때 문장에서 k 번째 단어를 나타내는 표현을 아래와 같이 한다고 할 때 $${\\boldsymbol{x}=\\left(x_{1}, \\cdots, \\underline{x}{k}, \\cdots, x{L}\\right)}$$ proposal score $s_{p}\\left(x_{k}^{\\prime} | \\boldsymbol{x}, k\\right)$ 는 $x_{k}$의 대체 단어로 $x_{k}^{\\prime}$ 를 선택하는 점수임 $s_{p}\\left(x_{k}^{\\prime} | \\boldsymbol{x}, k\\right)=\\log \\frac{P\\left(x_{k}^{\\prime} | \\widetilde{\\boldsymbol{x}}, k\\right)}{1-P\\left(x_{k} | \\widetilde{\\boldsymbol{x}}, k\\right)}$ $P\\left(x_{k} | \\boldsymbol{x}, k\\right)$ 는 x라는 주어진 문장이 있을 때 k 번째 단어가 예측될 확률임 $P\\left(x_{k}^{\\prime} | \\widetilde{\\boldsymbol{x}}, k\\right)$ 이거는 k 번째 단어가 partially masked with embedding dropout인 경우임 분모 나눈건 origin word 의 확률을 빼줘서 normalize 해준 것임 {: height=”50%” width=”50%”} python1234567891011token_prediction_scores = prediction_scores[문장내에서 변경할 토큰 위치, bert_token_index] origin_token_index = bert_token_ids[bert_token_index]# 원래 토큰이 UNK면 그냥 UNK 확률 나오도록 살림if origin_token_index != tokenizer.unk_token_id: token_prediction_scores[origin_token_index] = -1e9 # 원래 token이 다시 생성되지 않도록 마스킹token_prediction_softmax_scores = F.softmax(token_prediction_scores)cand_probs = token_prediction_softmax_scoressum_probs = torch.sum(cand_probs)proposal_scores = torch.log(cand_probs / sum_probs) Substitute Candidate Validation 단어가 이상하게 선택되면 원래 문장과 의미가 달라지니, before after의 contextual representation을 비교하고자함 BERT의 top four layer 의 representation을 concat해서 사용하겠음 단어가 교체된 문장은 다음과 같이 씀$$\\boldsymbol{x}^{\\prime}=\\left(x_{1}, \\cdots, x_{k}^{\\prime}, \\cdots, x_{L}\\right)$$ 이때 validation score는 다음과 같음$$s_{v}\\left(x_{k}^{\\prime} | \\boldsymbol{x}, k\\right)=\\operatorname{SIM}\\left(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime} ; k\\right)$$ SIM은 두 문장에 대한 BERT’s contextualized representation similarity 임$$\\operatorname{Sim}\\left(\\boldsymbol{x}, \\boldsymbol{x}^{\\prime} ; k\\right)=\\sum_{i}^{L} w_{i, k} \\times \\Lambda\\left(\\boldsymbol{h}\\left(x_{i} | \\boldsymbol{x}\\right), \\boldsymbol{h}\\left(x_{i}^{\\prime} | \\boldsymbol{x}^{\\prime}\\right)\\right)$$ $\\boldsymbol{h}\\left(x_{i} | \\boldsymbol{x}\\right)$ 는 BERT의 i 번째 토큰에 대한 contextualized representation임 $\\Lambda(\\boldsymbol{a}, \\boldsymbol{b})$ 은 a, b 벡터의 cosine similarity임 $w_{i, k}$ 은 i-&gt;k 방향의 모든 head에 대한 self-attention score의 평균임 Substitute Candidate Validation 음..오른쪽은 cosine으로 벡터간 유사도 보겠다는 거고 왼쪽은 attention으로 그것의 중요도를 대략 계산해보겠다는 거 같군 이렇게하면 예제에 나왔던 hot and touch 등의 단어는 validation (s_v) 점수가 낮게 나와서 랭킹에서 떨어지고 반면에 powerful 같은 단어는 높게 나와서 랭킹에서 올라감 최종적으로 다음과 같이 점수를 linear combination 해서 단어를 선택할 수 있음 $$s\\left(x_{k}^{\\prime} | \\boldsymbol{x}, k\\right)=s_{v}\\left(x_{k}^{\\prime} | \\boldsymbol{x}, k\\right)+\\alpha \\times s_{p}\\left(x_{k}^{\\prime} | \\boldsymbol{x}, k\\right)$$ python12345678910111213141516171819202122232425input_ids = torch.tensor(token_ids_list, device=device) with torch.no_grad(): last_hidden_states, pooler_output, hidden_states, attentions = bert_model(input_ids) num_of_top_hidden_use = 4 # top 4 layer stacked_token_attentions = torch.stack([ attention[0].mean(axis=[0]) for attention in attentions[-num_of_top_hidden_use:] ]) # layer 별로도 평균 token_attentions = torch.mean(stacked_token_attentions, axis=0) # hidden layer 'num_of_top_hidden_use' 개수만큼 합치자 subj_h = torch.cat([ hidden[0] for hidden in hidden_states[-num_of_top_hidden_use:] ], dim=-1) list_of_cand_h = [] for i in range(1, len(token_ids_list)): list_of_cand_h.append(torch.cat([ hidden[i] for hidden in hidden_states[-num_of_top_hidden_use:] ], dim=-1)) output = [] for cand_i, cand_h in enumerate(list_of_cand_h): cos_sims = cos(subj_h, cand_h) change_token_index = list_of_change_token_index[cand_i] w_i_to_k = token_attentions[:,change_token_index] # 논문대로 i to k attention만 남기기 weighted_cos_sims = w_i_to_k * cos_sims validation_score = torch.sum(weighted_cos_sims) # cls, sep 제거안함 output.append(validation_score.item()) ExperimentsExperimental Setting target word’s embedding dropout ratio: 0.3 weight alpha: 0.01 num candidates: 50 Experimental Results original BERT는 기존 sota를 이길 수 없음 (Table 2 참고) embedding dropout 후 SOTA 먹음 GAP score 라는 걸로 평가함 (MAP의 변형임){: height=”50%” width=”50%”}{: height=”50%” width=”50%”}{: height=”50%” width=”50%”} Conclusion annotated data and manually curated resources 없이도 결과 잘 나옴 end-to-end 모델이고 sota임","link":"/2020-06-18-bert_based_lexical_substitution/"},{"title":"Document Expansion by Query Prediction","text":"Author 저자: Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho (New York University, Facebook AI Research), 2019 조경현 교수님과 co-work 같은 년도에 쓴 Passage Re-ranking with BERT도 인용수가 높은 편 (4페이지 짜리인데도){: height=”50%” width=”50%”} 느낀점 요즘엔 T5로 시도한 방법들이 결과가 좋다고 나오고 있음 DE (document expansion) 관련 논문들이 은근히 없다.. 다 QE (Query expansion) BERT를 검색에 적용한 논문들은 거의 re rank 수준.. inverted index에 적용한건 거의 없고 약간 흑마법처럼 보이기도.. 참고 https://paperswithcode.com/paper/document-expansion-by-query-prediction https://github.com/castorini/docTTTTTquery Abstract 검색을 효과적으로 개선하는 방법중 하나는 문서 텀을 확장하는 것임 QA 시스템의 관점에서는 문서가 질문을 잠재적으로 포함한다고도 볼 수 있음 (query, relevant documents) pair 셋으로 seq2seq 모델 학습해서 query 예측하는 방법 제안 re-ranking component와 결합하면 two retrieval task에서 SOTA 결과 나옴 Introduction query term과 relevant doc 간의 “vocabulary mismatch” problem는 IR에서 메인 challenge중 하나임 neural retrieval models이 등장함에 따라 이 문제는 대부분 query expansion 기술쪽으로 적용됨 개선된 랭킹을 위해 semantic level에서 문서와 매칭을 시도했지만 대부분 규모가 큰 검색엔진에서는 initial retreival 단계에서 exact term match를 사용함 query representation은 풍부해지기 시작했지만 document representation은 그에 비해 변화가 크게 없었음 본 논문에서는 document representation을 개선하는 방법쪽으로 접근함 This is the first successful application of document expansion using neural networks that we are aware of MS MARCO dataset에서 best result 기록 DE의 장점: indexing 가능 눈에 띌만한 개선효과 거둠 Method: Doc2query 제안 방법은 &quot;Doc2query&quot;라 칭함 seq-to-seq transformer model로 (query, relevant document) 학습 &amp; 생성 Moses tokenizer로 한번 짜른 뒤 BPE로 토크나이징함 (생성되는게 BPE면 어떡하나?) document는 앞에서 400 토큰까지 사용하고 query는 100 토큰까지 사용 top-k random sampling 을 통해 10개의 질의를 예측함 확장된 문서가 인덱싱 되고 나면 BM25를 통해 결과 출력함 BERT 이용해서 re-rank 함 (option){: height=”50%” width=”50%”} Experimental Setup모델 학습과 평가를 위해 두가지 데이터셋 활용 MS MARCO: a passage re-ranking dataset with 8.8M passages obtained from the top-10 results retrieved by the Bing search engine (from 1M queries). tr: 500k pairs (query, relevant document) 질의는 평균적으로 한개정도의 relevant passage를 가짐 dev &amp; test: 6,900 queries (dev set은 doc 포함) TREC-CAR: the input query is the concatenation of a Wikipedia article title with the title of one of its sections. The ground-truth documents are the paragraphs within that section. 데이터셋은 크게 5개의 predefined fold로 구성됨 tr: 앞의 4개 fold (3M queries) val: 1개 fold (700k queries) test: 2,250 quries 평가 모델 BM25 BM25 + Doc2query RM3 (query expansion) BM25 + BERT BM25 + Doc2query + BERT Results BM25가 베이스라인임 예시 문장은 아래와 같음 (이정도로 잘되진 않을 것 같은데, 문서 도메인이 중요할 듯) input document에서 몇 단어를 copy하는 경향 있음 term re-weighting에 효과적일 수 있다 (중요한 단어를 재생산하는 것이니?!) -&gt; 약간 귀에걸면 귀걸이 코에걸면 코걸이 같기도.. 문서에 없던 단어도 생성하기도함 비율은 대략 69:31 = 기존단어:새단어 대략 10개 query로 top-k random sampling &gt; beam search{: height=”50%” width=”50%”} {: height=”50%” width=”50%”} {: height=”50%” width=”50%”} Conclusion neural net 기반 document expasnion의 성공적인 첫 사례다! rich input signal이 있는 longer document의 경우 neural models은 document expansion에 매우 유용하다 runtime이 아닌 indexing time에 자원을 투자할 수 있음 OpneNMT, Anserini(BM25), TensorFlow BERT 써서 구현함","link":"/2020-10-22-DocumentExpansionByQueryPrediction/"},{"title":"(ALiBi) TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION","text":"Ref https://github.com/ofirpress/attention_with_linear_biases huggingface 구현 https://github.com/huggingface/transformers/blob/ad28ca291bf851b48d7f2d4becf96ca90c98f8f1/src/transformers/models/bloom/modeling_bloom.py#L96 Author 저자: Ofir Press1,2 Noah A. Smith1,3 Mike Lewis21Paul G. Allen School of Computer Science &amp; Engineering, University of Washington 2Facebook AI Research 3Allen Institute for AI 요약 extrapolation 잘됨 11% 빠름 속도, 11% 메모리 적게씀 동일 길이로 학습한 모델 대비 짧은 길이로 학습해도 ppl 유지됨 구현도 간단하다 position embedding 지우고 대신에 길이에 linear하게 비례해서 attention score 깎아버리자! Abstract 이 질문에 대한 답변이 요구되어져왔음 how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? position representation만 변경하면됨 Attention with Linear Biases (ALiBi) not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. 1.3B모델에서 1024 -&gt; 2048로 바꿔도 2048모델을 sinusoidal position embedding으로 학습한 모델과 pl 같았고, 학습속도는 11%빠르고 11% 메모리 적게 사용함 여기서 말하고자 하는건 긴 길이로 사용할꺼면 학습도 거기에 맞게 해놔야된다 라는 것 같음 맨 처음 질문 자체가 짧은 길이에 대해서 학습한 후 긴길이에 대해서 사용해도 되냐는거 였으니 ALiBis’ inductive bias는 token\u001d의 recency(거리)와 관련있는데, 이게 WikiText-103 benchmark에 있던 multiple strong position methods보다 성능이 좋았음 Introduction More context, achieved by larger L, improves predictions at inference time. But longer sequences are more expensive to train on We define extrapolation as a model’s ability to continue performing well as the number of input tokens during validation increases beyond the number of tokens on which the the model was trained transformer language models (LMs) that use sinusoidal position embeddings have very weak extrapolation abilities; see Figure 1. 최근에 나온 포지셔널 인코딩은 성능 좋긴함.. 하지만!! 느리고 추가 메모리 필요함 However, the better of these, the T5 bias, is considerably slower than the sinusoidal approach and uses extra memory and parameters (Figure 2). introduce Attention with Linear Biases (ALiBi) to facilitate efficient extrapolation. 한마디로 길이에 따라서 QK로 매칭되는 attention score를 좀 깍아버리고 position embedding은 삭제시켜버리겠다는 것! ALiBi negatively biases attention scores with a linearly decreasing penalty proportional to the distance between the relevant key and query. Our simple approach eliminates position embeddings ALiBi can be imple- mented by changing only a few lines of existing transformer code. Background and Experimental Setup During both training and perplexity evaluation (i.e., scoring a fixed sequence), many predictions canbe calculated at once; this is done using a “causal mask” that ensures each position’s prediction isinfluenced only by tokens to its left Let L be the length of each input subsequence during training;it includes L predictions, which on average have access to (L+1)/2 tokens of (left) context sinusoidal transformer에서 사용됨 constant, non-learned vectors that are added to token embeddings on input to the first layer of the transformer. Rotary Roformer: Enhanced transformer with rotary position embedding, 2021.에서 첫 등장함 has recently been popularized by the open source GPT-3 (Brown et al., 2020) implementation GPT- J (Wang &amp; Komatsuzaki, 2021)에서 자주 사용됨 Instead of adding sinusoidal embeddings at the bottom of the transformer, they multiply the keys and queries of every attention layer by sinusoidal embeddings. every attn layer의 key랑 query쪽에 sin embedding을 다 곱한건가 Unlike the sinusoidal or learned positional embedding approach, the rotary method injects position information into the model at every layer, not just at the initial one In addition, it adds no position information to the values of the self-attention sublayer. The output of a self-attention sublayer is a linearly transformed, weighted sum of the input value vectors; therefore, by not inserting position information into the values, the outputs of each transformer-layer contain no explicit position information. We suspect that this segregation of position information may be beneficial for extrapolation, and we draw inspiration from it in the design of our method 포지션 임베딩을 직접 더해주지 않는 방식이 오히려 extrapolation에 더 좋을거라 판단했다는게 그 이유는 뭔지 안알려줌 T5 bias the T5 model of Raffel et al. (2020) uses a relative position method (Shaw et al., 2018; Huang et al., 2019) that adds no position information to word embeddings (as in the previous method) Instead, it modifies the way attention values are computed. We refer to this as the “T5 bias” method. In this method, we compute the attention values as before, but then we add a learned, shared bias to each query-key score that is dependent on just the distance between the query and key. QK score에 +learnable bias를 더해주되 그 bias는 query key distance에 의존성이있도록한다라.. ALiBi처럼 어떻게 보면 attention Score에 직접 개입하는것 같은데.. 그래서 성능이 Rotary보단 좋았나보다 그래서 추가적인 메모리가 필요하고 느리다고 했나보다 (모든 레이어에 대해서 bias가 필요해서) As in the rotary method, the T5 bias injects position information into the model at every layer and integrates no explicit position information into the self-attention value vectors. ATTENTION WITH LINEAR BIASES (ALIBI) 그림을 보면 learnable한게 없다 (m도 안배움..) 그래서 변수가 없으니 속도도 빠르고, 메모리도 절약했네 궁금한건 T5 relative position method는 learnable한걸 쓰는데 겨우 저걸로 어떻게 이긴거지 싶은거야 we do not add position embeddings at any point in the network. The only modification we apply is after the query-key dot product, where we add a static, non-learned bias: slope 개념이 잘 이해가 안가네, m에 대한 값을 head따라 다르게 줘보겠다가 핵심인거 같긴한데, 좋은 head에 나쁜 m값이 할당 될 수 있는 리스크를 안고 가는 느낌 (애초에 좋은 head라는걸 우린 알수도없지만) The ALiBi bias is not multiplied by the √dk scaling factor from Equation 1 of Vaswani et al. (2017). ALiBi has an inductive bias towards recency; it penalizes attention scores between distant query-key pairs, with the penalty increasing as the distance between a key and a query grows. The different heads increase their penalties at different rates, depending on the slope magnitude. We initially experimented with making the slopes trainable, but this did not yield strong extrapolation results 처음에 slope(m)을 trainable하게 해봤지만 좋은 결과를 얻진 못했다고함 왜일까..? trainable하게 하면 속도도 3%정도 느려짐 Our main insight from this exploration is that the slope sets that work best are those with slopes in the (0, 1) range, with the slopes’ density increasing as we get closer to 0 We also found our method to be robust to slope choice. Even randomly sampling from the exponential distribution worked well in some cases (although that method had high variance). 이렇게 지수적으로 하도록 (exp 분포에서 랜덤하게 뽑아봐도) m을 정하면 꽤 robust한 결과가 나온다고.. Since ALiBi is a relative position method, we add position information at every layer to the keys and queries but not to the values, as is done in the T5 bias and rotary methods. We hypothesize that these properties might be beneficial for extrapolation. Implementation implement it by modifying the mask matrix by adding the linear biases to it (in practice, when training a transformer LM, query qi attends only to keys 1 to i; this is implemented by adding a mask matrix to the query-key dot product before the softmax operation is applied ). This means that there is no runtime penalty when using our method since we add no operations to the network 왜 런타임에는 operation이 없을까.. 학습할때만쓴다? 뭐지..코드봐야되나 Results 이 그래프는 왜 놨는지 잘 모르겠음, ALiBi의 길이가 더 길때 ppl이 낮다를 보여줘야될것같은데 뭐지.. Conclusion showed that the sinusoidal position embedding approach does not enable transformers to extrapolate to inputs longer than the ones they were trained on established that extrapolation in transformers can be enabled by just changing the position method showed that our ALiBi method offers an extremely simple replacement for existing position approaches and allow models to extrapolate ALiBi is simple to implement and does not slow down runtime or require extra parameters sped up the training of a 1.3 billion parameter model evaluated on the same input sequence length as GPT-3 (2048)","link":"/(ALiBi)%20TRAIN%20SHORT,%20TEST%20LONG:%20ATTENTION%20WITH%20LINEAR%20BIASES%20ENABLES%20INPUT%20LENGTH%20EXTRAPOLATION/"},{"title":"A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More","text":"Author 저자: Iddo Drori1,a,b, Sunny Trana, Roman Wangb, Newman Chengb, Kevin Liua, Leonard Tangc, Elizabeth Kea, Nikhil Singha, Taylor L. Pattic, Jayson Lynchd, Avi Shporera, Nakul Vermab, Eugene Wub, and Gilbert Strang(아니 그 유명한 길버트 스트랭..)a aMIT; bColumbia University; cHarvard University; dUniversity of Waterloo 느낀점 large scale 모델이 생각보다 할줄아는게 많다는걸 알게됨.. 코드로 파인튜닝하면 수학문제 푸는 코드도 만드는구나 (그런 코드가 깃헙에 있었겠지만..!) Abstract program synthesis을 통해 PLM &amp; code에 finetune된 모델(Codex Transformer model)이 수학문제를 풀수있음을 논함 university-level Mathematics course questions을 생성하는 연구(?) Introduction PLM: text, Finetuning with code (from OpenAI) novel techniques to automatically rephrase problems so neural networks can synthesize correct executable programs Main Contribution: 특별한 파인튜닝없이도 6개의 MIT 수학코스와 1개의 컬럼비아 대학 코스를 푸는 뉴럴넷을 보임 (table 1) 아웃풋은 an executable program임 채점도 가능하고, 새로운 문제를 만들기도함 (깃헙의 날리지를 다 이런식으로 흡수하면 이런 놀라운 일도 하는 뉴럴넷이 되는건가) Adding Context Topic Library (like sympy, streamplot, ….) Definition Context Conclusion Codex가 대학수준의 문제를 program synthesis통해 풀고, 채점하고, 생성할 수 있음을 보임 단순 PLM은 안된다","link":"/A%20Neural%20Network%20Solves%20and%20Generates%20Mathematics%20Problems%20by%20Program%20Synthesis:%20Calculus,%20Differential%20Equations,%20Linear%20Algebra,%20and%20More/"},{"title":"Efficient Training of Language Models to Fill in the Middle (FIM)","text":"발표자료(발표)Efficient Training of Language Models to Fill in the Middle.pdf 느낀점 첫인상은 data augmentation 기법에 관련된 내용을 extensive하게 검증했다정도..? free-form generation을 하고 싶다에 초점을 두고 논문 전개 Note 50%란게 어떤걸까 데이터셋에서 FIM으로 transformation하는 비율 (FIM 자체는 랜덤하게 짜르니까) SPM에서 캐싱이 무슨 의미 일까 Author Mohammad Bavarian ∗ Heewoo Jun∗ Nikolas Tezak John Schulman Christine McLeavey Jerry Tworek Mark Chen OpenAI Github https://github.com/openai/human-eval-infilling Abstract autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default Introduction Finally, causal decoder-based language models, like the GPT model series [Radford et al., 2018, 2019, Brown et al., 2020], are trained using the left-to-right next token prediction objective. The largest and most capable generative language models today, such as GPT-3, Codex, LaMDA, GLaM, PaLM, Gopher, Jurassic-1, and Chinchilla due to their superiority in open-ended text generation, in-context learning (using few-shot priming), pretraining computational efficiency also architecturally simpler and generally more effective without task specific finetuning, making them more attractive for inference and deployment. 모델 구조에 따라서 infilling하는 능력이 제한되어있다 All model classes are limited when it comes to infilling, where the model is tasked with generating text at a specific location within a prompt, while conditioning on both a prefix and a suffix. Left-to-right models can only condition on the prefix. While encoder-only and encoder-decoder models are capable of conditioning on suffixes, the lengths of infill regions seen at training time are typically much shorter than what is useful in practice 여러 application 상황에선 이런점들이 unfortunate로 볼 수 있음 This is unfortunate because infilling naturally arises in applications where there is context both before and after the point of generation. For example, in creating a coding assistant, infilling can be used for docstring generation, import statement generation, or for completing a partially written function. Our goal in this work is to address this limitation by adding fill-in-the-middle (FIM) capability to causal decoder-based language models which are currently the most dominant paradigm for large scale language modelling 왜 limitation을 address하는거에 초점을 맞췄을까.. 해결하는게 보통의 접근방법인데 이것만으로도 의미가 있다는건가 a simple modification to training data and without changing the model architecture, causal decoder-based autoregressive (AR) language models can learn infilling without compromising their normal left-to-right generative capability. we split documents into three pieces at random and move the middle piece to the end: document → (prefix, middle, suffix) → (prefix, suffix, middle) concatenate the three pieces using sentinel tokens (&lt;PRE&gt; prefix &lt;SUF&gt; suffix &lt;MID&gt; middle) Compared to prior work, our work emphasizes the computational efficiency of training FIM models. This emphasis is important given the increased interest in training very large language models, which are very expensive to train and have a substantial energy footprint show that models trained jointly on a mixture of FIM transformed data and ordinary left-to-right data achieve the same left-to-right capability while learning how to fill-in-the-middle call this the FIM-for-free property use the term FIM model to refer to any model trained on a mixture of FIM transformed and normal left-to-right data. We refer to models trained without any FIM data (i.e. 0% FIM rate) as AR models. Our contributions cetral contributions FIM-for-free property (기존 loss에 영향안줌) extensive scaling study by training a suite of 8 models, with and without FIM, and show that FIM can be learned without compromising the left-to-right capability in pretraining. We examine this claim in both code and language, using both perplexity and sampling-based benchmarks Best practices for FIM in pretraining (FIM rate 찾아냄) clarify the effects of many hyperparameters related to training FIM models using comprehensive ablations. In particular, we study the FIM rate (the probability at which FIM transformation is applied to the data), different variants of FIM transformation, and the choice of middle span. Finetuning inefficiency (finetuning으로 하기엔 pretraining만큼 필요함, 흥미로운점임) An alternative to training FIM models from scratch is to learn this capability by finetuning existing language models. We show that finetuning with FIM is computationally inefficient. While FIM can be learned for free during pretraining, learning FIM during finetuning requires a significant amount of additional compute to reach similar levels of performance as pretraining. New infilling benchmarks need to evaluate the correctness of free-form generated samples. For this, we focus on code where we can use unit tests to evaluate the correctness of long FIM samples use the single-line and multi-line infilling benchmarks introduced by [Fried et al., 2022] by removing non-empty lines of canonical solutions of HumanEval create two new benchmarks called random span infilling and random span infilling light. Need for sampling evaluations find that changing various hyperparameters in FIM training often leads to negligible differences in FIM test losses but large differences in sampling based benchmarks. (어떤 벤치마크지..?, 생성쪽인듯) ablation study on both code and language across a range of scales FIM-for-free property를 확인하는 실험셋팅 train 8 models from 50M to 6.9B parameters, both with and without FIM, and compare the performance across a variety of autoregressive benchmarks. code와 LM에서 left-to-right LM test loss 비교하는 실험셋팅 train 16 models on code for 100B tokens and another 16 models on natural language for 100B tokens. The comparison of these models in terms of normal autoregressive left-to-right language modeling test loss is presented in Figure 1. In both domains, FIM models achieve similar AR test loss as the non-FIM models org 데이터 반정도만 봐도 left-to-right loss 차이가 거의 없게 나온다 it is also important to show that the models are in fact learning to infill from FIM training. Figure 2 provides evidence for this in the context of FIM test losses 그냥 left-to-right loss는 거의 같은데, FIM loss는 차이가 나니까 FIM이 AR모델보다 낫다고 얘기하는것 같은데 Evaluation AR loss refers to the cross entropy loss on normal left-to-right data and FIM loss as the loss on 100% FIM transformed data All test losses are in nats per token unit. In all sampling-based benchmarks, we use nucleus sampling [Holtzman et al., 2020] with a nucleus parameter of 0.95 Autoregressive evaluation benchmarks with the exception of DROP and QuAC are evaluated with few-shot prompting. For code, we measure the pass rates on HumanEval Infilling evaluation To create FIM tests, we apply the FIM transformation to the examples from the AR test sets with a FIM rate of 100%. Using the same underlying examples in FIM and AR test sets allows us to compare FIM and AR test losses create a masked version of these test sets where we only measure the loss on the middle span tokens. The latter test sets are used to measure P (middle∣prefix, suffix) for FIM models and P(middle∣prefix) for AR models allowing us to investigate the amount of information FIM models gain by being able to condition on the suffix. 이건 FIM이 정보 더 보는데 약간 애매하지않나.. For generative infilling capabilities, we focus on code since we are interested in free-form generation in contrast to single or few token generations common in cloze-style natural language benchmarks 코드는 open ended generation이여도 정답 체크가 가능하니까 장점이 있다 (The advantage of working with code is that we can use test suites to evaluate the correctness of samples in our tasks even when evaluating long samples from open-ended generations.) All the sampling based infilling benchmarks single-line, multi-line, and random span infilling partial function completions tasks created by removing middle spans from the canonical solutions of HumanEval use the single-line and multi-line infilling benchmarks proposed by [Fried et al., 2022] where different spans of non-empty lines in the canonical solutions of HumanEval are turned into a FIM task create a new benchmark called random span infilling2, where for each HumanEval problem, we create infilling tasks by selecting the middle span from the canonical solution uniformly at random. We show an example of such a task below where the model must predict the highlighted section (or an alternative completion accomplishing the same goal) also use random span infilling light, a smaller version of random span infilling, with only one random FIM task per HumanEval problem and just 164 tasks, to track the infilling capability trends during training. FIM can be prepared in two different ways denoted as PSM and SPM. We report just the SPM infilling results for brevity, except in cases when the use of PSM changes the conclusions. FIM training and inference implement FIM using a random transformation applied to our dataset experiment with two different implementations: document level and context level difference between the two is at which stage of the data loading pipeline the FIM transformation occurs. This choice naturally arises because a long document can be broken into many contexts, or a context can contain multiple documents when the documents are small. document-level case In document-level FIM, with a certain probability p called the FIM rate (we use p = 0.5 for our main suite of models), we cut each document into three parts: prefix, middle, and suffix. We perform this split prior to tokenization, when the document is still a sequence of characters. (토큰화하기 전에 3등분한다) split uniformly at random, which means the lengths of prefix, middle, and suffix are each 1/3 of the full document in expectation. (랜덤하게 3등분씩 잘라~) encode each of the three sections separately and prepend sentinel tokens to the beginning of each section. We denote these sentinel tokens by &lt;PRE&gt;, &lt;MID&gt;, and &lt;SUF&gt; PSM, SPM 등등으로 구성가능, 문서사이는 &lt;EOT&gt;로 분리 keep the loss on all three sections prefix, middle, and suffix, so FIM training does not cause a decrease in the autoregressive learning signal. section에 대한 loss를 backward할때 고려안한다는건가..? 아니면 해서 도움이 된다는건가 -&gt; 다 계산한다는 의미일듯 그래야 AR효과가 나서..?! this choice is crucial for the FIM-for-free property to hold. This property does not change whether the sentinels are masked or not; however, it is important to always train on the &lt;EOT&gt; tokens as it signals a successful join to the suffix. 무슨 의미일까 SPM mode a variant of the above procedure where we swap the order of prefix and suffix, called SPM, to emphasize the changing of the order to suffix, prefix, and middle Our main motivation for introducing SPM is improved key-value caching during inference. The reason for this advantage is that with SPM, appending tokens to the prefix no longer invalidates the keys and values computed in the suffix section 무슨의미일까 Note that superiority of SPM caching is not universal and may depend on the applications. In particular, in the SPM mode, minor changes to the suffix will invalidate the cache for prefix, but we expect changes to the suffix to be rarer than changes in prefix in real workloads. Interestingly, we find in Section 4.3 beside the caching advantages, SPM in fact has also a slight edge over PSM in the infilling benchmarks. apply the FIM transformation with 50% probability in PSM mode and with 50% probability in SPM mode, so the model is able to handle both types of formatting in inference. In other words, each mode inherits half of the total FIM rate p 여기서는 joint training의 효과를 극대화하기 위해 SPM의 variant를 활용함 (괴랄하다 괴랄해..) Context-level FIM In language model training, documents are often joined with a boundary token, referred to as &lt;EOT&gt;, and are then chunked to the model context length. When applying FIM to long documents, this operation can result in fragmented FIM data where the entire prefix or suffix could get cut out of the context during chunking we can apply FIM after the chunking step. A context slice may have multiple documents in them joined with the &lt;EOT&gt; boundary token we split based on &lt;EOT&gt;, turn some of the documents into FIM examples with probability given by the FIM rate, and join the examples again with &lt;EOT&gt; 문서마다 &lt;EOT&gt;로 짤라서 FIM하고 다시 &lt;EOT&gt;로 붙였다는 말123456789101112131415def token_level_psm_fim(document: str, vocab: Vocab) -&gt; List[int]: tokens = vocab.encode(document) prefix, middle, suffix = randomly_split(tokens) return [ vocab.sentinel(&quot;prefix&quot;), *prefix, vocab.sentinel(&quot;suffix&quot;), *suffix, vocab.sentinel(&quot;middle&quot;), *middle,]def character_level_psm_fim(document: str, vocab: Vocab) -&gt; List[int]: prefix, middle, suffix = randomly_split(document) return [ vocab.sentinel(&quot;prefix&quot;), *vocab.encode(prefix), vocab.sentinel(&quot;suffix&quot;), *vocab.encode(suffix), vocab.sentinel(&quot;middle&quot;), *vocab.encode(middle),] show this technique can boost performance relative to document-level FIM, and adopt context-level FIM in all our main FIM runs in this work Pretraining resultsEvaluation of left-to-right capabilities in downstream benchmarks train a series of models from 50M to 6.9B parameters from scratch with and without 50% FIM augmentation on natural language and code domains evaluate our models on a suite of standard downstream benchmarks, the result of which is presented in Figure 3. We again find that joint FIM pretraining does not result in any degradation in standard AR benchmarks as the performance matches within error for both natural language and code. 성능 깍아먹지 않고 비슷하게 잘 나온다가 포인트 (10^9 == B 단위에서도 다 비슷비슷) FIM rate Questions Does FIM-for-free still hold even at higher FIM rates? If so, how high can we increase the FIM rate without degrading the left-to-right capabilities? Does using a higher FIM rate lead to stronger FIM capabilities? Or does the benefit saturate after a threshold? Settings 6 large models with FIM rates (0, 0.25, 0.5, 0.75, 0.9, 1.0) for 50B tokens results FIM rate even up to 90% does not cause any degradation in left-to-right capabilities. However, there is a clear sign of degradation in ordinary AR test loss with 100% FIM rate On the other hand, we find that the FIM rate does significantly affect infilling capabilities. Even though the gain in FIM perplexity in Figure 4 due to a higher FIM rate is negligible, increasing this rate yields a consistent improvement in the infilling pass rate as shown in the right plot in Figure 5. Appendix B Scaling trends for FIM rate ablations SPM vs PSM vs joint SPM+PSM training The main finding is that SPM is slightly stronger than PSM in our benchmarks in general as evidenced by Figure 6. This is likely due to the fact that in SPM, there is no distinction between the prefix and the middle sections as they are one contiguous sequence of text. This makes it more natural for the model to continue from the prefix in contrast to PSM where attention has to first identify where the span token is. However, this does not imply that we should train solely on SPM. In Table 1, we train large models on pure PSM, pure SPM, and our default 50-50 SPM+PSM mix, and evaluate them in all modes. Not only is joint pretraining the most efficient, but it also yields the most flexible model with two inference modes. Context-level vs document-level FIM context-level and document-level FIM, where augmentation is applied either before or after packing and chunking perplexity evaluation does not always capture the gains in the sampling performance. (4.2 참고) document-level FIM can result in fragmented FIM data with a missing prefix and/or suffix from the chunking step of data loading pipeline. Figure 8 (left) shows that training on these invalid examples in document-level FIM does not affect the left-to-right evaluation. Hence, practitioners might still sometimes prefer document-level FIM due to its simpler implementation. Middle span selection An important consideration in FIM training is the choice of middle span. select spans in three different ways, splitting randomly by lines, tokens, and characters. The section boundaries are selected uniformly at random from the allowed splitting positions based on the span type. Here, a token refers to a word in the byte-pair encoding (BPE) vocabulary. In practice, this is implemented by applying the FIM augmentation after the documents are encoded with BPE (see Appendix C). For simplicity, we run all our experiments in PSM mode in this ablation. line-based middle spans gives the models a slight advantage in the single-line and multi-line infilling benchmarks On the other hand, the line based training fails almost completely in the random span infilling benchmark token-level random spans does slightly better on random span infilling, but is still not competitive compared to character-level runs on this benchmark character level subtokens are introduced naturally at the beginning and the end boundaries of the middle section. There is no train-test mismatch and the model is able to understand and solve more random span infilling tasks while still performing well in single-line and multi-line infilling. Finetuning results 결론은..finetuning보단 pretraining으로 하는게 나을 수 있다. finetuning도 50B tokens&amp; 90% FIM로 해야 그나마 비슷해진다 investigate whether we can finetune existing AR models to learn the FIM capability. 16 models with that of the XL model trained for 100B tokens with a FIM rate of 50% without any finetuning. It is evident from this figure that even with significant additional finetuning compute, AR models finetuned with FIM do not reach the same performance as the models pretrained with FIM (and without any FIM finetuning) More generally, we find that higher learning rate, FIM rate, and longer finetuning all seem helpful for improving FIM performance in finetuning. Discussion Pretraining vs finetuning The main intuition for why FIM can be learned for free in pretraining is that breaking a document into three pieces and shifting the middle one to the end effectively creates three smaller documents. In particular, each piece still requires predicting next tokens from left to right, keeping the total number of tokens processed autoregressively the same On the other hand, even though FIM data is locally identical to autoregressive data, FIM does impose a different global attention pattern over the whole document. show the causal attention mask of a FIM document in Figure 10. These new attention pattern could be the reason why it takes a relatively long token horizon and a high learning rate to learn FIM in finetuning FIM loss, AR loss, and the difficulty of FIM task There is substantial evidence that FIM can often be much harder than normal left-to-right generation. Intuitively, it is often easier to continue a text in a plausible manner than to continue the text conditioned on ending in a specific suffix. The latter requires planning a plausible narrative connecting the two pieces, starting the generation in a way that matches the prefix, and stopping the generation at the right time so it connects to the suffix in FIM the model is trained to generate when the middle ends and connects to the suffix. On the other hand, when the model fails to produce in the allotted budget, we often end up with truncated samples which do not connect well to the suffix. For example, consider the following: prefix뿐만 아니라 suffix하고도 문맥상 잘 맞게 생성해야되서 더 어렵다.. post processing으로 커버도 어렵고 this type of failure is more troublesome in FIM since a failure to connect to the suffix cannot easily be fixed by post-processing. Both completions above connect well to the prefix, but only the first manages to connect well to the suffix. The second completion in contrast fails to produce in the allotted budget resulting ina bad sample.5 This turns out to be a common failure in FIM sampling Appendix H 성공케이스 실패케이스 완충케이스 (numbered items로 힌트주기) PPL도 FIM이 AR보다 더 높은걸 보면 FIM이 좀 더 어려운 task로 볼 수 있음 아래 그래프는 모두 FIM 모델이고 test loss의 type만 바꾼거임 Context-level vs document-level FIM and FIM rate The basic observation is that document-level FIM effectively leads to a lower FIM rate compared to context-level FIM, even with the same nominal value of FIM rate. 이하 생략 Related work similar to this work, (utilize left-to-right autoregressive modeling by moving the infill regions to the end of context, with regions separated by sentinels) GLM: General Language Model Pretraining with Autoregressive Blank Infilling CM3: A CAUSAL MASKED MULTIMODAL MODEL OF THE INTERNET (Facebook AI Research) InCoder: A Generative Model for Code Infilling and Synthesis Conclusion show that causal decoder-based language models can learn to fill in the middle of a document after being jointly trained on a mixture of traditional left-to-right and FIM transformed data A single FIM model can import modules, write docstrings, and complete functions, subsuming specialized models finetuned for individual tasks [Chen et al., 2021], providing substantial extra capability over traditional left-to-right language models. One important finding here is the FIM-for-free property. FIM models achieve the same test loss as AR models on left-to-right test loss while achieving lower FIM loss investigate FIM finetuning since a lot of the existing language models do not have FIM capabilities. Our results demonstrate that a canonically pretrained left-to-right model does not acquire the new skill to the fullest extent of the given model size even with careful hyperparameter tuning and a significant amount of finetuning compute relative to pretraining. This suggests that for the best FIM performance, pretraining jointly from scratch with our recommended hyperparameters is more effective than finetuning. use the infilling code benchmarks from InCoder [Fried et al., 2022] and introduce the new random span infilling benchmarks based on HumanEval [Chen et al., 2021]. From these, we learn a few important lessons First, perplexity does not reflect the true infilling performance, and one should design the infilling benchmarks carefully to measure progress Second, FIM capabilities depend considerably on the FIM rate and implementation like context-level FIM but left-to-right capabilities are unaffected by these choices as long as the FIM rate is kept below 100% Third, applying FIM at the character level imbues the model with natural robustness to subtokens and makes it possible to deploy the model in the wild, for example, as a coding assistant. Recommended FIM hyperparameters FIM transformation at the character level and always including some character- level random spans as it allows the model to generate sensible completion even when the prefix and suffix end in the middle of a token pretraining with joint PSM and SPM yields the best performance due to a positive transfer between the two formats context-level FIM is superior but document-level FIM is also an option if a simpler implementation is desired Finally, we observe improved performance even up to a FIM rate of 90% without any cost in AR capabilities In practice, any value in the range between 50% and 90% is a reasonable choice. Note that this is in contrast with some related prior work such as InCoder [Fried et al., 2022] which typically uses lower values of FIM rate such as 15%, which our results indicate to be suboptimal. Future directions Smarter span selection Steerable generation Further examination of the FIM-for-free property Multiple infilling slots Improving natural language FIM performance Role of bidirectionality and attention Finally, our experience with the FIM-for-free property brings up the intriguing question of what other useful skills can be learned jointly with no or little cost to the original capabilities of language models.We propose the following methodology to help advance research toward answering this question: Establish a budget in the amount of original capabilities that one is willing to sacrifice to learn a new capability. Maximize the new capability within this budget.","link":"/Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle%20(FIM)/"},{"title":"GPT Understands, Too","text":"Author 저자: Xiao Liu* 1 2 Yanan Zheng* 1 2 Zhengxiao Du1 2 느낀점 neural model은.. 작은 변화에 너무 민감하다?! Abstract GPTs 계열에서 기존의 fine-tuning 방법이 NLU task에서 좋은 결과를 내기 어려웠음 새로운 방법인 p-tuning이라는 방법을 제안해서 BERT와 비슷한 크기의 모델에서는 좋은 결과를 내게함 knowledge probing (LAMA) benchmark에서 64%(P@1)를 기록했음, SuperGlue에선는 BERT의 지도학습보다 좋은 결과를 냄 p-tuning이 BERT 성능도 좋게함을 발견함(few-sho &amp; 지도학습 셋팅에서) p-tuning은 few-shot SuperGlue에서 SOTA임 Introduction ptrLM은 3가지로 나뉨, unidirectional LM, bidirectional LM, hybrid LM GPT-style LM은 파인튜닝할때 NLU에서 좋은 결과를 못냈었음 GPT-3가 나오면서 hand-crafted prompts기반의 few-shot, zero-shot learning이 부각됨 prompt에 따라 NLU 좋아질수있음을 보임 하지만 handcraft로 best prompt 찾기는 건초더미에서 바늘찾기임 neuralnet은 태생적으로 continuous한 성격을 갖기에, discrete prompts는 sub-optimal 정도를 갖게된다는 한계가 있음 본 연구에서는 P-tuning이라는 새로운 방법론을 제안해서 GPTs와 NLU의 갭을 메울것임 p-tuning은 continuous free parameters를 prompts로 ptrLM에 입력하는 방식임 continuous prompts를 gradient descent 방식으로 업데이트 해서 최적화함 본 연구에서는 다음과 같은 점을 밝혀냄 (contribution) P-tuning사용시 GPTs 계열이 BERTs 계열에 대비 NLU 태스크에서 경쟁력이 있음 P-tuning은 GPTs, BERTs 계열 모두 few-shot, fully supervised setting에서 사용 가능함 Motivation GPT-3, DALLE는 giant model이 machine intelligence를 촉진시킬 것을 보여줬지만 단점이 있었음 transfer ability가 떨어짐, trillion-scale model을 파인튜닝하는 건 어려움 handcrafted prompts 찾는 일도 버거운 방법이고, 이를 위해 validation set에 의존하는것도 비현실적임 뿐만아니라, prompt에 따라 성능도 너무 확확바뀜 Method: P-tuning input 수정하거나 자체를 바꾸진 않음 대신에 PLM의 input embedding의 일부를 differential output embedding으로 교체함 Architecture pseudo token을 template에 맵핑해서 넣음 Optimization prompt encoder를 통과시켜서 적용했고, LSTM &amp; two-layer MLP(ReLU) 사용함 ExperimentsKnowlege Probing transform the triple (Dante, born-in, Florence) into a cloze sentence with the handcraft prompt “Dante was born in [MASK].”, and then we ask language models to inference the target SuperGLUE 8개의 NLU task Conclusion best prompt를 continuous space에서 자동으로 찾아주는 P-tuning 방법을 제안함 기존대비 large validdation set에 의존하지 않음 (덜 의존함), adversial prompts에 덜 취약함 knowledge probing (LAMA) benchmark에서 64%(P@1)를 기록 GPT-style 모델이 BERTs 대비 NLU 잘할 수 있게 함 P-tuning은 bidirectional model에도 도움을 줌 few-shot SuperGlue에서 SOTA 뛰어넘음","link":"/GPT%20Understands,%20Too/"},{"title":"Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training","text":"Author 저자: Oshin Agarwal∗1 Heming Ge2 Siamak Shakeri2 Rami Al-Rfou2 (1 University of Pennsylvania 2Google Research), 2021 느낀점 기존에 KG triples을 자연어 문장으로 바꾸는 연구가 생각보다 적었었나? 싶었음 (혹은 잘 안되었던것 같음. explicit하게 표현이 안된다던지) Abstract KG triples를 자연어로 바꾸는 연구들(Data-To-Text Generation)은 주로 도메인특화된 벤치마크셋 중심으로 연구되었음 wikidata와 같은 데이터셋도 structetured KGs와 natural language corpora를 결합하는데 쓸수있음을 본 연구에서 보였음 (existing LM과 결합가능) Introduction (subject, relation, object)를 자연어 문장으로 바꾸는 연구 == Data-To-Text Generation WebNLG와 같은 standard dataset이 존재했음 KG 전체를 verbalize하는 연구는 기존에 없었음 (KG를 verbalize한다는게 뭘까) full KG에 대해 verbalizing하는건 entitiy, relation 커버리지, triples 부족등의 단점이 있음 본 논문에서는 Wikidata KG 를 corpus로 바꿨고 KeLM Corpus로 칭하기로함 코퍼스 스펙: ∼18M sentences spanning ∼45M triples with ∼1500 distinct relations 기존 연구에서는 facts가 문장에서 제대로 안드러나게 변환되던지하는 문제가 있었음 contribution data-to-text seq2seq 모델 개발 Text-KG aligned corpora 생성 KELM: 라지스케일의 Wikidata KG 를 자연어문장으로 만든 합성 코퍼스 (위에꺼를 기반으로 만들었다고 생각하면 될듯) Data-totext generation이 open domain QA와 LAMA probe를 개선한다는걸 보여줌 TEKGEN TEKGEN (Text from KG Generator) 은 결국 wikipedia(text)와 wikidata(KG)를 align해서 input, target 데이터셋을 만들고, T5로 1차 파인튜닝 그리고 WebNLG로 2차 파인튜닝해서 BERT score가 높은 문장들을 모아서 Corpus를 만들어준다고 생각하면 될듯! (Wikidata에서 distant supervision 차이로 wikipedia랑 맵핑이 안된 triples는 나중에 다시 컨버팅해서 KELM Corpus에 넣는듯) Wikidata 스펙: 6M entities 1500 relations WebNLG 스펙: 600 entities 20 relation 모델링 철학 약간 노이지한 큰 데이터셋을 distant supervision으로 만들자 (약간 수도레이블링 같은거죠..) 순차적으로 T5 파인튜닝하자, 처음엔 노이지한 큰 데이터로하고 나중엔 작지만 클린한 데이터(WebNLG로 하자) BERT를 시멘틱 필터로 써서 KG triples와의 의미적 품질을 확인해서 필터링하자 Alignment 관련 알고리즘 Align되었을때 결과 통계표 Types of Triples 위키피디아 페이지가 있는 오브젝트 위키피디아 페이지가 없는 오브젝트 quantity date subproperty Model two-step 순차적 파인튜닝을 T5-large로 함 triples는 다 concat함 (subject relation_1 object_1, ....relation_n object_n) 5000 step까지는 엔티티커버리지 때문에 학습했지만, 예상했던 input triple이 없는 경우엔 랜덤한 값을 생성(hallucination 현상)해내기도 하기 때문에 WebNLG 2017 데이터로 500 step 파인튜닝함 Quality Filtering BERT uncased model 사용 [CLS] concatenated-triples [SEP] reference-or-generated-sentence 에 포멧으로 평가하고, WebNLG 2017 에 대해 1000 step 파인튜닝했음 시멘틱스코어는 0~1로 스케일링했고 gold reference는 스코어를 1로줌 2706개의 예제에 대해서 90%는 학습, 10%는 평가를 진행했고 점수와 human 평가가 높은 correlation이 나옴 Knowledge Enhanced LMs Conclusion 본 논문에서는 다양한 시도를 통해 KG -&gt; natural text 로 바꾸는 연구를함 retrieval-based langauge model을 합성코퍼스인 KELM 코퍼스를 retrieval corpus로 써서 확장시킴 이러한 확장모델을 open domain QA와 knowledge prob에 적용했더니 둘다 개선효과가 있었음 데이터는 이곳에서 받아볼 수 있음: https://github.com/google-research-datasets/KELM-corpus","link":"/Knowledge%20Graph%20Based%20Synthetic%20Corpus%20Generation%20for%20Knowledge-Enhanced%20Language%20Model%20Pre-training/"},{"title":"LLM(Large-Scale Language Model)을 위한 넓고 얕은 지식들","text":"최근 LLM 관련 일을 하면서 익혀야할게 너무나 많다는 사실을 새삼스럽게 알게 되었다. 예전엔 아 그냥 하면 되지~ 정도로 생각했는데.. 디테일한게 생각보다 많구나 싶어서 이것 저것 많이 보기야 봤는데 머리에 남는게 없는 것 같아서 글로 간단하게 그리고 약간 어쩔수 없이 파편화된 상태로 정리해놓으려한다. 나 포함 누군가에게 도움이 되기를 바라며 PyTorch로 분산 어플리케이션 개발하기 제일 먼저 볼 것! 참고 Pytorch Multi-GPU 정리 중 분산학습을 할때 로그를 찍으면 프로세스 개수만큼 찍힌다 -&gt; 따로 처리가 필요해짐! if rank==0일때만 찍게 한다던지 code snippetall-reduce EleutherAI gpt-neox123456def reduce_losses(losses): &quot;&quot;&quot;Reduce a tensor of losses across all GPUs.&quot;&quot;&quot; reduced_losses = torch.cat([loss.clone().detach().view(1) for loss in losses]) torch.distributed.all_reduce(reduced_losses) reduced_losses = reduced_losses / torch.distributed.get_world_size() return reduced_losses Multi GPU &amp; Node 모델을 학습하다보면 결국 multi-gpu, multi-node등으로 스케일을 키울 수 밖에 없다 multi-gpu는 주로 Data Parallel(DP)할때 많이 쓴다 (스레드 기반이라고) multi-node는 여러 노드에서 multi-gpu를 쓰며 DP를 하는데 Distributed Data Parallel(DDP)라 부른다 (프로세스 기반이라고) 최근엔 torch.distributed.run(launch)을 torchrun이 대체했다 torchrun은 pytorch 1.1x이상부터 쓸 수 있다 torchrun을 사용해도 init process 과정을 아래처럼 거쳐야하는데 huggingface에서는 TrainingArguments쪽에 해당 부분이 내장되어있다. huggingface를 쓸땐 중복 선언하지 않게 조심! torchrun 사용시 주어지는 Environment Variables도 잘 보자!, global rank의 경우 Env Var는 RANK로 표기된다 1234567import torch.distributed as distdist.init_process_group(backend=&quot;gloo|nccl&quot;)local_rank = int(os.environ[&quot;LOCAL_RANK&quot;])model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank) torch.distributed.run/torchrun multi-node를 쓸 경우 네트워크등 모종의 이유로 iteration당 시간은 조금 더 늘어날 수 있다 (하지만 더 많은 데이터를 본다!) HPC reference: https://docs.likejazz.com/wiki/HPC/ DeepOps DevOps, MLOps와 함께 딥러닝을 중심으로 DeepOps라고 부르며, NVIDIA가 공개한 자사 DGX 서버에 Slurm clusters를 구성하는 툴의 이름 slurm: 각 노드에 배치작업을 실행하는 스케줄러, 노드에는 작업을 관리하는 데몬이 떠 있다. enroot: 도커 이미지를 sqsh(squashfs) 파일로 관리 NVIDIA에서 개발 pyxis: slurm의 컨테이너 플러그인으로, srun에서 sqsh 컨테이너 파일을 주고 받을 수 있다. NVIDIA에서 개발 SLURMslurm에는 많은 옵션들이 존재하는데, 이 옵션에 대해서 몇가지만 정리해보고자한다. SLURM에 대한 전반적인 가이드 자료ex) sbatch script 예시123456789101112#!/bin/bash#SBATCH --nodes=1#SBATCH --gpus-per-node=8#SBATCH --cpus-per-task=16#SBATCH --ntasks-per-node=1#SBATCH --partition=batch#SBATCH --output=logs/%j.%x.info.log#SBATCH --error=logs/%j.%x.error.log#SBATCH --exclude=제외할노드주소export MASTER_ADDR=$(hostname -i | cut -d' ' -f1)export WORLD_SIZE=$((SLURM_JOB_NUM_NODES * SLURM_NTASKS_PER_NODE)) ntasks-per-node 각 노드에서 살행되는 프로세스 수인듯 #SBATCH --gres=gpu:1와 #SBATCH --gpus-per-task=1의 차이 https://stackoverflow.com/questions/67091056/gpu-allocation-in-slurm-gres-vs-gpus-per-task-and-mpirun-vs-srun pyxis로 enroort로 만든 sqsh 이미지 실행방법 123456srun -l \\ --container-workdir=&quot;$HOME/공유할디렉토리&quot; \\ --container-image=&quot;$ENROOT_IMAGE_PATH/node에서사용할enroot이미지.sqsh&quot; \\ --container-mounts=&quot;$HOME:$HOME&quot; \\ --output=$LOG_PATH/%j.%x.log \\ sh -c &quot;$run_cmd&quot; drain 노드 확인 12345678910111213# 상태조회$ sinfo -lFri Sep 02 17:29:07 2022PARTITION AVAIL TIMELIMIT JOB_SIZE ROOT OVERSUBS GROUPS NODES STATE NODELISTbatch* up infinite 1-infinite no NO all 4 drained 노드1,노드2,...,노드4batch* up infinite 1-infinite no NO all 16 mixed 노드n,...,노드n+15batch* up infinite 1-infinite no NO all 14 allocated 노드k,...,노드k+13# 전체 노드 상태조회$ sinfo -N -l# queue 상태조회$ squeue 전체 노드 스펙 확인 cpu, gpu, socket 개수, 메모리등 확인가능 cat /etc/slurm/slurm.conf 각 노드 스펙 확인 cat /etc/nhc/nhc.conf job 히스토리 확인 sacct 사용자 조회, 그룹 조회 sacctmgr list user, sacctmgr show account drain 이벤트 조회 sacctmgr list event Pyxis Slurm Workload Manager를 위한 SPANK(Slurm Plug-in Architecture for Node and job (K)control) 플러그인임 srun 명령어로 containerized task를 클러스터에서 돌릴 수 있게 해줌! 이미지 배포에 오랜 시간이 걸리는데, 각 노드에 $ enroot list로 이미지가 존재하면 container-name 지정으로 호출할 수 있다. 이 경우 이미지 전송을 하지 않으므로 매우 빠르게 실행 가능함. 예시 $ srun --gres=gpu:1 -N2 -l --container-name=pytorch-slurm nvidia-smi Enroot PyTorch hook srun은 SLURM_으로 시작하는 다양한 환경변수를 셋팅해준다. 하지만 RANK, WORLD_SIZE는 torchrun이 셋팅함 enroot의 hooks/extra에 50-slurm-pytorch라는 hook2이 있어서, 만약 PyTorch 이미지인 경우 PYTORCH_VERSION 환경변수가 있는지 확인하고, SLURM_*을 이용해 torchrun이 해주는 RANK, WORLD_SIZE, MASTER_ADDR등을 대신 설정해준다. 따라서 pyxis로 enroot 이미지를 srun 할 경우 따로 torchrun 할 필요 없으며, rank/size를 얻기 위한 MPI 통신도 필요없다. 이 부분은 잘 이해가 안간다..! 분산 GPU 학습가이드 Azure 가이드를 참고한다 MPI(메시지 전달 인터페이스) Horovod DeepSpeed Open MPI의 환경 변수 PyTorch InfiniBand를 통해 GPU 학습 가속화 MPI 각 노드에 지정된 수의 프로세스를 시작하는 MPI 작업을 지정해야함 노드별로 1개씩 프로세스를 실행할 수도 있고 디바이스/GPU 수만큼 프로세스를 시작할 수도 있음 참고 OMPI_COMM_WORLD_RANK - 프로세스의 순위 OMPI_COMM_WORLD_SIZE - 세계 크기 (전체 프로세스 개수) AZ_BATCH_MASTER_NODE - MASTER_ADDR:MASTER_PORT 포트가 있는 기본 주소 OMPI_COMM_WORLD_LOCAL_RANK - 노드에 있는 프로세스의 로컬 순위 OMPI_COMM_WORLD_LOCAL_SIZE - 노드의 프로세스 수 PyTorch PyTorch의 네이티브 분산 학습 기능(torch.distributed -&gt; torchrun) 사용하기 프로세스 그룹 초기화 torch.distributed.init_process_group(backend=’nccl’, init_method=’env://‘, …) 프로세스를 그룹핑해야함 프로세스 그룹이 모든 분산 프로세스에서 torch.distributed.init_process_group를 호출해서 만들어짐 init_method는 각 프로세스가 서로를 검색하는 방법, 통신 백 엔드를 사용하여 프로세스 그룹을 초기화하고 확인하는 방법을 알려줌. 기본적으로 init_method가 지정되어 있지 않으면 PyTorch에서는 환경 변수 초기화 메서드(env://)를 사용 이를 위해 다음의 환경변수를 셋팅해놔야함 MASTER_ADDR - 0 순위의 프로세스를 호스팅할 컴퓨터의 IP 주소입니다. MASTER_PORT - 0 순위의 프로세스를 호스팅할 컴퓨터의 사용 가능한 포트입니다. WORLD_SIZE - 총 프로세스 수입니다. 분산 학습에 사용되는 디바이스의 총수(GPU)와 같아야 합니다. RANK - 현재 프로세스의 (전체) 순위입니다. 가능한 값은 0~(세계 크기 - 1)입니다. LOCAL_RANK - 노드 내 프로세스의 로컬(상대) 순위입니다. 가능한 값은 0~(노드의 프로세스 수 - 1)입니다. 이 정보는 데이터 준비와 같은 많은 작업이 노드별로 한 번만 수행되어야 하기 때문에 유용합니다(일반적으로 local_rank = 0 -&gt;&gt;&gt;&gt;&gt; 노드별로 한개의 프로세스만 띄운다는 뜻일까?) NODE_RANK - 다중 노드 학습을 위한 노드의 순위입니다. 가능한 값은 0~(총 노드 수 - 1)입니다. 시작 옵션 프로세스별 시작 관리자: 시스템은 프로세스 그룹을 설정하기 위한 관련 정보(예: 환경 변수)를 모두 사용하여 모든 분산 프로세스를 시작합니다. 노드별 시작 관리자: 각 노드에서 실행될 유틸리티 시작 관리자를 클러스터에 제공합니다. 유틸리티 시작 관리자가 지정된 노드에서 각 프로세스의 시작을 처리합니다. 각 노드 내에서 로컬로 RANK 및 LOCAL_RANK가 관리자에 의해 설정됩니다. torch.distributed.launch 유틸리티(아마 torchrun도!)와 PyTorch Lightning은 모두 이 범주에 속합니다. 예시1234 python -m torch.distributed.launch --nproc_per_node &lt;num processes per node&gt; \\--nnodes &lt;num nodes&gt; --node_rank $NODE_RANK --master_addr $MASTER_ADDR \\--master_port $MASTER_PORT --use_env \\&lt;your training script&gt; &lt;your script arguments&gt; HuggingFace를 사용할 경우 Hugging Face는 torch.distributed.launch와 함께 변환기 라이브러리를 사용하여 분산 학습을 실행하기 위한 예제를 다수 제공 Trainer API를 사용하여 이러한 예제와 사용자 지정 학습 스크립트를 실행하려면 torch.distributed.launch 사용 섹션을 따름 프로세스별 시작 옵션을 이용해 torch.distributed.launch를 사용하지 않고도 분산 학습을 실행할 수 있습니다. 이 방법을 사용하는 경우 유의해야 할 한 가지 사항은 변환기 TrainingArguments에서 로컬 순위가 인수(--local_rank)로 전달될 것으로 예상한다는 점입니다. torch.distributed.launch에서는 –use_env=False일 때 이 작업을 처리하지만, 프로세스별 시작을 사용하는 경우 Azure ML에서 LOCAL_RANK 환경 변수만 설정하므로 –local_rank=$LOCAL_RANK 학습 스크립트에 대한 인수로 로컬 순위를 명시적으로 전달해야함 local_rank (int, optional, defaults to -1) — Rank of the process during distributed training. torchrun등을 안쓰고도 trainer API의 인자로 들어가는 TrainingArguments에 local_rank를 넣어줘서 돌릴 수 있다는거 같은데 확인을 해봐야할듯 InfiniBand를 통해 분산된 GPU 학습 가속화 모델을 학습하는 VM 수가 늘어남에 따라 모델 학습에 필요한 시간이 줄어듬. 시간 감소는 학습 VM 수에 따라 선형적으로 비례함. 예를 들어 1개의 VM에서 모델을 학습하는 데 100초가 걸릴 경우 2개의 VM으로 동일한 모델을 학습하면 이상적으로 50초가 걸림. 4개의 VM에서 모델을 학습하려면 25초가 걸림. InfiniBand는 이러한 선형적 확장을 유지하는 데 있어서 중요한 요소 InfiniBand는 클러스터에서 노드 간에 지연 시간이 낮은 GPU 간 통신을 가능하게 해줌. InfiniBand를 사용하려면 특별한 하드웨어가 필요함 이러한 VM은 대기 시간이 짧은 고대역폭 InfiniBand 네트워크를 통해 통신하는데, 이는 이더넷 기반 연결보다 훨씬 더 성능이 뛰어남!! LMwhole word masking huggingface에서는 PLM을 위한 많은 함수들을 제공하고 있다 wordpiece, bbpe, cbpe를 테스트한 이후에는 whole word masking등에 관심을 기울이게 되는데 이미 구현해놨으니 참고해보면 좋을 듯하다 DataCollatorForWholeWordMask 사용법은 해당 모듈을 github에 검색해보면 대략 나온다 https://github.com/search?q=from+transformers+import+DataCollatorForWholeWordMask&amp;type=code https://github.com/sophia-jihye/ComBERT/blob/f214bae0a4be57fd408f142898c6886d540e02e7/post_training_mlm.py","link":"/LLM%EC%9D%84%20%EC%9C%84%ED%95%9C%20%EB%84%93%EA%B3%A0%20%EC%96%95%EC%9D%80%20%EC%A7%80%EC%8B%9D%EB%93%A4%20%EC%A7%80%EC%8B%9D%EB%93%A4/"},{"title":"CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding","text":"Author 저자: Dong Wang1,2∗ , Ning Ding1,2∗, Piji Li3† , Hai-Tao Zheng1,2† 1Department of Computer Science and Technology, Tsinghua University 2Tsinghua ShenZhen International Graduate School, Tsinghua University 3Tencent AI Lab google scholar에서 찾긴 어려웠음 느낀점 이 논문에서는 adversarial을 같은말이라고 쓰는거 같고, constrastive를 반대말이라고 쓰는듯.. PLM을 학습할때 두번째 pair에 아무 문장이나 넣는게 아니라 의미적으로 다른 문장을 넣겠다가 핵심임 https://github.com/kandorm/CLINE Abstract PLM이 양질의 semantic representation을 만들어주지만 simple perturbations에 취약함 PLM을 강건하게 하기위해 adversarial training에 포커스를 맞추고 있음 이미지 프로세싱과는 다르게 텍스트는 discrete하기 때문에 몇개의 단어 교체는 매우 큰 차이를 만들어내기도함 이러한 결과를 연구하기 위해 perturbation 관련 여러 파일럿 실험을 진행했음 adversarial training이 useless하거나 오히려 모델에 안좋다는 사실을 발견함 이러한 문제를 해결하기 위해 Contrastive Learning withg semantIc Negative Examples (CLINE)을 제안함 unsupervised 방식의 의미적으로 네거티브한 샘플들을 구축했고, 이를 통해 semantically adversarial attacking에 robust하도록 개선하려함 실험적 결과로는 sentiment analysis, reasoning, MRC 등 태스크에서 개선효과가 있었음 문장레벨에서 CLINE이 서로 다른 의미에 대해서 분리되고 같은 의미에 대해서는 모이는 것도 확인할 수 있었음(임베딩얘긴듯..) Introduction BERT, RoBERTa등 PLM이 NLP를 개선하는데 효과적임을 보였음 PLM은 adversarial examples에 대해서 poort robustness를 가짐 (어쩌면..이래서 p-tuning등이 잘되는건 아닐까?) 테이블1에서 보면 ultimately 라는 단어를 비슷한 단어인 lastly로 교체하면 결과가 바뀌는걸 볼 수 있음 PLM의 robustness를 키우기 위해 adversarial training을 word embeddings에 gradient-based pertubation을 적용시켜서 하거나, high-quality adversarial textual examples를 추가하는 식으로 진행한 연구들이 있었음 하지만 작은 변화가 의미변화를 만드는걸 피할 순 없었음 can we train a BERT that is both defensive against adversarial attacks and sensitive to semantic changes by using both adversarial and contrastive examples? robust semantic-aware PLM을 학습하기 위해서 CLINE을 제안함 adversarial &amp; contrastive examples를 만드는 방법론임 WordNet을 사용함 (안돼 ㅠㅠ 넘 귀찮단…) replaced token detection &amp; contrastive objectives 적용 NLP benchmark에서 RoBERTa 모델 기준 +1.6% 개선(4 contrastive test sets), +0.5% 개선함(4 adversarial test sets) Pilot Experiment and Analysis TextFooler (Jin et al., 2020), as the word-level adversarial attack model 을 통해 adversarial examples 만듬 model’s true linguistic capabilities (Kaushik et al., 2020; Gardner et al., 2020)을 기반으로 contrastive sets 만듬 (MLM 같은건가..?) Model and Datasets IMDB SNLI 학습방법 및 모델 방법: adversarial training method FreeLB (Zhu et al., 2020) for our pilot experiment. 모델: vanilla BERT, RoBERTa Result Analysis Table2를 보면 방법, 데이터셋 간의 비교 결과를 확인할 수있음 Adv에서는 성능오르지만 Rev에서 떨어지는건 역시, adversarial training이 constrative set에 negative effect를 줄 수 있다는걸 보여줌 아마도 adv training이 labels를 유지하려하고, contrastive set은 작은 변화에도 label이 바뀔수 있으니 그런 것 같음 (이해가 안되네….) Case Study adversarial training이 contrastive sets에서 실패하는 이유에 대해 더 알아보기 위해 IMDB 데이터셋을 연구함 Table3는 vanilla BERT에서는 잘 예측했지만, FreeLB BERT에서는 잘못 예측한 케이스임 대부분의 파트가 positive sentiments로 구성된걸 볼 수 있고 특정부분이 negative로 된걸 볼 수 있음, 전체적으로는 negative한 내용이 주를 이루고 vanilla BERT는 이를 잘 잡아냄, FreeLB BERT는 negative sentiment를 noise로 보고 전체문장을 positive로 예측한걸로 보임 adversarial training이 semantic changed adversarial examples에는 적합하지 않은것을 알 수 있음 이러한 이유로 semantic negative examples로부터 semantic이 바뀌었는지를 배우는 적합한 방법을 찾을 필요가 있음 MethodGeneration of Examples contrastive learning의 아이디어를 사용함 positive pairs끼리 뭉치게하고 negative pairs는 밀어내게함 어떤 연구들은 augmentation 사용(synonym replacement, back translation..)해서 positive instances를 만들었지만, negative instances들에 초점을 맞춘 연구는 거의 없었음 직관적으로 문장에서 atonym(반대어)를 교체하는건 의미적으로 적합하지 않기 쉬움 Notation 설명: x_ori (원본), x_syn(동의어), x_ant(반의어) spaCy로 segmentation &amp; POS 했고 verbs, nouns, adjectives, adversb등 추출함 x_syn은 synonyms로 교체한 버전이고, x_ant는 antonyms와 random words로 교체함 x_syn은 40% 토큰이 대체됨, x_ant는 20% 토큰이 대체됨 Training Objectives neural text encoder(Transformer)를 학습시킴 Masked Language Modeling objective Replaced Token Detection objective x_syn, x_ant에 대해서 어떤 토큰이 replace되었는지 디텍팅함 Contrastive Objective (x_ori, x_syn) == positive, (x_ori, x_ant) == negative [CLS] embeddings을 contrastive objective로 사용함 다른 contrastive strategies는 랜덤하게 multiple negative example를 뽑았지만, 본 연구에서는 x_ant만을 negative example로 사용함 (위에서 랜덤워드도 교체하긴 한다고 하지않았었나..? 뭐지..아아 아무 문장이나 두번째 페어로 쓰지 않고, 반대 문장 페어를 넣는다는 뜻인듯!) 그 이유는 semantically adversarial attacking에 강건하게 만들기 위함임 최종 loss 구성 ExperimentsImplementation RoBERTa 30K steps with a batch size of 256 sequences of maximum length 512 tokens Adam with a learning rate of 1e-4, β1 = 0.9, β2 = 0.999, ε =1e-8, L2 weight decay of 0.01, learning rate warmup over the first 500 steps, and linear decay of the learning rate 0.1 for dropout on all layers and in attention 32 NVIDIA Tesla V100 32GB GPUs Our model is pre-trained on a combination of BookCorpus and English Wikipedia datasets Datasets IMDB SNLI PERSPECTRUM BoolQ AG MR Experiments on Contrastive Sets Contrast consistency (Con) is a metric defined by Gardner et al. (2020) to evaluate whether a model’s predictions are all correct for the same examples in both the original test set and the contrastive test set Ablation Study w/o RTD: we remove the replaced token detection objective (LRTD) in our model to verify whether our model mainly benefits from the contrastive objective. w/o Hard Negative: we replace the constructed negative examples with random sampling examples to verify whether the negative examples constructed by unsupervised word substitution are better. Sentence Semantic Representation 9626 문장 triplets를 MR sentiment analysis dataset에서 생성함 the model correctly identifies the semantic relationship (e.g., if BertScore(x_ori,x_syn)&gt;BertScore(x_ori,x_ant)) as Hits. max Hits on all layers (from 1 to 12) of Transformers-based encoder in Table 7 Conclusion how to train a pre-trained language model with robustness against adversarial attacks and sensitivity to small changed semantics. CLINE, a simple and effective method to tackle the challenge. In the training phase of CLINE, it automatically generates the adversarial example and semantic negative example to the original sentence the model is trained by three objectives to make full utilization of both sides of examples","link":"/CLINE-Contrastive%20Learning%20with%20Semantic%20Negative%20Examples%20for%20Natural%20Language%20Understanding/"},{"title":"COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining","text":"논문파일 COCO-LM- Correcting and Contrasting Text Sequences for Language Model Pretraining.pdf Ref github: https://github.com/microsoft/COCO-LM 발표슬라이드: COCO_LM_220622.pdf Author 저자: Yu Meng1∗, Chenyan Xiong2, Payal Bajaj2, Saurabh Tiwary2, Paul Bennett2, Jiawei Han1, Xia Song21 University of Illinois at Urbana-Champaign 2 Microsoft NeurIPS 2021 논문 요약 ELECTRA 개선 버전 논문 논문 나온 순서는 대략 ELECTRA - COCO-LM - SimCSE 가 됨 RTD를 copy mechanism에 녹여서 multi-task learning으로 All-token MLM 사용 sentence similarity를 PLM self-supervised learning 안에 추가함 present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences Related work 논문 순서는 대략 ELECTRA - COCO-LM - SimCSE 가 됨 약간 ELECTRA 논문의 연장선, 혹은 변형 ELECTRA의 contribution -&gt; 15% 계산량 -&gt; 100%로 늘려서 효율 높임 input token copy mechanism 실험 [MASK] token 대신 generator가 생성한 토큰을 써서 [MASK] 토큰이 일으키는 pre-train/fine-tune discrepancy 제거 Replaced Token Detection (RTD) 태스크 제안 Abstract present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences Following ELECTRA-style pretraining, COCO-LM employs an auxiliary language model to corrupt text sequences, upon which it constructs two new tasks for pretraining the main model The first token-level task, Corrective Language Modeling, is to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics. The second sequence-level task, Sequence Contrastive Learning, is to align text sequences originated from the same source input while ensuring uniformity in the representation space achieves the MNLI accuracy of ELECTRA with 50% of its pretraining GPU hours. With the same pretraining steps of standard base/large-sized models, COCO-LM outperforms the previous best models by 1+ GLUE average points. Introduction (무슨말이지..) ELECTRA, that uses an auxiliary language model (“generator”) to replace tokens in input texts and pretrains the main Transformer (“discriminator”) to detect replaced tokens. This improves the pretraining efficiency and effectiveness, but pretraining via binary classification hinders the model’s usage on applications requiring language modeling capability (e.g., prompt-based learning [15, 28, 46]). It could further distort the representation space as the Transformers are pretrained to output the same “non-replacement” label for all actual tokens. present a new self-supervised learning approach, COCO-LM, that pretrains Lan guage Models by COrrecting and COntrasting corrupted text sequences Following ELECTRA-style pretraining, COCO-LM employs an auxiliary model to corrupt the input texts, upon which it introduces two new pretraining tasks for the main Transformer, one at token level and one at sequence level. The token-level task, corrective language modeling (CLM), pretrains the main Transformer to detect and correct the tokens in the corrupted sequences. It uses a multi-task setup to combine the benefits of replaced token detection and language modeling. The sequence-level task, sequence contrastive learning (SCL), pretrains the model to align text sequences originated from the same source sequence and enforce uniformity of the representation space GLUE [54] and SQuAD [41] benchmarks, COCO-LM not only outperforms state-of-the-art pretraining approaches in effectiveness, but also significantly improves the pretraining efficiency Related Work Empirically, MLM is still among the most effective tasks to pretrain encoders Instead of randomly altering texts, ELECTRA [7] uses a smaller auxiliary Transformer pretrained by MLM to replace some tokens in the text sequences using its language modeling probability, and pretrains the main Transformer to detect the replaced tokens. ELECTRA achieves state-of-the-art accuracy in many language tasks [7]. Later, Clark et el. [6] developed ELECTRIC, which pretrains encoders by contrasting original tokens against negatives sampled from a cloze model. ELECTRIC re-enables the language modeling capability but underperforms ELECTRA in downstream tasks. Our work is also related to contrastive learning which has shown great success in visual representation learning [4, 22, 34]. Its effectiveness of in language is more observed in the fine-tuning stage, for example, in sentence representation [16], dense retrieval [60], and GLUE fine-tuning [19]. Method We present the preliminaries of PLMs, their challenges, and the new COCO-LM framework. Preliminary on Language Model Pretraining In this work we focus on pretraining BERT-style bidirectional Transformer encoders first recap the masked language modeling (MLM) task introduced by BERT [11] and then discuss the pretraining framework of ELECTRA BERT Pretraining ELECTRA Pretraining Challenges of ELECTRA-Style Pretraining Missing Language Modeling Benefits. classification task in ELECTRA is simpler and more stable [61], but raises two challenges. first is the lack of language modeling capability which is a necessity in some tasks [6]. For example, prompt-based learning requires a language model to generate labels second is that the binary classification task may not be sufficient to capture certain word-level semantics that are critical for token-level tasks Squeezing Representation Space the representations from Transformer-based language models often reside in a narrow cone, where two random sentences have high similarity scores (lack of uniformity) closely related sentences may have more different representations (lack of alignment) Figure 1 illustrates such behaviors with random sentence pairs (from pretraining corpus) and semantically similar pairs (those annotated with maximum similarity from STS-B [3]). With RoBERTa, the cosine similarities of most random sentence pairs are near 0.8, bigger than many semantically similar pairs. The representation space from ELECTRA is even more squeezed. Nearly all sentence pairs, both random and similar ones, have around 0.9 cosine similarity. This may not be surprising as ELECTRA is pretrained to predict the same output (“non-replacement”) for all tokens in these sequences. The irregular representation space raises the risk of degeneration [37, 55] and often necessitates sophisticated post-adjustment or fine-tuning to improve the sequence representations [16, 30, 32, 60]. COCO-LM Pretraining The auxiliary Transformer is pretrained by masked language modeling (MLM) and generates corrupted sequences. The main Transformer is pretrained to correct the corruption (CLM) and to contrast the corrupted sequences with the cropped sequences (SCL) Network Configurations auxiliary model Similar to ELECTRA, the auxiliary Transformer is smaller than the main model We reduce the number of layers to 1/3 or 1/4 (under base or large model setup, respectively) but keep its hidden dimension the same with the main model, instead of shrinking its hidden dimensions We disable dropout in it when sampling replacement tokens. main model standard architecture of BERT/ELECTRA Experimental Setup Pretraining Settings base, base++, and large++. Base is the BERTBase training configuration [11]: Pretraining on Wikipedia and BookCorpus [63] (16 GB of texts) for 256 million samples on 512 token sequences 32, 768 uncased BPE vocabulary Model Architecture base/base++ model uses the BERT Base architecture [11]: 12 layer Transformer, 768 hidden size, plus T5 relative position encoding. large++ model is the same with BERTLarge, 24 layer and 1024 hidden size, plus T5 relative position encoding auxiliary network uses the same hidden size but a shallow 4-layer Transformer in base/base++ and a 6-layer one in large++. When generating XMLM we disable dropout in the auxiliary model Downstream Tasks GLUE [54] and SQuAD 2.0 Standard hyperparameter search in fine-tuning is performed, and the search space can be found in Appendix B. reported results are the median of five random seeds on GLUE and SQuAD Evaluation Results COCO-LM outperforms all recent state-of-the-art pretraining models on GLUE average and SQuAD Efficiency COCO-LM is more efficient in GPU hours. It outperforms RoBERTa &amp; ELECTRA by 1+ points Ablation Studies base setting on GLUE DEV 예상과는 좀 다른.. Architecture. Removing relative position encoding (Rel-Pos) leads to better numbers on some tasks but significantly hurts MNLI. Pretraining Signal Construction. Using randomly replaced tokens to corrupt text sequence hurts significantly. Using a converged auxiliary network to pretrain the main model also hurts. It is better to pretrain the two Transformers together CLM Setup. Disabling the multi-task learning and using All-Token MLM [7] reduces model accuracy. The copy mechanism is effective. The benefits of the stop gradient operation are more on stability (preventing training divergence). Analyses of Contrastive Learning with SCLAblation on Data Augmentation Alignment and Uniformity The representation space from COCO-LM is drastically different from those in Figure 1 With COCO-LM, similar pairs are more aligned and random pairs are distributed more uniformly Their average cosine similarity is 0.925 when pretrained with SCL, while is 0.863 without SCL. This better alignment and uniformity is achieved by COCO-LM with SCL via pretraining Regularizing the Representation Learning for Better Few-Shot Ability. SCL is necessary to regularize the representation space and to reduce the risk of degeneration Analyses of Language Modeling with CLM CLM과 All-Token MLM 비교 It is quite an unbalanced task For the majority of the tokens (Original) the task is simply to copy its input at the same position. For the replaced tokens (7 − 8% total), however, the model needs to detect the abnormality brought by the auxiliary model and recover the original token Implicitly training the copy mechanism as part of the hard LM task is not effective: The copy accuracy of All-Token MLM is much lower, and thus the LM head may confuse original tokens with replaced ones As shown in Table 3 and ELECTRA [7], pretraining with All-Token MLM performs worse than using the RTD task, though the latter is equivalent to only training the copy mechanism The multi-task learning of CLM is necessary for the main Transformer to stably learn the language modeling task upon the corrupted text sequence. Prompt-Based Fine-Tuning with CLM the prompt-based fine-tuning experiments on MNLI for RoBERTa and COCO-LM under base++ and large++ sizes COCO-LM’s main Transformer does not even see any[MASK] tokens during pretraining but still performs well on predicting masked tokens for prompt-based learning. Note that ELECTRA and COCO-LM variants without the CLM task are not applicable: Their main Transformers are not pretrained by language modeling tasks (thus no language modeling capability is learned to generate prompt label words). Conclusion and Future Work we present COCO-LM, which pretrains language models using Corrective Language Modeling and Sequence Contrastive Learning upon corrupted text sequences With standard pre- training data and Transformer architectures, COCO-LM improves the accuracy on the GLUE and SQuAD benchmarks, while also being more efficient in utilizing pretraining computing resources and network parameters One limitation of this work is that the contrastive pairs are constructed by simple cropping and MLM replacements To better understand and tailor the training of the auxiliary model to the main model is another important future research direction 코드구현 loss 관련 코드 스니펫: https://github.com/microsoft/COCO-LM/issues/2#issuecomment-1003639940 scl쪽 (span으로 한번 임베딩뽑고, src로도 한번 뽑고) 위 코드 위치: https://github.com/microsoft/COCO-LM/blob/6bb6e5f62d65349657dd51f2f535454a1c50c2e9/fairseq/fairseq/models/cocolm/model.py#L190 unofficial implementation: https://github.com/lucidrains/coco-lm-pytorch/blob/main/coco_lm_pytorch/coco_lm_pytorch.py","link":"/COCO-LM-Correcting%20and%20Contrasting%20Text%20Sequences%20for%20Language%20Model%20Pretraining/"},{"title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","text":"Author 저자: Yinhan Liu∗§ Myle Ott∗§ Naman Goyal∗§ Jingfei Du∗§ Mandar Joshi† Danqi Chen§ Omer Levy§ Mike Lewis§ Luke Zettlemoyer†§ Veselin Stoyanov§ † Paul G. Allen School of Computer Science &amp; Engineering, University of Washington, Seattle, WA § Facebook AI 느낀점Abstract hyperparameter choices have significant impact on the final results carefully measures the impact of many key hyperparameters and training data size find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it Introduction We present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. modifications (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. contributions (1) We present a set of important BERT design choices and training strategies and introduce alternatives that lead to better downstream task performance; (2) We use a novel dataset, CC-NEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. BackgroundTraining Objectives MLM objective is a cross-entropy loss on predicting the masked tokens BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK ], 10% are left unchanged and 10% are replaced by a randomly selected vocabulary token In the original implementation, random masking and replacement is performed once in the be- ginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence (기존과 달리 dynamic masking 하겠습니다~) NSP (로버타는 쓰지 않지만) Optimization BERT is optimized with Adam (Kingma and Ba, 2015) using the following parameters: β1 = 0.9, β2 = 0.999, ǫ = 1e-6 and L2 weight decay of 0.01. The learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed BERT trains with a dropout of 0.1 on all layers and attention weights, and a GELU activation function Models are pretrained for S = 1,000,000 updates, with mini-batches containing B = 256 sequences of maxi- mum length T = 512 tokens. Data trained on a combination of BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA, which totals 16GB of uncompressed text Experimental SetupImplementation optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. additionally found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it we found setting β2 = 0.98 to improve stability when training with large batch sizes. Unlike Devlin et al. (2019), we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first 90% of updates. We train only with full-length sequences. train with mixed precision floating point arithmetic on DGX-1 machines, each with 8 × 32GB Nvidia V100 GPUs Data five English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA. This is the original data used to train BERT. (16GB) CC-NEWS, which we collected from the English portion of the CommonCrawl News dataset (Nagel, 2016). The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering) OPENWEBTEXT (Gokaslan and Cohen, 2019), an open-source recreation of the WebText corpus described in Radford et al. (2019). The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB). STORIES, a dataset introduced in Trinh and Le (2018) containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas. (31GB) Evaluation GLUE: is a collection of 9 datasets for evaluating natural language understanding systems SQuAD: is to answer the question by extracting the relevant span from the context. evaluate on two versions of SQuAD: V1.1 and V2.0 In V1.1 the context always contains an answer, whereas in V2.0 some questions are not answered in the provided context, making the task more challenging. For SQuAD V2.0, we add an additional binary classifier to predict whether the question is answerable, which we train jointly by summing the classification and span loss terms RACE: is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions is collected from English examinations in China, which are designed for middle and high school students. each passage is associated with multiple questions. For every question, the task is to select one correct answer from four options RACE has significantly longer context than other popular reading comprehension datasets and the proportion of questions that requires reasoning is very large Training Procedure Analysis Model Architecture: BERT_BASE (L = 12, H = 768, A = 12, 110M params) Static vs. Dynamic Masking To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training Thus, each training sequence was seen with the same mask four times during training 1문장을 10개씩 복사해서 다른 mask를 생성하게 만들게 하고 40에폭 돌림 -&gt; 10*4 -&gt; 완전히 똑같은 마스킹이된 문장은 4번만 보게됨! 약간 에폭 계산 방법이 이상한데 이게 맞긴맞나봄 becomes crucial when pretraining for more steps or with larger datasets 같은 문장을 여러 방법으로 쓸 수 있다는 점에서 dynamic masking에 epoch이 많은게 중요할듯 dynamic masking is comparable or slightly better than static masking 살짝만 좋아지긴하지만 5번 반복에 대한 meidan 값이니 유의미한 셋팅으로 봐야 Model Input Format and Next Sentence Prediction SEGMENT-PAIR+NSP: This follows the original input format used in BERT (Devlin et al., 2019), with the NSP loss. Each input has a pair of segments, which can each contain multiple natural sentences. 여러 문장들 SENTENCE-PAIR+NSP: Each input contains a pair of natural sentences, either sampled from a contiguous portion of one document or from separate documents. Since these inputs are significantly shorter than 512 tokens. 딱 한문장 FULL-SENTENCES: Each input is packed with full sentences sampled contiguously from one or more documents. Inputs may cross document boundaries. When we reach the end of one document, we begin sampling sentences from the next document and add an extra separator token between documents 여러 문장들 + 문서 끊기면 SEP 추가후 다음문서에서 여러문장들 DOC-SENTENCES: Inputs are constructed similarly to FULL-SENTENCES, except that they may not cross document boundaries. Inputs sampled near the end of a document may be shorter than 512 tokens, so we dynamically increase the batch size in these cases to achieve a similar number of total tokens as FULL- SENTENCES 문서 끝에서 샘플링하면 토큰수가 적을테니 이런 경우는 dynamic하게 배치사이즈 키워서 FULL-SENTENCES와 비슷한 토큰개수 만들려고함 DOC-SENTENCES가 결과상으로는 제일 좋네.. SEP 없이 한 문서내에서 샘플링하는게 좋다는걸까? Results using individual sentences hurts performance on downstream tasks hypothesize is because the model is not able to learn long-range dependencies removing the NSP loss matches or slightly improves downstream task performance restricting sequences to come from a single document (DOC-SENTENCES) performs slightly better than packing sequences from multiple documents (FULL-SENTENCES). However, because the DOC-SENTENCES format results in variable batch sizes, we use FULL-SENTENCES in the remainder of our experiments for easier comparison with related work Training with large batches Devlin et al. (2019) originally trained BERTBASE for 1M steps with a batch size of 256 sequences. This is equivalent in computational cost, via gradient accumulation, to training for 125K steps with a batch size of 2K sequences, or for 31K steps with a batch size of 8K. 첫번째는 gradient accumulation 8, 두번째는 32 observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training, and in later experiments we train with batches of 8K sequences. Text Encoding Byte-Pair Encoding (BPE) BPE vocabulary sizes typically range from 10K-100K subword units. However, unicode characters can account for a sizeable portion of this vocabulary when modeling large Radford et al. (2019) introduce a clever implementation of BPE that uses bytes instead of unicode characters as the base subword units. Using bytes makes it possible to learn a subword vocabulary of a modest size (50K units) that can still encode any input text without introducing any “unknown” tokens. The original BERT implementation (Devlin et al., 2019) uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules. Following Radford et al. (2019), we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input. This adds approximately 15M and 20M additional parameters for BERTBASE and BERTLARGE, respectively RoBERTa에서 byte-level BPE를 사용했었군.. 약간 골치아프겠네 Early experiments revealed only slight differences between these encodings, with the Radford et al. (2019) BPE achieving slightly worse end-task performance on some tasks. Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments. A more detailed comparison of these encodings is left to future work. 의외로 성능상에선 애매한 결과가 있었나..? RoBERTa We call this configuration RoBERTa for Robustly optimized BERT apporach trained with dynamic masking (Section 4.1) FULL-SENTENCES without NSP loss (Section 4.2) large mini-batches (Section 4.3) large byte-level BPE (Section 4.4) investigate two other important factors (1) the data used for pretraining, and (2) the number of training passes through the data. 학습했던 백본 셋팅: we begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters) Results 학습셋 추가 + 길게 학습하면 성능이 오른다 gradient accumulation 셋팅은 32정도인가 -&gt; 8k 배치를 위해서 GLUE Results In the first setting (single-task, dev) consider a limited hyperparameter sweep for each task, with batch sizes ∈ {16, 32} and learning rates ∈ {1e−5, 2e−5, 3e−5}, with a linear warmup for the first 6% of steps followed by a linear decay to 0. We finetune for 10 epochs and perform early stopping based on each task’s evaluation metric on the dev set In the second setting (ensembles, test) While many submissions to the GLUE leaderboard depend on multi-task finetuning, our submission depends only on single-task finetuning. For RTE, STS and MRPC we found it helpful to finetune starting from the MNLI single-task model, rather than the baseline pretrained RoBERTa. SQuAD Results we only finetune RoBERTa using the provided SQuAD training data RACE Results Conclusion We find that performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data Params","link":"/RoBERTa-A%20Robustly%20Optimized%20BERT%20Pretraining%20Approach/"},{"title":"WARP: Word-level Adversarial ReProgramming","text":"Author 저자: Karen Hambardzumyan1, Hrant Khachatrian1,2, Jonathan May3 (1YerevaNN, 2Yerevan State University,3Information Sciences Institute, University of Southern California), 2021 느낀점 PET + p-tuning Abstract 대부분의 transfer learning은 params sharing을 최대화해서, 하나 혹은 여러 task-specific layers를 LM 위에 쌓아서 학습하는 형태임 본 논문에서는 다른 형태로 automatic propmpt generation이라는 선행연구 기반의 adversarial reprogramming 방법을 사용함 Adversarial reprogramming에서는 task-specific word embeddings 을 학습하는데, 이는 특정 input text가 합쳐져서 입력으로 들어올때 LM이 specified task를 해결하게 하는 것임 (이래서 propmpt연구의 확장이라 했나..) 25K trainable params로 25M trainable params 모델까지 outperform했음 (GLUE benchmark 기준) task-specific human-readable prompts로 few-shot setting(32 training samples)에서 2개의 SuperGLUE task에서 GPT-3보다 좋은 성능을 냄 Introduction 요즘 pretrained model을 쓰는 대안은 adapters라고도 불리는데, 모든 레이어에 new weights를 더하는 방식으로 진행됨(ptrLM params은 frozen) 이러한 방법은 smaller set of task-specific params로 fine-tuning과 비슷한 성능을 냄 또 다른 연구는 “task descriptions”를 제공하는 방법론임 (labeled examples 없이) GPT-3의 경우가 이에 해당 이러한 방법론은 대신 huge LM (1.5B~175B) 가 필요함 reformulation-based approach (prompt)에서 성능을 좋게 만드는 extra tokens을 찾을 수 있으면 손으로 직접 디자인한 것보다 좋은 성능 낼 수 있을 것 optimal prompts르 찾는 테크닉 제안(WARP: Word-level Adversarial ReProgramming) 이 방법론은 이미지쪽 adaversarial program을 보고 아이디어를 얻음 (이름부터가 이미..) 여러 결과에서 좋은 성적 얻음 GLUE leaderboard에서 81.6 test score를 얻었음(25K trainable params) 32 examples few-shot -&gt; SuperGLUDE에서 GPT-3를 이기기도함(2개 태스크) Related Work Towards Fewer Trainable Parameters 레이어마다 파라미터 추가하거나.. knowledge distillation하거나 등등 Task Reformulation GPT 계열처럼 prompt 넣기 MLM 처럼 빈칸채우기 (PET) Adversarial Reprogramming input값을 바꿔줘서 (perturbations) 학습시키는 것 text classification 쪽에도 연구가 있긴 했었음 AutoPrompt와 다르게, 본 연구에서는 word embedding space에 대해 gradient-based optimization을 수행함 WARP Goal: MLM이 원하는 verbalizer token을 answer로 뱉어낼 수 있는 최고의 prompt (continuous embeddng)를 찾는 것\u001c 다른말로하면, prompt에 대한 파라미터와 verbalizer embeddings 에대한 parameter를 찾고 있음 확률은 다음과 같이 나타냄 T는 프롬프트 임베딩이 들어가는 템플릿을 뜻함 C는 클래스 집합 f(x)는 MLM output이고 theta P, theta V는 워드 임베딩임과 같은 임베딩 스페이스에 있는 벡터임 P쪽이 prompt, V쪽이 class 라고 보면 될듯 Method prompt tokens [P_1], [P_2], ..., [P_K] 와 Maksed Token [MASK]를 input sequence에 추가함 prompt template에 따라 프롬프트 토큰은 문장 앞뒤중간에 존재함 (이게 좀 애매하다…..영~) Xentory로 MLM의 output head와 verbalizer tokens [V_1], [V_2], ..., [V_C] 간의 loss를 optimization함 (약간 PET + p tuning인데..) 나머지 LM params은 건드리지 않음 adversarial attack과는 다르게 original input tokens을 바꾸거나 하진 않음 Implementation Details GLUE task roberta-large pytorch few-shot task albert-xxlarge-v2 (iPET과 비교 위해) Optim Adam slanted triangular scheduler (6% warm-up steps &amp; 10-20 epochs on each task) batch 1024 tokens &amp; 8 examples speed up ptrLM의 dropout 제거 2.5-3배정도 fine-tuning보다 빠르고, frozen features 보다는 2배정도 느림 Experiments on GLUE Few-Shot Experiments Discussion prompts보단 verbalizers에서 좀 더 해석가능한 결과가 나왔음 해당 임베딩과 cosine sim으로 가장 가까운 토큰이 무엇인지 보여줌 (토큰벡터는 레이어중 어디꺼를 빼다 쓴건지.., 그냥 워드임베딩 레이어인가) Conclusion optimized embedding을 input text에 추가하는 방법론으로 transfer learning의 다른 대안을 제안해봄 GLUE나 SuperGLUE에서 좋은 성능을 보여줌","link":"/WARP-Word-level%20Adversarial%20ReProgramming/"},{"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism","text":"용어 reduce -&gt; 각 프로세스가 가진 값들을 특정한 하나의 process에 연산해서 모으는 연산 (하나의 값으로 모음) all+* -&gt; 이름 앞에 all이 붙으면 연산결과를 참여한 모든 프로세스가 동일하게 반환받음 all reduce -&gt; 하나의 디바이스가 reduce한 값을 참여한 모든 프로세스가 동일하게 받을 수 있게 전달 논문파일Megatron-LM- Training Multi-Billion Parameter Language Models Using Model Parallelism.pdf Ref https://www.youtube.com/watch?v=w4a-ARCEiqU Author 저자: Mohammad Shoeybi 1 2 Mostofa Patwary 1 2 Raul Puri 1 2 Patrick LeGresley 2 Jared Casper 2 Bryan Catanzaro 2 느낀점 gelu이슈로 col parallel 후 아풋값을 유지하기 위해 row parallel 하는게 포인트 Abstract efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. can be fully implemented with the insertion of a few communication operations in native PyTorch this approach by converging transformer based models up to 8.3 billion parameters using 512 GPU achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%). Introduction 메모리를 잘 쓰기 위해서 여러 방법들이 있었음 activation checkpointing ADAM은 momentum이랑 optimizer state때문에 파라미터마다 해당 정보를 저장해야했음 model parallelism overcome this limit by partitioning the model such that the weights and their associated optimizer state do not need to reside concurrently on the processor. Mesh-Tensorflow같은게 대표적인 프레임워크임 In this work, we implement a simple and efficient model parallel approach using intra-layer model-parallelism. We show that the existing BERT architecture results in model degradation as the size increases We overcome this challenge by rearranging the layer normalization and residual connection in the transformer layers and show that with this change, results for the downstream tasks on development sets improve monotonically as the model size increases our contributions are as follows: • We implement a simple and efficient model parallel approach by making only a few targeted modifications to an existing PyTorch transformer implementation. • We perform an in-depth empirical analysis of our model and data parallel technique and demonstrate up to 76% scaling efficiency using 512 GPUs. • We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased accuracies as the model grows. • We demonstrate that scaling the model size results in improved accuracies for both GPT-2 (studied up to 8.3 billion parameters) and BERT (studied up to 3.9B parameters) models. • We showcase that our models achieve state of the art results on test sets: perplexity on WikiText103 (10.8 ppl), accuracy on LAMBADA (66.5%), and accuracy on RACE (90.9%). Background and ChallengesTransformer Langauge Models and Multi-Head Attention Data and Model Parallelism in Deep Learning One solution to this problem is to employ parameter sharing to reduce the memory footprint of the model (Lan et al., 2019), but this limits the overall capacity of the model. Our approach is to utilize model parallelism to split the model across multiple accelerators. This not only alleviates the memory pressure, but also increases the amount of parallelism independently of the microbatch size. Within model parallelism, there are two further paradigms: layer-wise pipeline parallelism groups of operations are performed on one device before the outputs are passed to the next device in the pipeline where a different group of operations are performed. However these suffer from inconsistency issues. The GPipe framework for TensorFlow (Huang et al., 2018) overcomes this inconsistency issue by using synchronous gradient decent. This approach requires additional logic to handle the efficient pipelining of these communication and computation operations, and suffers from pipeline bubbles that reduce efficiency, or changes to the optimizer itself which impact accuracy. more general distributed tensor computation. Distributed tensor computation is an orthogonal and more general approach that partitions a tensor operation across multiple devices to accelerate computation or increase model size FlexFlow (Jia et al., 2018), a deep learning framework orchestrating such parallel computation, provides a method to pick the best parallelization strategy. Recently, Mesh-TensorFlow (Shazeer et al., 2018) introduced a language for specifying a general class of distributed tensor computations in TensorFlow (Abadi et al., 2015) Model Parallel Transformers We take advantage of the structure of transformer networks to create a simple model parallel implementation by adding a few synchronization primitives We introduce model parallelism in both of these blocks separately. Hence, we partition the first GEMM(X) in this column parallel fashion and split the second GEMM(A = [A_1, A_2]) along its rows so it takes the output(Y = [Y_1, Y_2]) of the GeLU layer directly without requiring any communication as shown in Figure 3a. The output of the second GEMM is then reduced across the GPUs before passing the output to the dropout layer. This approach splits both GEMMs in the MLP block across GPUs and requires only a single all-reduce operation in the forward pass (g operator) and a single all-reduce in the backward pass (f operator). 123456789 class f(torch.autograd.Function): def forward(ctx, x): return x def backward(ctx, gradient): all_reduce(gradient) return gradient# Code 1. Implementation of f operator.# g is similar to f with identity in the backward and all-reduce in the forward functions. for the self attention block we exploit inherent parallelism in the multihead attention operation, partitioning the GEMMs associated with key (K), query (Q), and value (V ) in a column parallel fashion such that the matrix multiply corresponding to each attention head is done locally on one GPU. This allows us to split per attention head parameters and workload across the GPUs, and doesn’t require any immediate communication to complete the self-attention The subsequent GEMM from the output linear layer (after self attention) is parallelized along its rows and takes the output of the parallel attention layer directly, without requiring communication between the GPUs. This approach for both the MLP and self attention layer fuses groups of two GEMMs, eliminates a synchronization point in between, and results in better scaling This enables us to perform all GEMMs in a simple transformer layer using **only two all-reduces in the forward path(self-attn g func에서 한번 MLP g func에서 한번) ** and two in the backward path (self-attn f func에서 한번 MLP f func에서 한번) We parallelize the input embedding weight matrix E_{H×v} along the vocabulary dimension E = [E1, E2] (column-wise) To reduce the communication size, we fuse the output of the parallel GEMM [Y1 , Y2 ] with the cross entropy loss which reduces the dimension to b × s. Communicating scalar losses instead of logits is a huge reduction in communication that improves the efficiency of our model parallel approach. Rather than having one GPU compute part of the dropout, layer normalization, or residual connections and broadcast the results to other GPUs, we choose to duplicate the computation across GPUs. Specifi- cally, we maintain duplicate copies of layer normalization parameters on each GPU, and take the output of the model parallel region and run dropout and residual connection on these tensors before feeding them as input to the next model parallel regions 카피 했다는게, 각자 그냥 갖고 있는다는건지.. 아니면 하나가 계산한 값을 같이 쓴다는건지.. 후자는 좀 이상한거같고 Setup In this work we focus on GPT-2 (Radford et al., 2019), a left- to-right generative transformer based language model, and BERT (Devlin et al., 2018), a bi-directional transformer model based on language model masking Training Dataset filtered out all the documents with content length less than 128 tokens used locality-sensitive hashing (LSH) to deduplicate content with a jaccard similarity greater than 0.7 resulting aggregate corpus contains 174 GB of deduplicated text Training Optimization and Hyperparameters utilize mixed precision training with dynamic loss scaling to take advantage of the V100’s Tensor Cores1234567ref: https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.htmlQ: How does dynamic scaling work?A: Dynamic loss scaling basically attempts to ride the edge of the highest loss scale it can use without causing gradient overflow, to make full use of the FP16 dynamic range.It does so by beginning with a high loss scale value (say, 2^24), then in each iteration, checking the gradients for overflows (infs/NaNs). If none of the gradients overflowed, gradients are unscaled (in FP32) and optimizer.step() is applied as usual. If an overflow was detected, optimizer.step is patched to skip the actual weight update (so that the inf/NaN gradients do not pollute the weights) and the loss scale is reduced by some factor F (F=2 by default). This takes care of reducing the loss scale to a range where overflows are not produced. However, it's only half the story.What if, at some later point, training has stabilized and a higher loss scale is permissible? For example, later in training, gradient magnitudes tend to be smaller, and may require a higher loss scale to prevent underflow. Therefore, dynamic loss scaling also attempts to increase the loss scale by a factor of F every N iterations (N=2000 by default). If increasing the loss scale causes an overflow once more, the step is skipped and the loss scale is reduced back to the pre-increase value as usual. In this way, by: reducing the loss scale whenever a gradient overflow is encountered, and Intermittently attempting to increase the loss scale, the goal of riding the edge of the highest loss scale that can be used without causing overflow is (roughly) accomplished. initializing our weights W with a simple normal distribution W ∼ N (0, 0.02) then scale weights immediately before residual layers by 1/root(2N) where N is the number of transformer layers comprised of self attention and MLP blocks our optimizer we utilize Adam (Kingma &amp; Ba, 2014) with weight decay (Loshchilov &amp; Hutter, 2019) λ = 0.01. use global gradient norm clipping of 1.0 to improve the stability of training large models. dropout of 0.1 is used to better manage our memory footprint we utilize activation checkpointing (Chen et al., 2016) after every transformer layer. For GPT-2 models all training is performed with sequences of 1024 subword units at a batch size of 512 for 300k iterations. Our learning rate of 1.5e-4 utilizes a warmup period of 3k iterations before following a single cycle cosine decay over the remaining 297k iterations We stop the decay at a minimum learning rate of 1e-5. For BERT models use the original BERT dictionary with vocab size of 30,522 replace the next sentence prediction head with sentence order prediction use whole word n-gram masking set the batch size to 1024 and use a learning rate of 1.0e-4 warmed up over 10,000 iterations and decayed linearly over 2 million iterations. Other training parameters are kept the same as (Devlin et al., 2018). Experiments 실험용 머신 32 DGX-2H servers (a total of 512 Tesla V100 SXM3 32GB GPUs) multi-node deep learning applications, with 300 GB/sec bandwidth between GPUs inside a server via NVSwitch and 100 GB/sec of interconnect bandwidth between servers using 8 InfiniBand adapters per server Scaling Analysis consider GPT-2 models with four sets of parameters detailed in Table 1 To have consistent GEMM sizes in the self attention layer, the hidden size per attention head is kept constant at 96 while the number of heads and layers are varied to obtain configurations ranging from 1 billion to 8 billion parameters 1.2 billion parameters fits on a single GPU whereas the 8 billion parameter model requires 8-way model parallelism (8 GPUs). original vocabulary size was 50,257, however, to have efficient GEMMs for the logit layer, it is beneficial for the per-GPU vocabulary size to be a multiple of 128. Since we study up to 8-way model parallelism, we pad the vocabulary such that it is divisible by 128 × 8 = 1024, resulting in a padded vocabulary size of 51,200. 그럼 나머지 1024 * 5에 해당하는 5개는 뭘까.. 8 way씩 5개를 쓴건가 For the model parallel scaling, a fixed batch size of 8 is used across all configurations. Data parallel scaling is necessary for training many state of the art models which typically use a much larger global batch size. To this end, for the model+data parallel cases we fix the global batch size to 512 for all experiments which corresponds to 64-way data parallelism. observe excellent scaling numbers in both settings. For example, the 8.3 billion parameters case with 8-way (8 GPU) model parallelism achieves 77% of linear scaling. Model+data parallelism requires further communication of gradients and as a result the scaling numbers drop slightly. However, even for the largest configuration (8.3 billion parameters) running on 512 GPUs, we achieve 74% scaling relative to linear scaling of the strong single GPU baseline configuration (1.2 billion parameters). weak scaling -&gt; linear 대비 나쁘지않다 Language Modeling Results Using GPT-2 Bi-directional Transformer Results Using BERT Prior work (Lan et al., 2019) found that increasing model size beyond BERT-large with 336M parameters results in unexpected model degradation. To address this degradation, the authors of that work (Lan et al., 2019) introduced parameter sharing and showed that that their models scale much better compared to the original BERT model. BERT에서 스케일업하려면 param sharing이 필요하다는건가.. We further investigated this behaviour and empirically demonstrated that rearranging the order of the layer normalization and the residual connections as shown in Figure 7 is critical to enable the scaling of the BERT-style models beyond BERT-Large. In all cases, the hidden size per attention head is kept constant at 64. 336M and 1.3B models are trained for 2 million iterations while the 3.9B model is trained for 1.5 million iterations and is still training. For finetuning, we follow the same procedure as (Liu et al., 2019b -&gt; 로버타 논문인듯..?). We first perform hyperparameter tuning on batch size and learning rate. Once we obtain the best values, we report the median development set results over 5 different random seeds for initialization Conclusion and Future Work we successfully surpassed the limitations posed by traditional single-GPU-per-model training by implementing model parallelism with only a few modifications to the existing PyTorch transformer implementations showed that for BERT models, careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased accuracies as the model size increases model size on down-stream task accuracy and achieve far superior results on downstream tasks and establish new SOTA for WikiText103, LAMBADA, and RACE datasets","link":"/Megatron-LM-Training%20Multi-Billion%20Parameter%20Language%20Models%20Using%20Model%20Parallelism/"},{"title":"Learning rate &amp; warmup step &amp; LR scheduling","text":"Background요즘 딥러닝을 하다보면 수도없이 접하게 되는 단어 중 하나는 learning rate, warmup, LR scheduler와 같은 것들입니다.이미 시중에 여러가지 기법들이 나와있고 한번은 정리해야겠다 생각했는데, 우연히 좋은 스레드를 발견하게 되서 공유해봅니다. 원문: What is exactly the learning rate warmup described in the paper? 버트 논문에는 다음과 같은 그림이 있습니다. 여기서 밑줄쳐진 learning rate warmup과 linear decay에 대한 내용입니다. Warmup일반적으로 lr warmup은 말그대로 천천히 lr을 올리는 작업을 뜻합니다.lr을 2e-5로 셋팅하고 warmup step을 10,000으로 셋팅한다면, (linear 일 경우) lr은 10,000 스텝동안 0에서 2e-5 까지 증가하게 됩니다.코드 구현을 보면 다음과 같습니다현재 스텝(global step)이 warmup 스텝 대비 어느정도이지 비율을 구하고 (warmup_percent_done = global_steps_float / warmup_steps_float)그에 맞는 warmup_lr을 구하는 방식으로 lr을 조절해갑니다. (warmup_learning_rate = init_lr * warmup_percent_done) 1234567891011121314151617181920212223242526272829303132333435363738394041def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu): &quot;&quot;&quot;Creates an optimizer training op.&quot;&quot;&quot; global_step = tf.train.get_or_create_global_step() learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32) # Implements linear decay of the learning rate. learning_rate = tf.train.polynomial_decay( learning_rate, global_step, num_train_steps, end_learning_rate=0.0, power=1.0, cycle=False) # Implements linear warmup. I.e., if global_step &lt; num_warmup_steps, the # learning rate will be `global_step/num_warmup_steps * init_lr`. if num_warmup_steps: global_steps_int = tf.cast(global_step, tf.int32) warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32) global_steps_float = tf.cast(global_steps_int, tf.float32) warmup_steps_float = tf.cast(warmup_steps_int, tf.float32) warmup_percent_done = global_steps_float / warmup_steps_float warmup_learning_rate = init_lr * warmup_percent_done is_warmup = tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32) learning_rate = ( (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate) # It is recommended that you use this optimizer for fine tuning, since this # is how the model was trained (note that the Adam m/v variables are NOT # loaded from init_checkpoint.) optimizer = AdamWeightDecayOptimizer( learning_rate=learning_rate, weight_decay_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-6, exclude_from_weight_decay=[&quot;LayerNorm&quot;, &quot;layer_norm&quot;, &quot;bias&quot;]) why warmup is required for BERT(MLM, from scratch)? LR을 조절해주는걸 LR scheduling이라 부르고, 이러한 행위는 DL model 성능 개선에 도움을 주는 것으로 알려졌습니다. 관련논문: A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay Another advantage of a high learning rate near the beginning (after warmup, which is another issue) is that it has a regularisation effect as it ends up in a relatively “flat” part of parameter space (ie: the hessian of the loss is relatively small). The idea of “super-convergence” tries to utilise this (paper, blog). 블로그 내용이 꽤 유익하니 참고하셔도 좋을 것 같습니다, linear, cycle등 scheduler에 대해 다룹니다. Weight decay &amp; AdamW 참고: https://hiddenbeginner.github.io/deeplearning/paperreview/2019/12/29/paper_review_AdamW.htmlweight decay는 gradient를 업데이트할때 이전 weight의 크기를 일정부분 감소시켜줘서 업데이트를 원활하게 해주는 트릭입니다. 0~1 사이로 셋팅하며 PLM에서는 보통 0.01 정도로 셋팅합니다.","link":"/Learning-rate-warmup-scheduling/"},{"title":"A Contrastive Framework for Neural Text Generation (NeurIPS 2022)","text":"발표자료 논문: A Contrastive Framework for Neural Text Generation.pdf 발표자료: A Contrastive Framework for Neural Text Generation.pdf 느낀점 잘 쓴 논문 같다 간단한 아이디어지만 효과적 decoding 시간이 생각보다 덜 걸려서 신기했음 contrastive 방법론이 결국 비슷한 토큰 안나오게 하겠다인데, simCTG는 MLE 로 보완되지만 디코딩 부분은 아예 비슷한걸 견제하는 식으로 나오는데도 결과가 좋게 나오는게 신기 (물론 기존의 확률아 있어서 보완이 되지만) -&gt; degeneration penalty를 크게 줘도 ppl 결과가 좋길래 신기했음) NoteAuthorYixuan Su* Tian Lan** Yan Wang** Dani Yogatama+ Lingpeng Kong++ Nigel Collier**Language Technology Lab, University of Cambridge**Tencent AI Lab +DeepMind++Department of Computer Science, The University of Hong Kong Abstract 문제: maximization-based decoding methods (e.g., beam search) of neural language models often lead to degenerate solutions the generated text is unnatural and contains undesirable repetitions 시중에 나온 대안: Existing approaches introduce stochasticity via sampling or modify training objectives to decrease the probabilities of certain tokens (e.g., unlikelihood training) 대안의 문제: However, they often lead to solutions that lack coherence 본 논문에서 보인 것: an underlying reason for model degeneration is the anisotropic distribution of token representations. present a contrastive solution: (i) SimCTG, a contrastive training objective to calibrate the model’s representation space, anisotropic 해소하겠다 (ii) a decoding method—contrastive search—to encourage diversity while maintaining coherence in the generated text. 다양하게 뽑지만 coherence 유지해보겠다 SOTA를 이기는 결과 보여줬음 Introduction the conventional approach of training a language model with maximum likelihood estimation (MLE) and decoding the most likely sequence is often not sufficient 평범한 접근방법인 MLE 기반의 학습과 디코딩은 대체로 충분하지 않음 degeneration 결과를 보여주기도함 tend to be dull and contain undesirable repetitions at different levels (e.g., token-, phrase-, and sentence-level) 해결방법중 하나는 less likely vocab에서 샘플링하는 디코딩방법을 사용하는 것 (To alleviate this problem, previous solutions modify the decoding strategy by sampling from less likely vocabularies) 하지만 이런 방법은 의미적으로 안맞거나 반대되기도하는등 부작용이 있음 Another approach addresses the degeneration problem by modifying the model’s output vocabulary distribution with unlikelihood training (unlikelihood training도 비슷한 맥락) 이러한 이유는 token representation distribution의 비대칭 때문이라고 주장해보겠음 (the degeneration of neural language models stems from the anisotropic distribution of token representations, i.e., their representations reside in a narrow subset of the entire space [10, 9, 44].) Figure 1은 GPT-2의 token representation에 대한 cosine sim matrix임 대부분이 0,95 이상인걸 볼수 있음 In an ideal setting, the token representations should follow an isotropic distribution, i.e., the token similarity matrix should be sparse and the representations of distinct tokens should be discriminative 본 논문에서 제안하는 모델 SimCTG (a simple contrastive framework for neural text generation) that encourages the model to learn discriminative and isotropic token representations. The Key intuition (i) at each decoding step, the output should be selected from the set of most probable candidates predicted by the model to better maintain the semantic coherence between the generated text and the human-written prefix (ii) the sparseness of the token similarity matrix of the generated text should be preserved to avoid degeneration. PPL이나 휴먼 평가등에서도 개선된 결과를 보여줌 the experimental results verify that SimCTG improves the intrinsic qualities of the language model, as evaluated by perplexity and token prediction accuracy (§4.2 and Appendix D). Moreover, we demonstrate that the proposed contrastive search significantly outperforms previous state-of-the-art decoding methods in both human and automatic evaluations Background MLE로 학습하는 LM은 transformer-based model 구조에서 모델의 표현이 anisotropic distribution을 갖게 됨 Deterministic Sampling은 greedy, beam이고 highest probability에 의존해서 degeneration을 야기함 Stochastic Sampling은 top-k, nucleus sampling류임, 가끔 의미적으로 반대되는 단어까지 생성하기도 하는 부작용 있음 Methodology how to apply contrastive learning to calibrate the representation space of the language model introduce our proposed contrastive search decoding algorithm Contrastive Training Our goal is to encourage the language model to learn discriminative and isotropic token representations cosine sim으로 유사한 토큰들은 더 큰 loss를 받는 구조 -&gt; 붙어있는 토큰을 더 멀리 떨어뜨리게 하는 효과 Q) 벡터적으로 유사한 토큰표현을 떨어뜨리는 효과가 서로 구분할 수 있는 효과를 주지만, 학습은 잘 되게하는걸까? 이 부분은 MLE가 잘해야하는 구조인듯 ρ 값이 0 이면 적용 안하는거나 마찬가지 -&gt; MLE만 쓰는 구조 Contrastive Search 모델이 예측한 셋 안에서 확률 높은 후보들이되, 이전 문맥과 구분이 될 수 있어야함 토큰 생성에 대한 확률값에 해당 토큰의 hidden states와 이전 토큰들의 hidden states의 유사도중 max값을 뽑아서 penalty term으로 줌 token들이 많으면 이거 계산시간 오래 걸리지 않을까? Document Generation Open-ended document generation에 적용 Our proposed approach is architecture-agnostic GPT2 (117M)에 Loss_SimCTG 적용해서 파인튜닝하고, contrastive search 이용해서 decoding 해봤음 baseline은 GPT2를 evaluated benchmark에 대해서 finetuning하되 아래 방법으로 함 [1] MLE GPT2 [2] unlikelihood GPT2 Evaluation Benchmark는 Wikitext-103 데이터 Training은 SimCTG, MLE는 Wikitext-103 (40k training steps)데이터에 대해 파인튜닝했고 UL baseline에 대해서는 38.5K steps를 token-level, 1.5K steps를 sentence-level로 UL 학습함 bs: 128, max_seq_len: 256, optim: adam, lr: 2e-5 Decoding은 prefix를 32~128 length정도 되는 정보를 주고 시작함 deterministic method: greedy, beam (10) search stochastic method: p=0.95 proposed contrastive search: k and α in Eq. (5) are set as 8 (top_k 8개 보고) and 0.6. (degeneration penalty에 점수를 좀 더 줬음) Evaluation Metrics평가 기준은 아래의 관점으로 정함 (1) language modelling quality Perplexity on the test set of Wikitext-103. Prediction Accuracy (토큰맞추기) Prediction Repetition (next token의 top-1 예측이 prefix(이전입력)에 있으면 카운팅됨), 낮은게 좋음 (2) generation quality Generation Repetition (sentence-level에서 n-grams의 반복을 카운팅) rep-n = 100 × (1.0 − ( |unique n-grams(xˆ )| / |total n-grams(x^)| )) Diversity (n-gram levels에서 repetition을 계산함) MAUVE (생성한거랑 human-written text와 token distribution closeness를 계산함) Semantic Coherence (simCSE로 prefix와 generated text의 representation을 구해서 coherence score를 계산함) Perplexity of Generated Text Human Evaluation 평가 기준 (5점 척도) Coherence: Whether the generated text is semantically consistent with the prefix. Fluency: Whether the generated text is fluent and easy to understand. Informativeness: Whether the generated text is diverse and contains interesting content. 평가 데이터 randomly select 200 prefixes with length of 32 from the test set of Wikitext-103 큰 모델에서 가장 좋은 점수를 얻었기 때문에 GPT3등에 대해서도 future work 해볼 예정 Open-domain Dialogue Generation Benchmark and Baselines 영어랑 중국어 진행함 영어: DailyDialog 중국어: LCCC Further Analysis Token Representation Self-similarity Self-similarity? 토큰끼리 sim의 평균 Figure2. 보면 중간 레이어는 self-similarity가 비슷함 output layer에서는 차이가 확 나기 시작함 The Effect of Contrastive Loss Margin contrastive loss margin ρ (Eq. (2))에 대해서 분석해보면 perplexity on the Wikitext-103 test set기준에서는 0.5값이 가장 적당한 마진임을 알 수 있음 Contrastive Search versus Nucleus Sampling 두가지 관점에서 분석함 (1) generation diversity (2) perplexity of the generated text (gen-ppl) Decoding Latency Comparison 생각보다 latency차이가 많이 안남 (신기) 아래 그림은 simCTG 모델 기준임 Case Study 실제 예제들 Comparison of Token Similarity Matrix 각 기법별 token similarity matrix보면 제안 기법의 align이 잘 되어있는걸 볼 수 있음 Conclusion Neural LM의 degeneation의 문제는 token representation의 anisotropic distribution 문제임을 보임 SimCTG 제안함, isotropic, discriminative representation space 만들어줌 contrastive search이라는 디코딩 방식도 제안함 automatic and human evaluations에서 모두 가장 좋은 점수 얻고 SOTA보다 높은 점수 기록함 AppendixUsage installation 1pip install simctg --upgrade. example 1234567891011121314151617import torch# load the language modelfrom simctg.simctggpt import SimCTGGPT model_name = r’cambridgeltl/simctg_wikitext103’ model = SimCTGGPT(model_name)model.eval()tokenizer = model.tokenizer# prepare inputprefix_text = # The prefix text in Table 4print (’Prefix is: {}’.format(prefix_text))tokens = tokenizer.tokenize(prefix_text)input_ids = tokenizer.convert_tokens_to_ids(tokens) input_ids = torch.LongTensor(input_ids).view(1,-1)# generate result with contrastive searchbeam_width, alpha, decoding_len = 8, 0.6, 128output = model.fast_contrastive_search(input_ids=input_ids, beam_width=beam_width, alpha=alpha,decoding_len=decoding_len) print(&quot;Output:\\n&quot; + 100 * ’-’)print(tokenizer.decode(output)) Gen-ppl Results Measured by Different Models 다른 모델들에 대한 결과를 보면 ppl 자체는 낮은 모델들도 있지만 human-written text와 가장 유사한건 역시 제안하는 모델 ppl이 낮은것 보다 사람이랑 유사한게 제일 좋은 것이다라고 주장 (이런 주장들은 기존에도 쭉 있었고 여기서도 같은 주장제기)","link":"/A-Contrastive-Framework-for-Neural-Text-Generation-NeurIPS-2022/"},{"title":"SOCIAL CHEMISTRY 101 - Learning to Reason about Social and Moral Norms","text":"Note web site: https://maxwellforbes.com/social-chemistry/ paper: SOCIAL CHEMISTRY 101- Learning to Reason about Social and Moral Norms.pdf 데이터셋 수집 순서 상황 수집 필요 (여기선 레딧, 고민상담, 찬반좌등이 있을법한 상황설정된곳에서 가져옴) → 상황마다 RoT를 1~5개 생성 → RoT마다 multiple annotation 작성 (같은 RoT를 다르게 볼 수 있으니..) → RoT Breakdown, Action Breakdown (Action도 해야되나.. RoT안에 포함된거아닌가.. attribute 자체는 다르긴한데? 정답은! RoT는 도덕적인 관점에서 따지고, Action은 법이나 문화적인 관점에서 따짐) → 추가적인 태깅도 진행 불분명한 상황이거나, 19금상황이거나, 너무불편한주제(학대나,…); 이런 경우는 스킵가능 RoT가이드라인등은 비슷하게 가져가면 될 것으로 보임 137명이 태깅함 split our dataset into 80/10/10% train/dev/test partitions by situation 데이터 예시 area m split rot-agree rot-categorization rot-moral-foundations rot-char-targeting rot-bad rot-judgment action action-agency action-moral-judgment action-agree action-legal action-pressure action-char-involved action-hypothetical situation situation-short-id rot rot-id rot-worker-id breakdown-worker-id n-characters characters amitheasshole 1 train 4 advice loyalty-betrayal char-1 0 it’s bad doing something that causes other people to lose trust in you. agency -1 3 legal -2 char-1 hypothetical losing trust in my friend reddit/amitheasshole/aypvmz It’s bad to do something that causes other people to lose trust in you. rot/reddit/amitheasshole/aypvmz/3K5TEWLKGYQFYAIY0H6JQMIY5MEIVM/127/2 127 0 2 narrator|my friend amitheasshole 1 dev 3 social-norms loyalty-betrayal char-0 0 expected people participating in the big events in their friends’ lives if asked. agency 0 3 legal 2 char-0 explicit-no saying no to being a bridesmaid at a friend’s wedding reddit/amitheasshole/9tzn0z People are expected to participate in the big events in their friends’ lives if asked. rot/reddit/amitheasshole/9tzn0z/3EG49X351XRR9FSSYVYCH4PEC656XX/89/1 89 39 3 narrator|a bridesmaid|a friend amitheasshole 1 test 3 social-norms care-harm|loyalty-betrayal char-1 0 Partners should Listening to each other’s issues. agency 2 3 legal 2 char-1 probable telling my boyfriend I am bored and unhappy at my job reddit/amitheasshole/a1311q Partners should listen to each other’s issues. rot/reddit/amitheasshole/a1311q/3JV9LGBJWWT6CZ369HK2AIBAUGUGOV/111/2 111 145 2 narrator|my boyfriend AuthorMaxwell Forbes†‡ Jena D. Hwang‡ Vered Shwartz†‡ Maarten Sap† Yejin Choi†‡†Paul G. Allen School of Computer Science &amp; Engineering, University of Washington ‡Allen Institute for AI Abstract introduce SOCIAL-CHEM- 101, a large-scale corpus that catalogs 292k rules-of-thumb such as “It is rude to run a blender at 5am” as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people’s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. NEURAL NORM TRANSFORMER, learns and generalizes SOCIAL-CHEM-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb Introduction 맞닥뜨리게 되는 예시 “wanting to call the cops on my neighbors,” (내 이웃을 경찰에 신고하고 싶다) 여러 이유들이 있음 (legality, cultural pressure, …, morality) “reporting a crime” and “being friends with your neighbor” are conflicting norms (범죄 신고 vs 이웃의 친구가 되라가 상충됨) Figure 1 가운데 hexagon 내용이 상황 1차로 작은 hexagon에 들어 있는 내용은 category 그 안에 Tube에 들어있는 내용이 RoT Situtation &amp; RoTs RoTs we organize descriptive norms via free-text rules-of-thumb (RoTs) as the basic conceptual units. RoT는 Judgement와 action으로 구성됨 RoT는 12개의 차원으로 구성됨 such as social judgments of good and bad, theoretical categories of moral foundations expected cultural pressure, and assumed legality SOCIAL CHEM-101, a new type of NLP resource that catalogs 292k RoTs over 104k real life situations, along with 365k sets of structural annotations, which break each RoT into 12 dimensions of norm attributes. Together, this amounts to over 4.5M categorical and free-text annotations. 30만개의 RoT, 10만개의 situation, 36만개의 annotation (12차원의 RoT를 갖는) 합치면 450만개의 categorical &amp; free-text annotations.. -&gt; 실제 데이터는 33만개정도던데.. 어디서 이렇게 차이가 나지? Even so, this breadth of this task proves challenging to current neural models, with humans rating model’s adherence to different attributes from 0.28 to 0.91 micro-F1. Approach social norms are culturally-sensitive standards Social norms preserving biological needs to survival (e.g., refraining from harming or killing) maintaining social civility and order (e.g., maintain- ing politeness, recognizing personal space) providing identity and belonging to a community (e.g., respecting the elderly) RoTs Our aim is then to forefront these implicit expectations about social norms via RoTs formalize the definition of RoTs as situationally relevant evaluative judgments of social norm 예시 12Punching someone.RoT: It is unacceptable to injure a person. More complex situations can be associated with multiple RoTs 예시 RoTs about stealing (RoT 1) vs. punching (RoT 2) RoTs targeting the different characters in the situation (RoTs 1, 4 target the narrator; RoTs 2, 3 target narrator’s friend) additional social interpretation implicit in the situation (RoT 3: theft from a friend is cast as an act of betrayal) 12345Punching a friend who stole from me.RoT 1: It is unacceptable to injure a person.RoT 2: People should not steal from others. RoT 3: It is bad to betray a friend.RoT 4: It is OK to want to take revenge. 추가 예제 SOCIAL-CHEM-101 Dataset obtained 104k source situations from 4 text domains (§3.1), for which we elicited 292k RoTs from crowd workers (§3.2) define a structured annotation task where workers isolate the central action described by the RoT and provide a series of judgments about the RoT and the action (§3.3). In total, we collect 365k structured annotations, performing multiple annotations per RoT for a subset of the RoTs to study the variance in annotations. 최종적으로 36만개정도 모은듯, RoT마다 multiple annotation하면서! Situations 레딧과 기존 연구에서 상황에 대한 문장들 10만개 모았음 gather a total of 104k real life situations from four domains scraped titles of posts in the subreddits r/confessions (32k) r/amitheasshole (r/AITA, 30k) 30k sentences from the ROCStories corpus (rocstories, Mostafazadeh et al., 2016) scraped titles from the Dear Abby advice column archives3 (dearabby, 12k) Rules-of-Thumb (RoTs) 주어진 상황에 대해서 1~5개 정도의 RoTs 작성하게 함 To collect RoTs, we provide workers with a situation as a prompt and them to write 1 – 5 RoTs inspired by that situation 10만개 상황에서 30만개 RoTs 추출 From the 104k situations, we elicit a total of 292k RoTs. RoTs는 보통 10단어정도 됨 Despite RoTs averaging just 10 words, we observe that 260k/292k RoTs are unique across the dataset RoTs 만들기 위해서 workers에게 basics of social norms에 대해 설명 instruct the workers to produce RoTs that explain the basics of social norms 1234561. inspired by the situation, to maintain a lower bound on relevance; ㄴ 상황에 근거해야함2. self-contained, to be understandable without additional explanation; and ㄴ 추가적인 설명이 없이도 이해가능해야함3. structured as judgment of acceptability (e.g., good/bad, (un)acceptable, okay) and an action that is assessed. ㄴ 좋다 나쁘다등의 판단과 액션으로 이루어져야함 RoT의 diversity를 위해서는 Vagueness와 Specificity 사이에서 밸런스를 맞춰야함 Vagueness: “It is rude be selfish.” Specificity: “It is rude not to share your mac’n’cheese with your younger brother.” 구별 가능한 아이디어로 작성할 것을 권고, 단순한 도치나 변형은 피할 것 also ask workers to write RoTs illustrating distinct ideas and avoid trivial inversions to prevent low-information RoTs that rephrase the same idea or simply invert the judgement and action. Character Identification ask workers to identify phrases in each situation that refer to people ex) “My brother chased after __the Uber driver__” 밑줄친 캐릭터에 대해서 셋팅하고 RoTs 모았다는 것 같은데, Narrator도 디폴트로 셋팅하고 workers mark the underlined spans. We collect three workers’ spans, calling each span a character. All characters identified become candidates for grounding RoTs and actions in the structured annotation. As such, we optimize for recall instead of precision by using the largest set of characters identified by any worker. We also include a narrator character by default. RoT Breakdowns 구조적인 어노테이션을 breakdown이라는 용어로 사용하고자함 We perform a structured annotation, which we term a breakdown In an RoT breakdown, a worker isolates the underlying action contained in the RoT central annotation goals The first goal is to tightly ground RoTs to their respective situations. The second goal is to partition social expectations using theoretically motivated categories. 아래 그림을 보면 ROT에 대한 Attribute가 다르고 Action에 대한 Attribute가 다름 (데이터셋 구축 난이도 증가할듯..) Grounding Attributes We call three attributes grounding attributes to ground the RoT and action to the situation and characters At the RoT-level, workers mark which character should heed the RoT with the RoT Targeting attribute. At the action level, workers first pick the action’s best candidate character, for whom the action is most relevant Social Attributes social expectations in an RoT. The first two social attributes both label anticipated agreement For an RoT, this attribute asks how many people probably agree with the RoT as stated. At the action level, it asks what portion of people probably agree with the judgment given the action. RoT는 도덕적인 관점에서 따지고, Action은 법이나 문화적인 관점에서 따짐 (어렵네..) An RoT-level attribute is the set of Moral Foundations, based on a well-known social psychology theory that outlines culturally innate moral reasoning (Haidt, 2012). The action-level attributes legality and cultural pressure are designed to reflect the two coarse-grained categories proposed by the Social Norms Theory (Kitts and Chiang, 2008; Perkins and Berkowitz, 1986) Finally, the social judgment aims to capture subjective moral judgment. A base judgment of what is good or bad is thought to intrinsically motivate social norms The RoT Category attribute estimate distinctions between morality, social norms, and other kinds of advice; general world knowledge (e.g., “It is good to eat when you are hungry”) The attribute agency is designed to let workers distinguish RoTs that involve agentive action from those that indicate an an experience (e.g., “It is sad to lose a family member” -&gt; 경험적인 행동). 경험적 행동 인지 구별 Analysis three key aspects of our formalism: social judgment, anticipated agreement, and cultural pressure Figure5는 RoTs 기반으로 위 3가지 attribute를 분석한 것 도덕적 판단 vs 문화적 (Moral Judgement를 social judgement X agreement로 만든건가..?) &quot;serving customers after close&quot; (Moral Judgement: 최고등급(8) / Cultural Pressure: Discretionary) 도덕적으로는 매우 훌륭한 판단이지만 문화적으로는 호불호 갈림 사회적 판단 vs 합의 &quot;giving ultimatums&quot;(최후통첩 주기) (Social Judgement: bad / Agreement: Controversial(50%)) 사회적인 판단에선 그렇게 좋은 판단은 아니지만, 의견이 분분함 In the left plot (Figure 5 (a)), the x-axis contains a new quantity, where social judgment (∈ [−2, 2]) is multiplied by agreement (∈ [0, 4]) to scale it. x values range from universally-agreed bad actions (-8) to universally-agreed good actions (+8). Model investigate neural models based on pre-trained language models for learning various sub-tasks de- rived from SOCIAL-CHEM-101 Training Objectives RoT가 있어야 Action도 있는거기 때문에 (RoT가 Judgement와 Action으로 이루어져있긴 하지만) 상황이 주어지면 -&gt; RoT에 대한 attribute를 생성하고 -&gt; action에 대한 attribute를 생성함 하지만 이 연구에서는 상황이 주어졌을때 action을 생성하는 것에 대해서 집중 In this paper, we instead focus our study of actions on a more difficult distribution that conditions only on the situation: original training obj focus on action 여러가지 실험 셋팅을 시도해봄 Table 1 shows the setups that we consider, and Figure 6 illustrates an example objective. combine and shuffle all objectives’ views of the data. Table1 Figure 6 Architectures present results for the GPT and GPT-2 architectures (Radford et al., 2018, 2019), as well as two encoder-decoder language models (BART and T5, Lewis et al., 2019; Raffel et al., 2019). term these architectures trained on our objectives the NEURAL NORM TRANSFORMER. Experiments and ResultsTasks pick two particular objectives to asses the models The first is p(y, b_y | s) — “model choice.” each model is allowed to pick the most likely attributes b_y, given a situation s generate an RoT (or action) y that adheres to those attributes setup should be easier because a model is allowed to pick the conditions attribute 선택하고 ROT (or action) 생성하고 second setting is p(y|s, b_y) — “conditional.” provide models with a set of attributes b_y that y they must follow when generating an RoT (or action) y. more challenging setup, because models cannot simply condition on the set of attributes that they find most likely select sets of attributes b_y provided by the human annotators for the situation s to ensure models are not tasked with generating from impossible constraints 생성해본적 없거나 불가능해보이는 constraints로부터 생성해야되니 더 어렵다는 말 같다 Setup split our dataset into 80/10/10% train/dev/test partitions by situation For all models we use top-p decoding with p = 0.9 Baselines Random RoT baseline to verify the dataset diversity (selections should have low relevance to test situations) evaluation setup (RoTs and actions should still be internally consistent) -&gt; 이게 무슨 뜻일까 use a BERT-Score (Zhang et al.,2020) retrieval baseline that finds the most similar training situation If attributes b_y are provided, y the retriever picks the RoT (or action) from the retrieved situation with the most similar attributes attribute가 어떻게 생겼길래 similar한 situation 문장을 고를수가있지? Ablations -Small finetune GPT-2 Small with the same general architecture -No pretrain randomly initialize the model’s weights. ResultsHuman Evaluation Table 2 presents a human evaluation measuring how effective models are at generating RoTs and actions for both task settings Relevance score가 중요함 (whether RoTs actually apply to the provided situation) In both setups, T5’s generations rank as most tightly relevant to the situation. But in terms of correctly following attributes, GPT-2 is more consistent, especially in the controlled task setup (lower; top scores on 5/9 attributes). However, no model is able to achieve a high score on all columns in the bottom half of the table Automatic Evaluation train attributes classifiers using RoBERTa use them to classify the model outputs Table 3 presents test set model performance on perplexity, BLEU (Papineni et al., 2002), and at- tribute micro-F1 classifier score The automatic metrics are consistent with human evaluation. T5 is a strong generator overall, achieving the high- est BLEU score and the highest relevance score in §5.2. However, GPT-2 more consistently adheres to attributes, outperforming T5 in attribute F1 automatic eval도 human eval이랑 결과가 비슷하다! Morality &amp; Political Bias Table 4 shows the correlations between RoT attributes and the political leaning and reliability of sources Conclusion present SOCIAL-CHEM-101, an attempt at providing a formalism and resource around the study of grounded social, moral, and ethical norms. preliminary success in generative modeling of structured RoTs, and corroborate findings of moral leaning in an extrinsic task AppendixAdditional Dataset DetailsSituationsDomains provide here a more thorough description how we collected situations from the four domains we consider. Figure 7 gives more example situations from each domain. r/amitheasshole(30k) 도덕적인 질문들 많이 올라오는 곳 r/confessions (32k) 고백이나 고민상담하는 곳 ROC Stories corpus 30K 2명의 캐릭터가 나오는 문장들의 subset 사용 (2명의 캐릭터는 pos tagging으로 골라냄) dearabby (12K) 미국의 조언 칼럼 사용 Titles of the Dear Abby advice column 다 스크래핑하고 몇몇 문장만 휴리스틱으로 필터링함 Additional Labels allow annotators to mark each situation with any of the following labels that apply. (상황에따라 비고란에 추가 태깅) Unclear: The situation was too simple, vague, or confusing to understand what happened. (불분명한 것) NSFW: The situation contains suggestive or adult content. (19금상황) Dark / disturbing / controversial: The situation contained content that may make folks uncomfortable, like suicide, torture, or abuse. (불편한 상황들, 학대나 등등..) 어노테이터는 저런 케이스는 넘어가도 되거나 RoT 계속 작성하거나 선택가능 Annotators may pass on writing RoTs for a situation marked with any of those boxes, or they may still choose to do so We keep all of the labels collected. They are included in the dataset as additional fields. they could be used to omit certain training data to keep a model biased away from potentially controversial subjects Character Identification to find the most descriptive phrase referring to each unique non-narrator person in the passage exactly once always having a single, best reference to each person in the situation enables more consistent grounding. provide here the character identification guidelines that we give to the crowd worker annotators Character Identification Guidelines guide_1 guide_2 Rules-of-Thumb (RoTs) Figure 8 shows a sample of RoTs organized both by situation domain and topic. RoT Writing Guidelines 백화사전(단순 정보성)이랑은 구분될만한, 매일 겪는 사회적인 통념등이 있어야함 단순 문장(보트는 비싸)이 아니라, 판단과 액션 두개로 나뉘어져야함 외부 정보가 필요한 내용은 안됨 (글에 없는 내용, 사건을 만들어내는) 모호함과 구체성 사이에 균형 필요 같은말을 패러프레이징해서 반복하는 식의 문구는 지양 RoT_1 RoT_2 RoT Breakdowns RoT Categorization Moral Foundations Action and Judgment Agency Social Judgment Anticipated Agreement Legality Cultural Pressure Taking Action RoT Categorization to distinguish more desired annotation topics (morality/ethics, social norms) from less desired ones (advice and “it is what it is” statements) RoT categories are not mutually-exclusive (exclusive한건 아니다..?!) and the lines are not always clear use all data regardless of RoT category in this paper’s experiments(실험에서는 카테고리 상관없이 사용했다고..), future work using this dataset may consider filtering based on RoT category Moral FoundationsAction and Judgment- Agency challenging to distinguish agency from experience in cases where the action involves think- ing thoughts or feeling emotions (생각이냐 경험이냐) Social Judgment transcribe the intent of RoT’s original judgment, rather than pick their own workers can mark their disagreement through their annotation of the anticipated agreement attribute Anticipated AgreementLegalityCultural PressureTaking ActionCrowdsourcing Workers undergo an extensive vetting process before working on RoTs This includes a paid qualification (qual) with a quiz on each of the guidelines and a manual review of sample RoTs Workers then pass the qual move to a staging pool where they can work on a small number of situations Annotator Demographics With an extensive qualification process, 137 workers participated in our tasks. Of those, 55% were women and 45% men. 89% of workers identified as white, 7% as Black. 39% were in the 30-39 age range, 27% in the 21-29 and 19% in the 40-49 age ranges. A majority (53%) of workers were single, and 35% were married. 47% of workers considered themselves as middle class, and 41% working class. In terms of education level, 44% had a bachelor’s degree, 36% some college experience or an associates degree. Experimental details Generative Models We use the Transformers package (Wolf et al., 2019) to implement our models. We train all the models for a single epoch with a batch size of 64, with the random seed 42 Each input and output sequence is prefixed with a special token indicating its type (e.g. [attrs], [rot], [action]). define a special token for each attribute value (e.g. , , , ) initialize the special token embeddings with the embedding of their corresponding words, taking the average for multiword expressions","link":"/SOCIAL-CHEMISTRY-101/"}],"tags":[{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"web","slug":"web","link":"/tags/web/"},{"name":"drone","slug":"drone","link":"/tags/drone/"},{"name":"nlp, kb","slug":"nlp-kb","link":"/tags/nlp-kb/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"grpc","slug":"grpc","link":"/tags/grpc/"},{"name":"생각정리","slug":"생각정리","link":"/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"},{"name":"search","slug":"search","link":"/tags/search/"}],"categories":[{"name":"cslog","slug":"cslog","link":"/categories/cslog/"},{"name":"paper","slug":"paper","link":"/categories/paper/"},{"name":"photo","slug":"photo","link":"/categories/photo/"},{"name":"ML","slug":"ML","link":"/categories/ML/"}],"pages":[{"title":"","text":"About 안녕하세요, 하고 싶은 개발하면서 살고 싶은 머신러닝 개발자 윤주성입니다. 자연어처리와 머신러닝을 전공했고 검색과 챗봇쪽 일을 하다가 현재는 Large Scale Language Model (LLM) 관련 일을 하고 있습니다. 시간이 무한하게 주어진다면, 소설을 읽으면서 인생을 즐기면 좋겠다는 작은 꿈(?)이 있습니다. - 자기개발: 글을 읽고 정리하는 것 &amp; “연구/구현/발표” 하는 것 등을 좋아합니다 - 출구전략: ML &amp; NLP 교육에 관심이 있습니다 - 취미: 드론, 영상 편집, 보드게임, 기타 및 최근엔 커피 연구 하는 것에도 관심 갖게 되었습니다 - 인생관: 인생관은 은근 자주 바뀝니다. 최근엔 `Pilgrim`처럼 살고자 노력합니다 \"Better is one day in your court than a thousand elsewhere\" Email: xelloss705@gmail.com / eagle705@korea.ac.kr Research Domain Large Scale Language Model Sentiment Analysis Named Entity Recognition Document Analysis Presentation Slides: https://github.com/eagle705/Presentation Current Interest Large Scaling Language Model Search Modeling Active Learning Challenge &amp; Award 2nd Place @ Intent Classification of AI Starthon 2019 (NIPA, Naver) 3rd Place @ Query Classification of AI Starthon 2019 (NIPA, Naver) 4th Place @ Movie Rating Classification of AI Starthon 2019 (NIPA, Naver) 7th Place @ Named Entity Recognition of NLP Challenge (NAVER, Changwon univ) Qualcomm Innovation Award 2016 (Qualcomm Korea) Personal Projects BERT based NER Tagger BERT+CRF based Named Entity Recognition model for Korean GitHub Skills var ctx = document.getElementById(\"cs\"); var data = { labels: \"NLP, Research, Python, Web dev, Docker & Hadoop, ML\".split(\",\"), datasets: [{ label: \"Ability\", backgroundColor: \"rgba(179,181,198,0.2)\", borderColor: \"#3385FF\", pointBackgroundColor: \"#3385FF\", pointBorderColor: \"#fff\", pointHoverBackgroundColor: \"#3385FF\", pointHoverBorderColor: \"#3385FF\", data: [100, 85, 85, 60, 70, 90] }] }; var myRadarChart = new Chart(ctx, { type: 'radar', data: data, options: { scale: { responsive: true, ticks: {min: 0, max: 100}, lineArc: false, pointLabels: {fontSize: 14}, }, scaleFontSize: 0, legend: {display: false}, } }); Bio May 2022 - Current Machine Learning Engineer (LLM & Dialogue) @ SKTelecom Feb 2020 - May 2022 Machine Learning Engineer (UGC Search) @ NAVER Feb 2018 - Feb 2020 AI Research Engineer (Chatbot &amp; NLP) @ Com2uS July 2017 - August 2017 Research Intern @ NAVER Education 2016 - 2017 Korea Univ Graduate School (Computer Science) 2010 - 2015 Korea Univ (Industrial Engineering & Computer Science) 2007 - 2009 Ansan Dongsan Christian High school","link":"/about/index.html"}]}