<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>(Chinchilla) Training Compute-Optimal Large Language Models - Luke&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Luke&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Luke&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Note paper file: Training Compute-Optimal Large Language Models.pdf 최적 모델 크기와 데이터 크기, FLOPs를 알기위한 함수를 estimate했던 논문 데이터 스케일링도 모델스케일링만큼 중요하다!  Author Jordan Hoffmann★, Sebastian Borgeaud★, Arthur Mensc"><meta property="og:type" content="blog"><meta property="og:title" content="(Chinchilla) Training Compute-Optimal Large Language Models"><meta property="og:url" content="https://eagle705.github.io/Chinchilla-Training-Compute-Optimal-Large-Language-Models/"><meta property="og:site_name" content="Luke&#039;s Blog"><meta property="og:description" content="Note paper file: Training Compute-Optimal Large Language Models.pdf 최적 모델 크기와 데이터 크기, FLOPs를 알기위한 함수를 estimate했던 논문 데이터 스케일링도 모델스케일링만큼 중요하다!  Author Jordan Hoffmann★, Sebastian Borgeaud★, Arthur Mensc"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/209597324-638983ca-fe6f-4815-9f3d-54fc0b0f33de.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/209597971-6e27cea7-b6d7-494e-a10e-b12e64bf36d9.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/209598534-c9ee49ed-85bd-48b1-84bd-efa3251eecde.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/209598557-572408c5-2443-43d9-9478-93e95608d8f5.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211252937-53301335-5653-479b-8ec7-b2634a423418.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211255227-afe2390b-1748-4008-9f02-865da593e02f.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211256057-9aa5e6db-f848-4d6c-81ea-96d2f6168b84.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211256619-a737684d-46c7-4558-a0f3-df3bd9c8dd44.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211256954-9200a06b-3871-4d4e-b355-54a93b5ae493.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211256780-6734b4cc-6bd1-487f-a936-8a6916ecc7ee.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211257449-b9d0edf1-2cff-4612-8a90-23b9e0792056.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211257516-02468291-528d-4444-a4ff-92b817c18e25.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211257691-72b9ec61-9e26-4ca7-9b97-8515441fb48d.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211258708-2bca2577-eb55-4f56-bd6d-1435e2ebd9ea.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211258824-c70d15df-b81a-4f57-8b9b-c129c27874c8.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/211259037-c9da3e07-9c1c-4e9d-99a8-8e23f36866c9.png"><meta property="article:published_time" content="2023-01-09T07:34:18.000Z"><meta property="article:modified_time" content="2023-01-09T07:34:37.397Z"><meta property="article:author" content="Joosung Yoon"><meta property="article:tag" content="nlp"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://user-images.githubusercontent.com/7252598/209597324-638983ca-fe6f-4815-9f3d-54fc0b0f33de.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://eagle705.github.io/Chinchilla-Training-Compute-Optimal-Large-Language-Models/"},"headline":"(Chinchilla) Training Compute-Optimal Large Language Models","image":["https://user-images.githubusercontent.com/7252598/209597324-638983ca-fe6f-4815-9f3d-54fc0b0f33de.png","https://user-images.githubusercontent.com/7252598/209597971-6e27cea7-b6d7-494e-a10e-b12e64bf36d9.png","https://user-images.githubusercontent.com/7252598/209598534-c9ee49ed-85bd-48b1-84bd-efa3251eecde.png","https://user-images.githubusercontent.com/7252598/209598557-572408c5-2443-43d9-9478-93e95608d8f5.png","https://user-images.githubusercontent.com/7252598/211252937-53301335-5653-479b-8ec7-b2634a423418.png","https://user-images.githubusercontent.com/7252598/211255227-afe2390b-1748-4008-9f02-865da593e02f.png","https://user-images.githubusercontent.com/7252598/211256057-9aa5e6db-f848-4d6c-81ea-96d2f6168b84.png","https://user-images.githubusercontent.com/7252598/211256619-a737684d-46c7-4558-a0f3-df3bd9c8dd44.png","https://user-images.githubusercontent.com/7252598/211256954-9200a06b-3871-4d4e-b355-54a93b5ae493.png","https://user-images.githubusercontent.com/7252598/211256780-6734b4cc-6bd1-487f-a936-8a6916ecc7ee.png","https://user-images.githubusercontent.com/7252598/211257449-b9d0edf1-2cff-4612-8a90-23b9e0792056.png","https://user-images.githubusercontent.com/7252598/211257516-02468291-528d-4444-a4ff-92b817c18e25.png","https://user-images.githubusercontent.com/7252598/211257691-72b9ec61-9e26-4ca7-9b97-8515441fb48d.png","https://user-images.githubusercontent.com/7252598/211258708-2bca2577-eb55-4f56-bd6d-1435e2ebd9ea.png","https://user-images.githubusercontent.com/7252598/211258824-c70d15df-b81a-4f57-8b9b-c129c27874c8.png","https://user-images.githubusercontent.com/7252598/211259037-c9da3e07-9c1c-4e9d-99a8-8e23f36866c9.png"],"datePublished":"2023-01-09T07:34:18.000Z","dateModified":"2023-01-09T07:34:37.397Z","author":{"@type":"Person","name":"Joosung Yoon"},"publisher":{"@type":"Organization","name":"Luke's Blog","logo":{"@type":"ImageObject","url":"https://eagle705.github.io/img/eagle705-logo.png"}},"description":"Note paper file: Training Compute-Optimal Large Language Models.pdf 최적 모델 크기와 데이터 크기, FLOPs를 알기위한 함수를 estimate했던 논문 데이터 스케일링도 모델스케일링만큼 중요하다!  Author Jordan Hoffmann★, Sebastian Borgeaud★, Arthur Mensc"}</script><link rel="canonical" href="https://eagle705.github.io/Chinchilla-Training-Compute-Optimal-Large-Language-Models/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-110980734-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-110980734-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-2655870716902046" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-01-09T07:34:18.000Z" title="1/9/2023, 4:34:18 PM">2023-01-09</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2023-01-09T07:34:37.397Z" title="1/9/2023, 4:34:37 PM">2023-01-09</time>&nbsp;업데이트 됨</span><span class="level-item">11분안에 읽기 (약 1598 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">(Chinchilla) Training Compute-Optimal Large Language Models</h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>paper file: <a target="_blank" rel="noopener" href="https://github.com/eagle705/presentation/files/10371283/Training.Compute-Optimal.Large.Language.Models.pdf">Training Compute-Optimal Large Language Models.pdf</a></li>
<li>최적 모델 크기와 데이터 크기, FLOPs를 알기위한 함수를 estimate했던 논문</li>
<li>데이터 스케일링도 모델스케일링만큼 중요하다!</li>
</ul>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><ul>
<li>Jordan Hoffmann★, Sebastian Borgeaud★, Arthur Mensch★, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre★ (★Equal contributions)<ul>
<li>DeepMind</li>
</ul>
</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>investigate the optimal model size and number of tokens for training a transformer language model</li>
<li>By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, find that for compute-optimal training, <strong>the model size and the number of training tokens should be scaled equally</strong><ul>
<li>모델 사이즈와 학습 토큰의 스케일은 비례함</li>
</ul>
</li>
<li>for every <strong>doubling</strong> of model size the number of training tokens should also be <strong>doubled</strong></li>
<li>test this hypothesis by training a predicted compute-optimal model, <code>Chinchilla</code>, that uses the <code>same compute budget as Gopher</code> but with <code>70B parameters and 4× more more data</code></li>
<li>Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.<ul>
<li>친칠라가 다운스트림태스크에서 다 이겼다?</li>
</ul>
</li>
<li>Chinchilla reaches a state-of-the-art average accuracy of <code>67.5% on the MMLU benchmark</code>, greater than a 7% improvement over Gopher</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul>
<li>LLMs을 학습하면서 생기는 이슈들<ul>
<li>accurately estimating the best model hyperparameters for a given compute budget is critical<ul>
<li>Chinchilla는 이번에 알게된 연구 내용을 토대로 Gopher보다 모델 4배 줄이고 토큰 4배 늘렸다!<br><img src="https://user-images.githubusercontent.com/7252598/209597324-638983ca-fe6f-4815-9f3d-54fc0b0f33de.png" alt="image"></li>
</ul>
</li>
</ul>
</li>
<li>revisit the question: <em>Given a fixed FLOPs budget,1 how should one trade-off model size and the number of training tokens?</em></li>
<li>400개 이상의 모델에 대해서 여러 파라미터와 데이터 사이즈로 실험해서 FLOPs(N,D)&#x3D;C 제한 아래서 L(N, D)를 가장 낮추는 모델파라미터_N, 학습토큰_D에 대한 함수를 측정함<br><img src="https://user-images.githubusercontent.com/7252598/209597971-6e27cea7-b6d7-494e-a10e-b12e64bf36d9.png" alt="image"></li>
<li>we <strong>predict</strong> that for the compute budget used to train Gopher, an optimal model should be <strong>4 times smaller, while being training on 4 times more tokens</strong></li>
<li>verify this by training a more <code>compute-optimal 70B model</code>, called Chinchilla, on <strong>1.4 trillion tokens</strong></li>
</ul>
<table>
<thead>
<tr>
<th>Figure 1</th>
<th>Figure A3</th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://user-images.githubusercontent.com/7252598/209598534-c9ee49ed-85bd-48b1-84bd-efa3251eecde.png" alt="image"></td>
<td><img src="https://user-images.githubusercontent.com/7252598/209598557-572408c5-2443-43d9-9478-93e95608d8f5.png" alt="image"></td>
</tr>
</tbody></table>
<h1 id="Estimating-the-optimal-parameter-x2F-training-tokens-allocation"><a href="#Estimating-the-optimal-parameter-x2F-training-tokens-allocation" class="headerlink" title="Estimating the optimal parameter&#x2F;training tokens allocation"></a>Estimating the optimal parameter&#x2F;training tokens allocation</h1><ul>
<li>Research Question) <code>Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens?</code><ul>
<li>사실 난 이게 더 궁금하긴함</li>
</ul>
</li>
<li>모델 파라미터와 토큰은 동일하게 비율적으로 올라가야<ul>
<li>parameter count and number of training tokens should be increased equally with more compute3— with proportions reported in Table 2</li>
</ul>
</li>
</ul>
<h2 id="3-1-Approach-1-Fix-model-sizes-and-vary-number-of-training-tokens"><a href="#3-1-Approach-1-Fix-model-sizes-and-vary-number-of-training-tokens" class="headerlink" title="3.1. Approach 1: Fix model sizes and vary number of training tokens"></a>3.1. Approach 1: Fix model sizes and vary number of training tokens</h2><ul>
<li>시간을 파라미터로 쓰기 애매하니까 FLOPs로 처리 해버린걸까? 왜 굳이 FLOPs를 써야하는지 의문이다</li>
<li>모델 사이즈 범위내에서 픽스해놓고 (ranging from 70M to over 10B parameters), FLOPs(𝑁, 𝐷) &#x3D; 𝐶 측정</li>
<li>At 1500 logarithmically spaced FLOP values, we find which model size <code>achieves the lowest loss</code> of all models <code>along with the required number of training tokens</code></li>
<li>fit power laws to estimate the optimal model size and number of training tokens for any given amount of compute<br>(see the center and right panels of Figure 2)<ul>
<li>obtaining a relationship 𝑁𝑜𝑝𝑡 ∝ 𝐶^𝑎 and 𝐷𝑜𝑝𝑡 ∝ 𝐶^𝑏</li>
<li>We find that 𝑎 &#x3D; 0.50 and 𝑏 &#x3D; 0.50 —as summarized in Table 2.<br><img src="https://user-images.githubusercontent.com/7252598/211252937-53301335-5653-479b-8ec7-b2634a423418.png" alt="image"></li>
</ul>
</li>
</ul>
<h2 id="3-2-Approach-2-IsoFLOP-profiles"><a href="#3-2-Approach-2-IsoFLOP-profiles" class="headerlink" title="3.2. Approach 2: IsoFLOP profiles"></a>3.2. Approach 2: IsoFLOP profiles</h2><ul>
<li>vary the model size for a <code>fixed set of 9 different training FLOP counts</code> (ranging from 6 × 1018 to 3 × 1021 FLOPs), and consider the final training loss for each point</li>
<li>in contrast with Approach 1 that considered points (𝑁, 𝐷, 𝐿) along the entire training runs. This allows us to directly answer the question: For a given FLOP budget, what is the optimal parameter count?</li>
<li>토큰이 많을 수록 Loss가 낮아진다 (같은 FLOPs 일지라도!)</li>
<li>fit a parabola to each IsoFLOPs curve to directly estimate at what model size the minimum loss is achieved (Figure 3 (left))</li>
<li>fit a power law between FLOPs and loss-optimal model size and number of training tokens, shown in<br>Figure 3 (center, right). Again, we fit exponents of the form 𝑁𝑜𝑝𝑡 ∝ 𝐶^𝑎 and 𝐷𝑜𝑝𝑡 ∝ 𝐶^𝑏.  𝑎 &#x3D; 0.49 and 𝑏 &#x3D; 0.51—as summarized in Table 2.</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/211255227-afe2390b-1748-4008-9f02-865da593e02f.png" alt="image"></p>
<h2 id="3-3-Approach-3-Fitting-a-parametric-loss-function"><a href="#3-3-Approach-3-Fitting-a-parametric-loss-function" class="headerlink" title="3.3. Approach 3: Fitting a parametric loss function"></a>3.3. Approach 3: Fitting a parametric loss function</h2><ul>
<li>model all final losses from experiments in Approach 1 &amp; 2 as a parametric function of model parameter count and the number of seen tokens<br><img src="https://user-images.githubusercontent.com/7252598/211256057-9aa5e6db-f848-4d6c-81ea-96d2f6168b84.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/211256619-a737684d-46c7-4558-a0f3-df3bd9c8dd44.png" alt="image"></li>
</ul>
<h2 id="3-4-Optimal-model-scaling"><a href="#3-4-Optimal-model-scaling" class="headerlink" title="3.4. Optimal model scaling"></a>3.4. Optimal model scaling</h2><ul>
<li><p>기존 논문(Kaplan et al.(2020) 과는 달리 파라미터와 데이터가 거의 equal한 스케일링을 보임<br><img src="https://user-images.githubusercontent.com/7252598/211256954-9200a06b-3871-4d4e-b355-54a93b5ae493.png" alt="image"></p>
</li>
<li><p>사실 이 논문에서는 아래표가 제일 중요했다<br><img src="https://user-images.githubusercontent.com/7252598/211256780-6734b4cc-6bd1-487f-a936-8a6916ecc7ee.png" alt="image"></p>
</li>
</ul>
<h1 id="4-Chinchilla"><a href="#4-Chinchilla" class="headerlink" title="4. Chinchilla"></a>4. Chinchilla</h1><h2 id="4-1-Model-and-training-details"><a href="#4-1-Model-and-training-details" class="headerlink" title="4.1. Model and training details"></a>4.1. Model and training details</h2><ul>
<li>train Chinchilla on MassiveText (the same dataset as Gopher) but use a slightly different subset distribution (shown in Table A1) to account for the increased number of training tokens</li>
<li>AdamW (Loshchilov and Hutter, 2019) for Chinchilla</li>
<li>train Chinchilla with a slightly modified SentencePiece (Kudo and Richardson, 2018)<br>tokenizer that <code>does not apply NFKC normalisation</code> (이거 왜 안했지?)<ul>
<li>find that this particularly helps with the representation of <code>mathematics and chemistry</code> (MMLU 같은 곳에는 도움될수도?!)</li>
<li>vocabulary is very similar– 94.15% of tokens are the same as those used for training Gopher</li>
</ul>
</li>
<li>forward and backward pass are <code>computed in bfloat16</code>, we <code>store a float32</code> copy of the weights<br><img src="https://user-images.githubusercontent.com/7252598/211257449-b9d0edf1-2cff-4612-8a90-23b9e0792056.png" alt="image"></li>
</ul>
<h2 id="4-2-Results"><a href="#4-2-Results" class="headerlink" title="4.2. Results"></a>4.2. Results</h2><p><img src="https://user-images.githubusercontent.com/7252598/211257516-02468291-528d-4444-a4ff-92b817c18e25.png" alt="image"></p>
<h3 id="4-2-1-Lanugage-modeling"><a href="#4-2-1-Lanugage-modeling" class="headerlink" title="4.2.1. Lanugage modeling"></a>4.2.1. Lanugage modeling</h3><ul>
<li>bits-per-byte(bpb)가 뭐지<br><img src="https://user-images.githubusercontent.com/7252598/211257691-72b9ec61-9e26-4ca7-9b97-8515441fb48d.png" alt="image"></li>
</ul>
<h3 id="4-2-2-MMLU"><a href="#4-2-2-MMLU" class="headerlink" title="4.2.2. MMLU"></a>4.2.2. MMLU</h3><ul>
<li>이하 생략</li>
</ul>
<h1 id="Discussion-amp-Conclusion"><a href="#Discussion-amp-Conclusion" class="headerlink" title="Discussion &amp; Conclusion"></a>Discussion &amp; Conclusion</h1><ul>
<li>The trend so far in large language model training has been to increase the model size, often without increasing the number of training tokens<ul>
<li>모델크기만 키우고 토큰은 안키웠던 트렌드가 있었음, 근데 잘못됨</li>
</ul>
</li>
<li>propose three predictive approaches towards optimally setting model size and training dura- tion, based on the outcome of over 400 training runs<ul>
<li>실험 많이함</li>
</ul>
</li>
<li>Though there has been significant recent work allowing larger and larger models to be trained, our analysis suggests an increased focus on dataset scaling is needed<ul>
<li>데이터 스케일링도 필요하다고! (물론 퀄리티가 뒷받침되야함)</li>
</ul>
</li>
<li>Larger datasets will require extra care to ensure train-test set overlap is properly accounted for, both in the language modelling loss but also with downstream tasks<ul>
<li>LM loss와 downstream task 다 잘되게 하려면 데이터 양 신경쓰자</li>
</ul>
</li>
<li>Chinchilla does suffer from bias and toxicity but interestingly it seems less affected than Gopher<ul>
<li>친칠라도 bias와 toxicity 문제가 있었지만 고퍼보다 덜했다.</li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/211258708-2bca2577-eb55-4f56-bd6d-1435e2ebd9ea.png" alt="image"></p>
<h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h2 id="학습셋"><a href="#학습셋" class="headerlink" title="학습셋"></a>학습셋</h2><p><img src="https://user-images.githubusercontent.com/7252598/211258824-c70d15df-b81a-4f57-8b9b-c129c27874c8.png" alt="image"></p>
<h2 id="D-3-Predicted-compute-optimal-frontier-for-all-three-methods"><a href="#D-3-Predicted-compute-optimal-frontier-for-all-three-methods" class="headerlink" title="D.3. Predicted compute optimal frontier for all three methods"></a>D.3. Predicted compute optimal frontier for all three methods</h2><p><img src="https://user-images.githubusercontent.com/7252598/211259037-c9da3e07-9c1c-4e9d-99a8-8e23f36866c9.png" alt="image"></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>(Chinchilla) Training Compute-Optimal Large Language Models</p><p><a href="https://eagle705.github.io/Chinchilla-Training-Compute-Optimal-Large-Language-Models/">https://eagle705.github.io/Chinchilla-Training-Compute-Optimal-Large-Language-Models/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Joosung Yoon</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-01-09</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-01-09</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/nlp/">nlp</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=63297c6228f9450019a5f574&amp;product=sop" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/InstructGPT/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">(InstructGPT) Training language models to follow instructions with human feedback</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/Robust-Conversational-Agents-against-Imperceptible-Toxicity-Triggers/"><span class="level-item">Robust Conversational Agents against Imperceptible Toxicity Triggers</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://eagle705.github.io/Chinchilla-Training-Compute-Optimal-Large-Language-Models/';
            this.page.identifier = 'Chinchilla-Training-Compute-Optimal-Large-Language-Models/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eagle705-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/eagle705-logo.png" alt="Joosung Yoon"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joosung Yoon</p><p class="is-size-6 is-block">Machine Learning Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>There and Back Again</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">51</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/eagle705" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/eagle705"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hisdevelopers"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JSYoon53859120"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/joosung-yoon/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cslog/"><span class="level-start"><span class="level-item">cslog</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/categories/photo/"><span class="level-start"><span class="level-item">photo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">3월 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">2월 2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">1월 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">12월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">11월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">10월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">9월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">8월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">5월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">12월 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">12월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">11월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">10월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">9월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">8월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">7월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">5월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">4월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">11월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">6월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">5월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">4월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/drone/"><span class="tag">drone</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/grpc/"><span class="tag">grpc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">40</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp-kb/"><span class="tag">nlp, kb</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"><span class="tag">생각정리</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">광고</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-2655870716902046" data-ad-slot="2764253071" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Note"><span class="level-left"><span class="level-item">1</span><span class="level-item">Note</span></span></a></li><li><a class="level is-mobile" href="#Author"><span class="level-left"><span class="level-item">2</span><span class="level-item">Author</span></span></a></li><li><a class="level is-mobile" href="#Abstract"><span class="level-left"><span class="level-item">3</span><span class="level-item">Abstract</span></span></a></li><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">4</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Estimating-the-optimal-parameter-x2F-training-tokens-allocation"><span class="level-left"><span class="level-item">5</span><span class="level-item">Estimating the optimal parameter/training tokens allocation</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#3-1-Approach-1-Fix-model-sizes-and-vary-number-of-training-tokens"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">3.1. Approach 1: Fix model sizes and vary number of training tokens</span></span></a></li><li><a class="level is-mobile" href="#3-2-Approach-2-IsoFLOP-profiles"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">3.2. Approach 2: IsoFLOP profiles</span></span></a></li><li><a class="level is-mobile" href="#3-3-Approach-3-Fitting-a-parametric-loss-function"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">3.3. Approach 3: Fitting a parametric loss function</span></span></a></li><li><a class="level is-mobile" href="#3-4-Optimal-model-scaling"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">3.4. Optimal model scaling</span></span></a></li></ul></li><li><a class="level is-mobile" href="#4-Chinchilla"><span class="level-left"><span class="level-item">6</span><span class="level-item">4. Chinchilla</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#4-1-Model-and-training-details"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">4.1. Model and training details</span></span></a></li><li><a class="level is-mobile" href="#4-2-Results"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">4.2. Results</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#4-2-1-Lanugage-modeling"><span class="level-left"><span class="level-item">6.2.1</span><span class="level-item">4.2.1. Lanugage modeling</span></span></a></li><li><a class="level is-mobile" href="#4-2-2-MMLU"><span class="level-left"><span class="level-item">6.2.2</span><span class="level-item">4.2.2. MMLU</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Discussion-amp-Conclusion"><span class="level-left"><span class="level-item">7</span><span class="level-item">Discussion &amp; Conclusion</span></span></a></li><li><a class="level is-mobile" href="#Appendix"><span class="level-left"><span class="level-item">8</span><span class="level-item">Appendix</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#학습셋"><span class="level-left"><span class="level-item">8.1</span><span class="level-item">학습셋</span></span></a></li><li><a class="level is-mobile" href="#D-3-Predicted-compute-optimal-frontier-for-all-three-methods"><span class="level-left"><span class="level-item">8.2</span><span class="level-item">D.3. Predicted compute optimal frontier for all three methods</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-23T08:14:37.000Z">2023-03-23</time></p><p class="title"><a href="/Alpaca/">Alpaca (A Strong Instruction-Following Model)</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-20T13:07:50.000Z">2023-02-20</time></p><p class="title"><a href="/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/">SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기</a></p><p class="categories"><a href="/categories/ML/">ML</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-16T08:24:12.000Z">2023-02-16</time></p><p class="title"><a href="/Toolformer/">Toolformer: Language Models Can Teach Themselves to Use Tools</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-13T04:18:48.000Z">2023-02-13</time></p><p class="title"><a href="/SELF-INSTRUCT%20Aligning%20Language%20Model%20with%20Self%20Generated%20Instructions/">SELF-INSTRUCT Aligning Language Model with Self Generated Instructions</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-06T04:15:12.000Z">2023-02-06</time></p><p class="title"><a href="/(FLAN)%20Finetuned%20Language%20Models%20Are%20Zero-Shot%20Learners/">(FLAN) Finetuned Language Models Are Zero-Shot Learners</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 Joosung Yoon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>