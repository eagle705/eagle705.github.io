<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>(IA3) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning - Luke&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Luke&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Luke&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Note 아직은 Preprint. Under review!  github repo: https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;r-three&amp;#x2F;t-few IA3 논문 Infused Adapter by Inhibiting and Amplifying Inner Activations   LoRA 관련 논문 발표영상(추천) https:&amp;#x2F;&amp;#x2F;www.youtube.com&amp;#x2F;watc"><meta property="og:type" content="blog"><meta property="og:title" content="(IA3) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"><meta property="og:url" content="https://eagle705.github.io/ia3/"><meta property="og:site_name" content="Luke&#039;s Blog"><meta property="og:description" content="Note 아직은 Preprint. Under review!  github repo: https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;r-three&amp;#x2F;t-few IA3 논문 Infused Adapter by Inhibiting and Amplifying Inner Activations   LoRA 관련 논문 발표영상(추천) https:&amp;#x2F;&amp;#x2F;www.youtube.com&amp;#x2F;watc"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230258800-161be162-7ea6-458a-b19c-73108ecd7b0c.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230259439-fe58295d-9879-41c8-9454-0ecbe27cacde.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230269005-6e887dcd-d203-4c5f-89b0-a174f8a05514.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230269513-6f992622-fdb2-4fae-9066-05f0c0fe236e.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230270777-b74d969d-5602-441a-8660-64b486b93d5a.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230270792-f08f8866-d1bc-482e-a41c-aca398911c82.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230270801-f42bffe2-0565-4817-83a9-c02590c35f4a.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230270808-00f44e9a-d5bd-4420-8566-e0be0b7d1ad6.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/229989356-94d1f76b-a515-4285-8a6f-2ac75fb7617c.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/229991133-9284548d-b51f-48e0-96a8-fd8f23eb2bcb.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230011306-0e6184ac-af46-4920-b483-03e2481d0457.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230010560-64b5e1fe-1574-4532-9bb7-0b18e505cdcb.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230011771-6be13446-6fd8-4a6d-a558-219cdd975eee.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230024678-568dbeb2-8492-475c-b899-d4e727c1c475.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230026103-f3fec870-0d86-490c-836e-a0eae38a8df2.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230131726-78174f90-7cd2-46ec-8a43-d68a3f28994a.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230131757-304ff7c5-6d47-4520-9456-e25877509c58.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230250722-704fb5a9-798d-426e-ba71-05fef80ae7ca.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230132690-76f1edcf-c598-4753-8a12-b55cd38fd6d6.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230134081-556dc611-bf0f-46f6-bba4-ca3f835a388e.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230134768-3849c9a5-9ab6-44c8-8065-43c4a420dcbe.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230135414-cf65a5e9-dbe4-4f22-b84b-a58c8f964f84.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230136093-9b2adfe4-e6f4-4796-be7f-81312a1d2c13.png"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/230257608-7cf38e78-25f5-4ca2-b68d-ad0f75cad6f4.png"><meta property="article:published_time" content="2023-05-09T02:54:01.000Z"><meta property="article:modified_time" content="2023-05-09T02:56:40.200Z"><meta property="article:author" content="Joosung Yoon"><meta property="article:tag" content="nlp"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://user-images.githubusercontent.com/7252598/230258800-161be162-7ea6-458a-b19c-73108ecd7b0c.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://eagle705.github.io/ia3/"},"headline":"(IA3) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning","image":["https://user-images.githubusercontent.com/7252598/230258800-161be162-7ea6-458a-b19c-73108ecd7b0c.png","https://user-images.githubusercontent.com/7252598/230259439-fe58295d-9879-41c8-9454-0ecbe27cacde.png","https://user-images.githubusercontent.com/7252598/230269005-6e887dcd-d203-4c5f-89b0-a174f8a05514.png","https://user-images.githubusercontent.com/7252598/230269513-6f992622-fdb2-4fae-9066-05f0c0fe236e.png","https://user-images.githubusercontent.com/7252598/230270777-b74d969d-5602-441a-8660-64b486b93d5a.png","https://user-images.githubusercontent.com/7252598/230270792-f08f8866-d1bc-482e-a41c-aca398911c82.png","https://user-images.githubusercontent.com/7252598/230270801-f42bffe2-0565-4817-83a9-c02590c35f4a.png","https://user-images.githubusercontent.com/7252598/230270808-00f44e9a-d5bd-4420-8566-e0be0b7d1ad6.png","https://user-images.githubusercontent.com/7252598/229989356-94d1f76b-a515-4285-8a6f-2ac75fb7617c.png","https://user-images.githubusercontent.com/7252598/229991133-9284548d-b51f-48e0-96a8-fd8f23eb2bcb.png","https://user-images.githubusercontent.com/7252598/230011306-0e6184ac-af46-4920-b483-03e2481d0457.png","https://user-images.githubusercontent.com/7252598/230010560-64b5e1fe-1574-4532-9bb7-0b18e505cdcb.png","https://user-images.githubusercontent.com/7252598/230011771-6be13446-6fd8-4a6d-a558-219cdd975eee.png","https://user-images.githubusercontent.com/7252598/230024678-568dbeb2-8492-475c-b899-d4e727c1c475.png","https://user-images.githubusercontent.com/7252598/230026103-f3fec870-0d86-490c-836e-a0eae38a8df2.png","https://user-images.githubusercontent.com/7252598/230131726-78174f90-7cd2-46ec-8a43-d68a3f28994a.png","https://user-images.githubusercontent.com/7252598/230131757-304ff7c5-6d47-4520-9456-e25877509c58.png","https://user-images.githubusercontent.com/7252598/230250722-704fb5a9-798d-426e-ba71-05fef80ae7ca.png","https://user-images.githubusercontent.com/7252598/230132690-76f1edcf-c598-4753-8a12-b55cd38fd6d6.png","https://user-images.githubusercontent.com/7252598/230134081-556dc611-bf0f-46f6-bba4-ca3f835a388e.png","https://user-images.githubusercontent.com/7252598/230134768-3849c9a5-9ab6-44c8-8065-43c4a420dcbe.png","https://user-images.githubusercontent.com/7252598/230135414-cf65a5e9-dbe4-4f22-b84b-a58c8f964f84.png","https://user-images.githubusercontent.com/7252598/230136093-9b2adfe4-e6f4-4796-be7f-81312a1d2c13.png","https://user-images.githubusercontent.com/7252598/230257608-7cf38e78-25f5-4ca2-b68d-ad0f75cad6f4.png"],"datePublished":"2023-05-09T02:54:01.000Z","dateModified":"2023-05-09T02:56:40.200Z","author":{"@type":"Person","name":"Joosung Yoon"},"publisher":{"@type":"Organization","name":"Luke's Blog","logo":{"@type":"ImageObject","url":"https://eagle705.github.io/img/eagle705-logo.png"}},"description":"Note 아직은 Preprint. Under review!  github repo: https:&#x2F;&#x2F;github.com&#x2F;r-three&#x2F;t-few IA3 논문 Infused Adapter by Inhibiting and Amplifying Inner Activations   LoRA 관련 논문 발표영상(추천) https:&#x2F;&#x2F;www.youtube.com&#x2F;watc"}</script><link rel="canonical" href="https://eagle705.github.io/ia3/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-110980734-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-110980734-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-2655870716902046" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-05-09T02:54:01.000Z" title="5/9/2023, 11:54:01 AM">2023-05-09</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2023-05-09T02:56:40.200Z" title="5/9/2023, 11:56:40 AM">2023-05-09</time>&nbsp;업데이트 됨</span><span class="level-item">28분안에 읽기 (약 4183 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">(IA3) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</h1><div class="content"><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li>아직은 Preprint. Under review! </li>
<li>github repo: <a target="_blank" rel="noopener" href="https://github.com/r-three/t-few">https://github.com/r-three/t-few</a></li>
<li>IA3 논문<ul>
<li><code>Infused Adapter</code> by <code>Inhibiting and Amplifying</code> <code>Inner Activations</code></li>
</ul>
</li>
<li>LoRA 관련 논문 발표영상(추천)<ul>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=BJqwmDpa0wM">https://www.youtube.com/watch?v=BJqwmDpa0wM</a></li>
</ul>
</li>
</ul>
<h1 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h1><ul>
<li>Haokun Liu∗ Derek Tam∗ Mohammed Muqeeth∗ Jay Mohta Tenghao Huang Mohit Bansal Colin Raffel<ul>
<li>Department of Computer Science</li>
<li>University of North Carolina at Chapel Hill {haokunl,dtredsox,muqeeth,craffel}@cs.unc.edu</li>
</ul>
</li>
</ul>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><ul>
<li>LoRA(2021 from MS)가 요즘 핫하지만..?! IA3의 장점이 많다 (싸고, 성능이 좋다)</li>
</ul>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ul>
<li>Few-shot in-context learning (ICL)이 PLM을 튜닝하지 않고도 unseen task를 잘 할 수 있게 해줬지만 substantial computational, memory, and storage costs 유발<ul>
<li>because it involves processing all of the training examples every time a prediction is made.</li>
</ul>
</li>
<li><code>Parameter-efficient fine-tuning (PEFT)</code> (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an <code>alternative paradigm</code> where a small set of parameters are trained to enable a model to perform the new task.</li>
<li>we rigorously compare few-shot ICL and PEFT and demonstrate that the <code>latter offers better accuracy as well as dramatically lower computational costs</code></li>
<li>we introduce a new PEFT method called <code>(IA)3</code> that <code>scales activations by learned vectors</code>, attaining stronger performance while only introducing a relatively tiny amount of new parameters.</li>
<li>propose a simple recipe based on the T0 model [1] called <code>T-Few</code> that can be applied to new tasks without task-specific tuning or modifications.<ul>
<li>applying it to the RAFT benchmark [2], attaining super-human performance for the first time and outperforming the state-of-the-art by 6% absolute.</li>
</ul>
</li>
</ul>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ul>
<li>NLP 발전 관련 내용<ul>
<li>Pre-trained language models have become a <code>cornerstone</code> of natural language processing, thanks to the fact that they can dramatically improve data efficiency on tasks of interest</li>
<li>Notably, ICL requires no gradient-based training and therefore allows a single model to immediately perform a wide variety of tasks. Performing ICL therefore solely relies on the capabilities that a model learned during pre-training</li>
</ul>
</li>
<li>Despite the practical benefits of <code>ICL, it has several major drawbacks</code>. <ul>
<li>First, processing <code>all prompted input-target pairs every time</code> the model makes a prediction incurs <code>significant compute costs</code>. </li>
<li>Second, ICL typically produces <code>inferior performance compared to fine-tuning</code> [4]. </li>
<li>Finally, the exact <code>formatting of the prompt</code> (including the wording [11] and ordering of examples [12]) can have significant and unpredictable impact on the model’s performance, far beyond inter-run variation of fine-tuning.</li>
</ul>
</li>
<li>Recent work has also demonstrated that <code>ICL can perform well</code> <code>even when provided with incorrect labels</code>, raising questions as to how much learning is taking place at all</li>
<li>An <code>additional paradigm</code> for enabling a model to perform a new task with minimal updates is <code>parameter-efficient fine-tuning (PEFT)</code>, where a pre-trained model is fine-tuned by only updating a small number of added or selected parameters.<ul>
<li>Recent methods have <code>matched the performance of fine-tuning</code> the full model while <code>only updating or adding a small fraction (e.g. 0.01%)</code> of the full model’s parameters</li>
<li>Furthermore, certain PEFT methods allow mixed-task batches where different examples in a batch are processed differently [14], making both PEFT and ICL viable for multitask models<ul>
<li>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.</li>
</ul>
</li>
</ul>
</li>
<li>PEFT 문제제기<ul>
<li>While the benefits of PEFT address some shortcomings of fine-tuning (when compared to ICL), there has been relatively little focus on whether PEFT methods work well <code>when very little labeled data is available</code></li>
</ul>
</li>
<li>Our primary goal in this paper is to close this gap by proposing a recipe<ul>
<li>Specifically, we base our approach on the T0 model [1], a variant of T5 [15] fine-tuned on a multitask mixture of prompted datasets.</li>
<li>To improve performance on classification and multiple-choice tasks, we <code>add unlikelihood [16, 17]</code> and <code>length normalization-based [4] loss terms</code><ul>
<li>이건 적용 안해봤는데, 적용을 검토해봐야겠다!</li>
</ul>
</li>
<li>In addition, we develop <code>(IA)3, a PEFT method that multiplies intermediate activations by learned vectors</code>.<ul>
<li>(IA)3 attains stronger performance than full-model fine-tuning while updating up to 10,000× fewer parameters</li>
</ul>
</li>
<li>Finally, we demonstrate the benefits of pre-training the (IA)3 parameters before fine-tuning [18, 19].</li>
</ul>
</li>
<li>Our overall recipe, which we dub “T-Few”, performs significantly better than ICL (even against 16× larger models) and outperforms humans for the first time on the real-world few-shot learning benchmark RAFT [2] while requiring dramatically less compute and allowing for mixed-task batches during inference.</li>
</ul>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Parameter-efficient-fine-tuning"><a href="#Parameter-efficient-fine-tuning" class="headerlink" title="Parameter-efficient fine-tuning"></a>Parameter-efficient fine-tuning</h2><ul>
<li>Early methods proposed adding <code>adapters</code> [22–24], which are small trainable feed-forward networks <code>inserted between the layers in the fixed pre-trained model</code></li>
<li>methods that <code>choose a sparse subset of parameters to train</code> [25, 26]<ul>
<li>produce <code>low-rank updates</code> [13]</li>
<li>perform optimization in a lower-dimensional subspace [27]</li>
<li>add <code>low-rank adapters</code> using hypercomplex multiplication [28]</li>
<li>Relatedly, <code>prompt tuning</code> [14] and <code>prefix tuning</code> [29] concatenate learned continuous embeddings to the model’s input or activations to induce it to perform a task</li>
</ul>
</li>
</ul>
<h3 id="요즘-핫한-LoRA만-조금-더-살펴보자"><a href="#요즘-핫한-LoRA만-조금-더-살펴보자" class="headerlink" title="요즘 핫한 LoRA만 조금 더 살펴보자!"></a>요즘 핫한 LoRA만 조금 더 살펴보자!</h3><ul>
<li>LoRA(2021 from MS)가 요즘 핫함!<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a><br><img src="https://user-images.githubusercontent.com/7252598/230258800-161be162-7ea6-458a-b19c-73108ecd7b0c.png" alt="image"></li>
</ul>
</li>
<li>원리<br><img src="https://user-images.githubusercontent.com/7252598/230259439-fe58295d-9879-41c8-9454-0ecbe27cacde.png" alt="image"></li>
<li>pre- trained language models have a low “instrisic dimension” and can still learn efficiently despite a low-dimensional reparametrization.</li>
<li>We limit our study to <code>only changing the attention weights</code> for downstream tasks and freeze the MLP modules<br><img src="https://user-images.githubusercontent.com/7252598/230269005-6e887dcd-d203-4c5f-89b0-a174f8a05514.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/230269513-6f992622-fdb2-4fae-9066-05f0c0fe236e.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/230270777-b74d969d-5602-441a-8660-64b486b93d5a.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/230270792-f08f8866-d1bc-482e-a41c-aca398911c82.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/230270801-f42bffe2-0565-4817-83a9-c02590c35f4a.png" alt="image"><br><img src="https://user-images.githubusercontent.com/7252598/230270808-00f44e9a-d5bd-4420-8566-e0be0b7d1ad6.png" alt="image"></li>
</ul>
<h1 id="Designing-the-T-Few-Recipe"><a href="#Designing-the-T-Few-Recipe" class="headerlink" title="Designing the T-Few Recipe"></a>Designing the T-Few Recipe</h1><h2 id="Model-and-Datasets"><a href="#Model-and-Datasets" class="headerlink" title="Model and Datasets"></a>Model and Datasets</h2><ul>
<li><code>In preliminary experiments</code> applying PEFT methods to different pre-trained models, <code>we attained the best performance with T0</code><ul>
<li>T0 was released in three billion and eleven billion parameter variants, referred to as “T0-3B” and simply “T0” respectively<ul>
<li>11B를 T0로 표기하겠다는 것</li>
</ul>
</li>
</ul>
</li>
<li>we use <code>T0-3B</code> to reduce computational costs</li>
<li>we use the <code>same number of few-shot training examples</code> for each dataset as Brown et al. [4], which varies from <code>20 to 70</code></li>
<li>we train our model for <code>1K steps</code> with <code>a batch size of 8</code> and report performance at the end of training.</li>
</ul>
<h2 id="Unlikelihood-Training-and-Length-Normalization"><a href="#Unlikelihood-Training-and-Length-Normalization" class="headerlink" title="Unlikelihood Training and Length Normalization"></a>Unlikelihood Training and Length Normalization</h2><ul>
<li>LM의 few-shot finetuning을 개선하기 위해 2가지 loss를 추가적으로 검토함</li>
<li>LM은 기본적으로 cross-entropy loss를 가짐 </li>
<li>기본적으로 평가는 Rank classification (likelihood가 더 높은게 이기는)를 쓸거기 때문에 Unlikelihood training을 통해 incorrect choices의 확률을 낮춰서 개선시키려함<ul>
<li>Unlikelihood -&gt; N개의 incorrect choices에 대해서 loss를 합해주는데(제일 바깥쪽 시그마), 각각을 계산할땐(안쪽 시그마) 1-p(y|x, y&lt;t)로 incorrect token이 나올 확률을 1에서 빼줌으로써 학습해야할 방향을 잘못된 토큰생성에서 올바른 토큰생성쪽으로 역전시켜줌<ul>
<li>Unlikelihood Training을 위한 샘플도 따로 뽑아놔야겠네<br><img src="https://user-images.githubusercontent.com/7252598/229989356-94d1f76b-a515-4285-8a6f-2ac75fb7617c.png" alt="image"></li>
</ul>
</li>
</ul>
</li>
<li>이번엔 Length Normalization쪽을 보자<ul>
<li>The possible target sequences for a given training example can have significantly different lengths<ul>
<li>타겟의 길이가 다 다르다는 것!</li>
</ul>
</li>
<li>Ranking each choice based on probability can therefore “favor” shorter choices because the model’s assigned probability to each token is ≤ 1.<ul>
<li>이건 랭킹쪽에서 이슈가 되는데, 토큰 생성 확률이 1 이하기 때문에 짧은게 더 유리하다는 것! (아마 곱하기하면 점점 더 작아져서 그런게 아닌가)</li>
<li>To rectify this, we consider <code>using length normalization</code> when performing rank classification, which divides the model’s score on each possible answer choice by the number of tokens in the choice</li>
<li>먼저 length-normalized log prob 계산! (First, we compute the length-normalized log probability of a given output sequence)</li>
<li>softmax cross entropy loss를 줄여서 length-normalized log prob 최대화! (we maximize the length-normalized log probability of the correct answer choice by minimizing the softmax cross-entropy loss)<ul>
<li>정확히는 왜 softmax cross-entropy로 재구성해서 추가해야하는지 모르겠다. 그냥 LM loss와 유사해보이는데..?!</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/229991133-9284548d-b51f-48e0-96a8-fd8f23eb2bcb.png" alt="image"></p>
<ul>
<li>결과적으론 위 loss term을 추가할 수록 좋아짐<ul>
<li>We find that adding L_LN improves the accuracy from 60.7% to 62.71% and including both L_UL and L_LN provides a further improvement to 63.3%</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/r-three/t-few/blob/4e581fa0b8f53e36da252a15bd581d365d4dd333/src/models/EncoderDecoder.py#L67">파이썬 코드 예제</a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">choices_scores = (</span><br><span class="line">    F.cross_entropy(model_output.logits.flatten(<span class="number">0</span>, <span class="number">1</span>), lm_target.flatten(<span class="number">0</span>, <span class="number">1</span>), reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">    .view(bs, num_choices, -<span class="number">1</span>)</span><br><span class="line">    .<span class="built_in">sum</span>(dim=-<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> self.config.length_norm &gt; <span class="number">0</span>: // 타겟 길이 정규화</span><br><span class="line">    choices_scores = choices_scores / torch.<span class="built_in">pow</span>(</span><br><span class="line">        (choices_ids != self.tokenizer.pad_token_id).<span class="built_in">sum</span>(dim=-<span class="number">1</span>), self.config.length_norm</span><br><span class="line">    )</span><br><span class="line">lm_loss = F.cross_entropy(</span><br><span class="line">    model_output.logits.view(bs, num_choices, *model_output.logits.size()[<span class="number">1</span>:])[<span class="built_in">range</span>(bs), labels].flatten(</span><br><span class="line">        <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    ),</span><br><span class="line">    lm_target.view(bs, num_choices, -<span class="number">1</span>)[<span class="built_in">range</span>(bs), labels].flatten(<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tensorboard_logs = &#123;<span class="string">&quot;lm_loss&quot;</span>: lm_loss.item()&#125;</span><br><span class="line"><span class="keyword">if</span> self.config.mc_loss &gt; <span class="number">0</span>:</span><br><span class="line">    mc_loss = F.cross_entropy(-choices_scores, labels)</span><br><span class="line">    tensorboard_logs[<span class="string">&quot;mc_loss&quot;</span>] = mc_loss.item()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    mc_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> self.config.unlikely_loss &gt; <span class="number">0</span>: // Unlikelihood loss 추가</span><br><span class="line">    cand_loglikely = -F.cross_entropy(</span><br><span class="line">        model_output.logits.flatten(<span class="number">0</span>, <span class="number">1</span>), lm_target.flatten(<span class="number">0</span>, <span class="number">1</span>), reduction=<span class="string">&quot;none&quot;</span></span><br><span class="line">    ).view(bs, num_choices, -<span class="number">1</span>)</span><br><span class="line">    cand_loglikely += (lm_target &lt; <span class="number">0</span>).view(bs, num_choices, -<span class="number">1</span>) * -<span class="number">100</span></span><br><span class="line">    cand_loglikely[<span class="built_in">range</span>(bs), labels] = -<span class="number">100</span></span><br><span class="line">    unlikely_loss = -torch.log(<span class="number">1</span> - torch.exp(cand_loglikely) + <span class="number">1e-2</span>).<span class="built_in">sum</span>() / (cand_loglikely != -<span class="number">100</span>).<span class="built_in">sum</span>()</span><br><span class="line">    tensorboard_logs[<span class="string">&quot;unlikely_loss&quot;</span>] = unlikely_loss.item()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    unlikely_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">loss = lm_loss + mc_loss * self.config.mc_loss + unlikely_loss * self.config.unlikely_loss</span><br><span class="line">tensorboard_logs[<span class="string">&quot;loss&quot;</span>] = loss.item()</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Parameter-efficient-fine-tuning-with-IA-3"><a href="#Parameter-efficient-fine-tuning-with-IA-3" class="headerlink" title="Parameter-efficient fine-tuning with (IA)3"></a>Parameter-efficient fine-tuning with (IA)3</h2><p><img src="https://user-images.githubusercontent.com/7252598/230011306-0e6184ac-af46-4920-b483-03e2481d0457.png" alt="image"></p>
<ul>
<li>we need a PEFT method that has the following properties:<ul>
<li>First, it must add or update <code>as few parameters as possible</code> to avoid incurring storage and memory costs.</li>
<li>Second, it should achieve strong accuracy after few-shot training on new tasks.</li>
<li>Finally, it must allow for mixed-task batches, since that is a capability of ICL.<ul>
<li>(In order to easily enable mixed-task batches, a PEFT method should ideally not modify the model itself.)</li>
</ul>
</li>
</ul>
</li>
<li>A more convenient alternative is provided by methods that <strong>directly modify</strong> the <code>activations of the model</code> since this can be done independently and cheaply to each example in the batch according to which task the example corresponds to.</li>
<li>As an alternative, we explored <code>element-wise multiplication (i.e. rescaling)</code> of the model’s activations against a learned vector.</li>
<li>Activation을 이용한 방법들이 있고 그중에 sequence of Activation에 learnable vector를 element-wise로 곱해서 adaptation!를 할수도있음</li>
<li>실험해보니 each set of activations에 learned rescaling vector를 사용할 필요가 없고 대신 <ul>
<li>self-attention + encoder-decoder attention에 있는 key, value쪽에 rescaling vectors를 도입 및</li>
<li>position-wise feed-forward networks의 intermediate activation에도 도입하면 충분했음</li>
</ul>
</li>
<li>즉 3가지 learnable vector를 제안하는 것임! <code>l_k, l_v, l_ff</code> (처음엔 다 ones vector로 init됨)<ul>
<li>(특이한건 query에는 따로 처리해주는게 없음, 왜 없을까? 학습을 다 끝내고나면 저 <code>l_k, l_v, l_ff</code> 벡터들은 그대로 남는걸까? -&gt; 아마 남는 형태가 되면 체크포인트 저장은 어떤식일까? 모델 자체가 변경되는건가?)</li>
<li>attention mechanism을 보면 key쪽에 리스케일 한번 하고 value쪽에서 리스케일 한번함 (bias가 아니라 그냥 rescaling 벡터같은데..근데 이게 layer마다 필요한건 아니고 하나만들어 놓으면 layer에 쭉 재활용하는건가? 그럼 싸게 먹히긴하는데 -&gt; 식보니까 재활용 아니네..)</li>
</ul>
</li>
<li><code>인코더에서는 L(d_k+d_v+d_ff)</code>개만큼 필요하고 <code>디코더에서는 L(2d_k+2d_v+d_ff)</code>개만큼 필요함<ul>
<li>layer 개수만큼이니까 뭔가 그렇게 싸게 먹히는것 같진 않은데.. 흠</li>
</ul>
</li>
<li>We call our method (IA)3, which stands for <strong>“Infused Adapter by Inhibiting and Amplifying Inner Activations”</strong></li>
<li>(IA)3 makes mixed-task batches possible because each sequence of activations in the batch can be separately and cheaply multiplied by its associated learned task vector</li>
<li>그런데 이 부분이 꽤 흥미롭다! 이 벡터들을 가중치 행렬에 영구적용할 수 있어서 아키텍쳐 변경이 필요하지 않을 수 있다는 것! (체크포인트 저장이 쉬워질수있겠군) -&gt; orginal 모델에서 추가적인 계산 코스트가 없다!<ul>
<li>We also note that, in the event that a model will only be used on a single task, <code>the modifications introduced by (IA)3 can also be applied to weight matrices permanently</code> so that <code>no elementwise multiplication is required and the model’s architecture remains unchanged</code>.</li>
</ul>
</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/230010560-64b5e1fe-1574-4532-9bb7-0b18e505cdcb.png" alt="image"></p>
<ul>
<li>T0-3B로 9개의 PEFT methods에 대해서 검증해봄</li>
<li>9 strong PEFT methods<ul>
<li>BitFit [47] which updates only the bias parameters;</li>
<li>Adapters [23] which introduce task-specific layers after the self-attention and position-wise feed-forward networks;</li>
<li>Compacter and Compacter++ [28] which improve upon adapters by using low-rank matrices and hypercomplex mul- tiplication;</li>
<li>prompt tuning [14] which learns task-specific prompt embeddings that are concatenated to the model’s input;</li>
<li>FISH Mask [26] which chooses a subset of parameters to update based on their ap- proximate Fisher information;</li>
<li>Intrinsic SAID [27] which performs optimization in a low-dimensional subspace;</li>
<li>prefix-tuning [29] which learns task-specific vectors that are concatenated to the model’s activations; </li>
<li>LoRA [13] which assigns low-rank updates to parameter matrices.<br><img src="https://user-images.githubusercontent.com/7252598/230011771-6be13446-6fd8-4a6d-a558-219cdd975eee.png" alt="image"></li>
</ul>
</li>
</ul>
<h2 id="Pre-training-IA-3"><a href="#Pre-training-IA-3" class="headerlink" title="Pre-training (IA)3"></a>Pre-training (IA)3</h2><ul>
<li>pre-training the prompt embeddings in prompt tuning can improve performance when fine-tuning on downstream few-shot tasks. For pre- training, Gu et al. [18] use a suite of self-supervised tasks applied to unlabeled text data, and Vu et al. [19] consider using embeddings from a separate task or multitask mixture. </li>
<li>We follow Vu et al. [19] and simply pre-train the new parameters introduced by (IA)3 on the same multitask mixture used to train T0. We pre-train for 100,000 steps with a batch size of 16 before fine-tuning the (IA)3 parameters on each individual downstream dataset</li>
<li>We find that pre-training improves fine-tuned accuracy from 64.6 to 65.8 and therefore add it to our recipe<ul>
<li>일단 성능은 오른건데, 잘 이해를 못하겠는데 어차피 matrix에 영구적으로 리스케일할건데 왜 굳이 또 이걸 pretraining에 학습을 해야돼?<br><img src="https://user-images.githubusercontent.com/7252598/230024678-568dbeb2-8492-475c-b899-d4e727c1c475.png" alt="image"></li>
</ul>
</li>
</ul>
<h2 id="Combining-the-ingredients"><a href="#Combining-the-ingredients" class="headerlink" title="Combining the ingredients"></a>Combining the ingredients</h2><ul>
<li>the T-Few recipe is defined as follows:<ul>
<li>We use the T0 model as a backbone.</li>
<li>We add (IA)3 for downstream task adaptation</li>
<li>use parameters initialized from pre-training (IA)3 on the same multitask mixture for T0<ul>
<li>T0 자체가 이미 instruction tuning 된건데 여기에 pre-training한다는게 도대체 뭐지.. T0 학습셋에 대해서 학습시킨걸 뜻하는건가…뭐지 ㅠ</li>
</ul>
</li>
<li>As an objective, we use the sum of a standard language modeling loss L_LM, an unlikelihood loss L_UL for incorrect choices, and a length-normalized loss L_LN.</li>
<li>We train for 1,000 steps with a batch size of 8 sequences using the Adafactor optimizer [49] with a learning rate of 3e−3 and a linear decay schedule with a 60-step warmup</li>
<li>apply prompt templates to downstream datasets during training and inference to convert each example into an instructive text-to-text format.</li>
<li>we apply this recipe to every downstream dataset in exactly the same way without per-dataset hyperparameter tuning or modifications.</li>
</ul>
</li>
</ul>
<h1 id="Outperforming-ICL-with-T-Few"><a href="#Outperforming-ICL-with-T-Few" class="headerlink" title="Outperforming ICL with T-Few"></a>Outperforming ICL with T-Few</h1><h2 id="Performance-on-T0-tasks"><a href="#Performance-on-T0-tasks" class="headerlink" title="Performance on T0 tasks"></a>Performance on T0 tasks</h2><p><img src="https://user-images.githubusercontent.com/7252598/230026103-f3fec870-0d86-490c-836e-a0eae38a8df2.png" alt="image"></p>
<h2 id="Comparing-computational-costs"><a href="#Comparing-computational-costs" class="headerlink" title="Comparing computational costs"></a>Comparing computational costs</h2><ul>
<li>we now compare the relative costs of each few-shot learning approach.</li>
<li>For simplicity, we use the <code>FLOPs-per-token</code> estimates for Transformer-based language models introduced by Kaplan et al. [20].<ul>
<li>Specifically, we estimate that a <code>decoder-only Transformer (e.g. the GPT series) with N parameters uses 2N FLOPs per token for inference</code> and <code>6N FLOPs per token for training</code>.<ul>
<li>왜지?</li>
</ul>
</li>
<li><code>Encoder-decoder models like T0 and T5</code> (where the encoder and decoder have the same number of layers and layer sizes) only process each token with either the encoder or decoder (each having roughly half the parameters of the full model), so the FLOPs per token estimates are halved to N and <code>3N FLOPs per token</code> for inference and training.</li>
</ul>
</li>
<li>FLOPs 측정의 단점?<ul>
<li>We note that FLOPs are not a direct measurement of real-world computational cost because latency, power usage, and other costs can vary significantly depending on hardware and other factors</li>
</ul>
</li>
<li>그럼에도 불구하고 선택한 이유<ul>
<li>However, we focus on FLOPs because it is a hardware-independent metric that closely with real-world costs the hardware setup used for running the different methods we consider would likely vary significantly across methods</li>
</ul>
</li>
</ul>
<h3 id="Inference-cost"><a href="#Inference-cost" class="headerlink" title="Inference cost."></a>Inference cost.</h3><ul>
<li>Processing a single input and all target choices with <code>T-Few</code> requires <code>11e9 × 103 = 1.1e12 FLOPs</code>, whereas <code>few-shot ICL with GPT-3 175B</code> requires <code>2 × 175e9 × (41 × 98 + 103) = 1.4e15 FLOPs</code>– more than 3 orders of magnitude more.</li>
</ul>
<h3 id="Training-cost"><a href="#Training-cost" class="headerlink" title="Training cost"></a>Training cost</h3><ul>
<li>T-Few is the only method that involves updating parameters, it is the only method that incurs a training cost.</li>
<li>Training an eleven billion parameter encoder-decoder model for 1,000 steps with a batch size of 8 length-103 sequences requires approximately 3 × 11e9 × 1, 000 × 8 × 103 &#x3D; 2.7e16 FLOPs</li>
<li>While not insignificant, this is only about 20 times larger than the FLOPs required to process a single example with few-shot ICL using GPT-3 175B. In other words, training T-Few costs as much as using GPT-3 175B to process 20 examples with few-shot ICL.</li>
<li>We also found that fine-tuning T0 with T-Few on a single dataset only takes about a <code>half an hour</code> on a <code>single NVIDIA A100 GPU</code>. As of writing, this would cost about <code>$2 USD using Microsoft Azure</code></li>
</ul>
<h4 id="Storage-cost"><a href="#Storage-cost" class="headerlink" title="Storage cost"></a>Storage cost</h4><ul>
<li>T-Few also incurs the largest storage cost.</li>
<li>When stored as single-precision floats, the parameters added by (IA)3 take up 4.2 MB of space on disk</li>
<li>In contrast, ICL methods only require storing the tokenized in-context examples (typically stored as 32-bit integers), resulting in a smaller 41 × 98 × 32 bits &#x3D; 16 kB disk space requirement. </li>
<li>However, we note that <code>4.2 MB</code> is dwarfed by the on-disk size of the model checkpoints themselves – storing the (IA)3 adaptation vectors for 10,000 tasks would take about as much space as the T0 checkpoint <code>(41.5 GB)</code>.</li>
</ul>
<h4 id="Memory-usage"><a href="#Memory-usage" class="headerlink" title="Memory usage"></a>Memory usage</h4><ul>
<li>T-Few will incur a lower memory cost during inference. Additional memory costs are incurred when training T-Few due to the need to cache intermediate activations for backpropagation and for the gradient accumulator variables in Adafactor.</li>
<li>it is possible to use the T-Few recipe on a single 80GB A100 GPU.</li>
</ul>
<h2 id="Performance-on-Real-world-Few-shot-Tasks-RAFT"><a href="#Performance-on-Real-world-Few-shot-Tasks-RAFT" class="headerlink" title="Performance on Real-world Few-shot Tasks (RAFT)"></a>Performance on Real-world Few-shot Tasks (RAFT)</h2><ul>
<li>기존 평가가 few-shot learning에 적합하지 않다<ul>
<li>So far, we have evaluated performance on a collection of datasets that were not explicitly designed for benchmarking few-shot learning</li>
</ul>
</li>
<li>To better evaluate T-Few’s performance in the real world, we evaluated our approach on the RAFT benchmark</li>
<li>RAFT consists of <code>11</code> “economically valuable” tasks that aim to mirror real-world applications</li>
<li>Importantly, each RAFT datasets has <code>only 50 training examples</code> with no validation set and a (larger) test set with no public labels, so it is <code>impossible to “cheat”</code> by tuning on an unrealistically-large validation set or by peeking at the test set [32, 31].</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/230131726-78174f90-7cd2-46ec-8a43-d68a3f28994a.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/7252598/230131757-304ff7c5-6d47-4520-9456-e25877509c58.png" alt="image"></p>
<ul>
<li>RAFT leaderborad<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/ought/raft-leaderboard">https://huggingface.co/spaces/ought/raft-leaderboard</a><br><img src="https://user-images.githubusercontent.com/7252598/230250722-704fb5a9-798d-426e-ba71-05fef80ae7ca.png" alt="image"></li>
</ul>
</li>
</ul>
<h2 id="Ablation-experiments"><a href="#Ablation-experiments" class="headerlink" title="Ablation experiments"></a>Ablation experiments</h2><ul>
<li>While the gains from adding each ingredient does not always significant increase the accuracy on each individual dataset, each ingredient consistently improves the average performance across datasets: Removing pre-training decreases accuracy by 1.6%, removing unlikelihood training and length normalization decreases accuracy by 4.1%, and removing both pre-training and our additional loss terms reduces accuracy by 2.5%.</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/7252598/230132690-76f1edcf-c598-4753-8a12-b55cd38fd6d6.png" alt="image"></p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul>
<li>We introduced T-Few, a parameter-efficient few-shot learning recipe that attains higher accuracy than few-shot ICL at a lower computational cost</li>
<li>T-Few uses (IA)3, a new PEFT method that rescales inner activations with learned vectors.</li>
<li>Using (IA)3 produces better performance than fine-tuning the full model while only introducing a tiny amount of additional parameters.</li>
<li>T-Few also uses two additional loss terms that encourage the model to output lower probabilities for incorrect choices and account for the length of different answer choices</li>
<li>we found that T-Few uses over 1,000× fewer FLOPs during inference than few-shot ICL with GPT-3 and only requires 30 minutes to train on a single NVIDIA A100 GPU</li>
</ul>
<h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h2 id="Full-Unlikelihood-Training-and-Length-Normalization-Results"><a href="#Full-Unlikelihood-Training-and-Length-Normalization-Results" class="headerlink" title="Full Unlikelihood Training and Length Normalization Results"></a>Full Unlikelihood Training and Length Normalization Results</h2><p><img src="https://user-images.githubusercontent.com/7252598/230134081-556dc611-bf0f-46f6-bba4-ca3f835a388e.png" alt="image"></p>
<h2 id="Full-PEFT-Results"><a href="#Full-PEFT-Results" class="headerlink" title="Full PEFT Results"></a>Full PEFT Results</h2><table>
<thead>
<tr>
<th>with L_UL, L_N</th>
<th>without L_UL, L_N</th>
</tr>
</thead>
<tbody><tr>
<td><img src="https://user-images.githubusercontent.com/7252598/230134768-3849c9a5-9ab6-44c8-8065-43c4a420dcbe.png" alt="image"></td>
<td><img src="https://user-images.githubusercontent.com/7252598/230135414-cf65a5e9-dbe4-4f22-b84b-a58c8f964f84.png" alt="image"></td>
</tr>
</tbody></table>
<h2 id="Comparing-T-Few-with-few-shot-ICL-methods"><a href="#Comparing-T-Few-with-few-shot-ICL-methods" class="headerlink" title="Comparing T-Few with few-shot ICL methods"></a>Comparing T-Few with few-shot ICL methods</h2><p><img src="https://user-images.githubusercontent.com/7252598/230136093-9b2adfe4-e6f4-4796-be7f-81312a1d2c13.png" alt="image"></p>
<h2 id="구현"><a href="#구현" class="headerlink" title="구현"></a>구현</h2><ul>
<li>LoRA쪽 구현체를 변경하는 식으로 되어있음</li>
<li>t-few&#x2F;configs&#x2F;ia3.json</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;lora_scaling_rank&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;lora_rank&quot;</span><span class="punctuation">:</span> <span class="number">0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;lora_init_scale&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;lora_modules&quot;</span><span class="punctuation">:</span> <span class="string">&quot;.*SelfAttention|.*EncDecAttention|.*DenseReluDense&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;lora_layers&quot;</span><span class="punctuation">:</span> <span class="string">&quot;k|v|wi_1.*&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;trainable_param_names&quot;</span><span class="punctuation">:</span> <span class="string">&quot;.*lora_b.*&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;model_modifier&quot;</span><span class="punctuation">:</span> <span class="string">&quot;lora&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;lr&quot;</span><span class="punctuation">:</span> <span class="number">3e-3</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;num_steps&quot;</span><span class="punctuation">:</span> <span class="number">1000</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>lora기반 ia3 구현체 스니펫</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">modify_with_lora</span>(<span class="params">transformer, config</span>):</span><br><span class="line">    <span class="keyword">for</span> m_name, module <span class="keyword">in</span> <span class="built_in">dict</span>(transformer.named_modules()).items():</span><br><span class="line">        <span class="keyword">if</span> re.fullmatch(config.lora_modules, m_name):</span><br><span class="line">            <span class="keyword">for</span> c_name, layer <span class="keyword">in</span> <span class="built_in">dict</span>(module.named_children()).items():</span><br><span class="line">                <span class="keyword">if</span> re.fullmatch(config.lora_layers, c_name):</span><br><span class="line">                    <span class="keyword">assert</span> <span class="built_in">isinstance</span>(</span><br><span class="line">                        layer, nn.Linear</span><br><span class="line">                    ), <span class="string">f&quot;LoRA can only be applied to torch.nn.Linear, but <span class="subst">&#123;layer&#125;</span> is <span class="subst">&#123;<span class="built_in">type</span>(layer)&#125;</span>.&quot;</span></span><br><span class="line">                    <span class="built_in">setattr</span>(</span><br><span class="line">                        module,</span><br><span class="line">                        c_name,</span><br><span class="line">                        LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale),</span><br><span class="line">                    )</span><br><span class="line">    <span class="keyword">return</span> transformer</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LoRALinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, linear_layer, rank, scaling_rank, init_scale</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.in_features = linear_layer.in_features</span><br><span class="line">        self.out_features = linear_layer.out_features</span><br><span class="line">        self.rank = rank</span><br><span class="line">        self.scaling_rank = scaling_rank</span><br><span class="line">        self.weight = linear_layer.weight</span><br><span class="line">        self.bias = linear_layer.bias</span><br><span class="line">        <span class="keyword">if</span> self.rank &gt; <span class="number">0</span>:</span><br><span class="line">            self.lora_a = nn.Parameter(torch.randn(rank, linear_layer.in_features) * init_scale)</span><br><span class="line">            <span class="keyword">if</span> init_scale &lt; <span class="number">0</span>:</span><br><span class="line">                self.lora_b = nn.Parameter(torch.randn(linear_layer.out_features, rank) * init_scale)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.lora_b = nn.Parameter(torch.zeros(linear_layer.out_features, rank))</span><br><span class="line">        <span class="keyword">if</span> self.scaling_rank:</span><br><span class="line">            self.multi_lora_a = nn.Parameter(</span><br><span class="line">                torch.ones(self.scaling_rank, linear_layer.in_features)</span><br><span class="line">                + torch.randn(self.scaling_rank, linear_layer.in_features) * init_scale</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">if</span> init_scale &lt; <span class="number">0</span>:</span><br><span class="line">                self.multi_lora_b = nn.Parameter(</span><br><span class="line">                    torch.ones(linear_layer.out_features, self.scaling_rank)</span><br><span class="line">                    + torch.randn(linear_layer.out_features, self.scaling_rank) * init_scale</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.multi_lora_b = nn.Parameter(torch.ones(linear_layer.out_features, self.scaling_rank))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">if</span> self.scaling_rank == <span class="number">1</span> <span class="keyword">and</span> self.rank == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># parsimonious implementation for ia3 and lora scaling</span></span><br><span class="line">            <span class="keyword">if</span> self.multi_lora_a.requires_grad:</span><br><span class="line">                <span class="comment"># 이 부분에서 IA3의 곱하기가 일어나는듯!!</span></span><br><span class="line">                hidden = F.linear((<span class="built_in">input</span> * self.multi_lora_a.flatten()), self.weight, self.bias)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hidden = F.linear(<span class="built_in">input</span>, self.weight, self.bias)</span><br><span class="line">            <span class="keyword">if</span> self.multi_lora_b.requires_grad:</span><br><span class="line">                hidden = hidden * self.multi_lora_b.flatten()</span><br><span class="line">            <span class="keyword">return</span> hidden</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># general implementation for lora (adding and scaling)</span></span><br><span class="line">            weight = self.weight</span><br><span class="line">            <span class="keyword">if</span> self.scaling_rank:</span><br><span class="line">                weight = weight * torch.matmul(self.multi_lora_b, self.multi_lora_a) / self.scaling_rank</span><br><span class="line">            <span class="keyword">if</span> self.rank:</span><br><span class="line">                weight = weight + torch.matmul(self.lora_b, self.lora_a) / self.rank</span><br><span class="line">            <span class="keyword">return</span> F.linear(<span class="built_in">input</span>, weight, self.bias)</span><br></pre></td></tr></table></figure></li>
<li><p>F.linear 관련 내용<br><img src="https://user-images.githubusercontent.com/7252598/230257608-7cf38e78-25f5-4ca2-b68d-ad0f75cad6f4.png" alt="image"></p>
</li>
<li><p>아래는 LoRA의 config인데 차이는 조금 있음</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LoRAConfig</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.lora_rank = <span class="number">4</span></span><br><span class="line">        self.lora_init_scale = <span class="number">0.01</span></span><br><span class="line">        self.lora_modules = <span class="string">&quot;.*SelfAttention|.*EncDecAttention&quot;</span></span><br><span class="line">        self.lora_layers = <span class="string">&quot;q|k|v|o&quot;</span></span><br><span class="line">        self.trainable_param_names = <span class="string">&quot;.*layer_norm.*|.*lora_[ab].*&quot;</span></span><br><span class="line">        self.lora_scaling_rank = <span class="number">1</span></span><br></pre></td></tr></table></figure>
</div><div class="article-licensing box"><div class="licensing-title"><p>(IA3) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</p><p><a href="https://eagle705.github.io/ia3/">https://eagle705.github.io/ia3/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Joosung Yoon</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-05-09</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-05-09</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/nlp/">nlp</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=63297c6228f9450019a5f574&amp;product=sop" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/llama/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">llama</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/Alpaca/"><span class="level-item">Alpaca (A Strong Instruction-Following Model)</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://eagle705.github.io/ia3/';
            this.page.identifier = 'ia3/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eagle705-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/eagle705-logo.png" alt="Joosung Yoon"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joosung Yoon</p><p class="is-size-6 is-block">Machine Learning Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>There and Back Again</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">54</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/eagle705" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/eagle705"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hisdevelopers"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JSYoon53859120"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/joosung-yoon/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cslog/"><span class="level-start"><span class="level-item">cslog</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/categories/photo/"><span class="level-start"><span class="level-item">photo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">5월 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">3월 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">2월 2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">1월 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">12월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">11월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">10월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">9월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">8월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">5월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">12월 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">12월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">11월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">10월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">9월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">8월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">7월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">5월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">4월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">11월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">6월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">5월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">4월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/drone/"><span class="tag">drone</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/grpc/"><span class="tag">grpc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">42</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp-kb/"><span class="tag">nlp, kb</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"><span class="tag">생각정리</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">광고</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-2655870716902046" data-ad-slot="2764253071" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Note"><span class="level-left"><span class="level-item">1</span><span class="level-item">Note</span></span></a></li><li><a class="level is-mobile" href="#Author"><span class="level-left"><span class="level-item">2</span><span class="level-item">Author</span></span></a></li><li><a class="level is-mobile" href="#Summary"><span class="level-left"><span class="level-item">3</span><span class="level-item">Summary</span></span></a></li><li><a class="level is-mobile" href="#Abstract"><span class="level-left"><span class="level-item">4</span><span class="level-item">Abstract</span></span></a></li><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">5</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Background"><span class="level-left"><span class="level-item">6</span><span class="level-item">Background</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Parameter-efficient-fine-tuning"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">Parameter-efficient fine-tuning</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#요즘-핫한-LoRA만-조금-더-살펴보자"><span class="level-left"><span class="level-item">6.1.1</span><span class="level-item">요즘 핫한 LoRA만 조금 더 살펴보자!</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Designing-the-T-Few-Recipe"><span class="level-left"><span class="level-item">7</span><span class="level-item">Designing the T-Few Recipe</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Model-and-Datasets"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">Model and Datasets</span></span></a></li><li><a class="level is-mobile" href="#Unlikelihood-Training-and-Length-Normalization"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">Unlikelihood Training and Length Normalization</span></span></a></li><li><a class="level is-mobile" href="#Parameter-efficient-fine-tuning-with-IA-3"><span class="level-left"><span class="level-item">7.3</span><span class="level-item">Parameter-efficient fine-tuning with (IA)3</span></span></a></li><li><a class="level is-mobile" href="#Pre-training-IA-3"><span class="level-left"><span class="level-item">7.4</span><span class="level-item">Pre-training (IA)3</span></span></a></li><li><a class="level is-mobile" href="#Combining-the-ingredients"><span class="level-left"><span class="level-item">7.5</span><span class="level-item">Combining the ingredients</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Outperforming-ICL-with-T-Few"><span class="level-left"><span class="level-item">8</span><span class="level-item">Outperforming ICL with T-Few</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Performance-on-T0-tasks"><span class="level-left"><span class="level-item">8.1</span><span class="level-item">Performance on T0 tasks</span></span></a></li><li><a class="level is-mobile" href="#Comparing-computational-costs"><span class="level-left"><span class="level-item">8.2</span><span class="level-item">Comparing computational costs</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Inference-cost"><span class="level-left"><span class="level-item">8.2.1</span><span class="level-item">Inference cost.</span></span></a></li><li><a class="level is-mobile" href="#Training-cost"><span class="level-left"><span class="level-item">8.2.2</span><span class="level-item">Training cost</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Performance-on-Real-world-Few-shot-Tasks-RAFT"><span class="level-left"><span class="level-item">8.3</span><span class="level-item">Performance on Real-world Few-shot Tasks (RAFT)</span></span></a></li><li><a class="level is-mobile" href="#Ablation-experiments"><span class="level-left"><span class="level-item">8.4</span><span class="level-item">Ablation experiments</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Conclusion"><span class="level-left"><span class="level-item">9</span><span class="level-item">Conclusion</span></span></a></li><li><a class="level is-mobile" href="#Appendix"><span class="level-left"><span class="level-item">10</span><span class="level-item">Appendix</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Full-Unlikelihood-Training-and-Length-Normalization-Results"><span class="level-left"><span class="level-item">10.1</span><span class="level-item">Full Unlikelihood Training and Length Normalization Results</span></span></a></li><li><a class="level is-mobile" href="#Full-PEFT-Results"><span class="level-left"><span class="level-item">10.2</span><span class="level-item">Full PEFT Results</span></span></a></li><li><a class="level is-mobile" href="#Comparing-T-Few-with-few-shot-ICL-methods"><span class="level-left"><span class="level-item">10.3</span><span class="level-item">Comparing T-Few with few-shot ICL methods</span></span></a></li><li><a class="level is-mobile" href="#구현"><span class="level-left"><span class="level-item">10.4</span><span class="level-item">구현</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:53.000Z">2023-05-09</time></p><p class="title"><a href="/pythia/">pythia</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:39.000Z">2023-05-09</time></p><p class="title"><a href="/llama/">llama</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:01.000Z">2023-05-09</time></p><p class="title"><a href="/ia3/">(IA3) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-23T08:14:37.000Z">2023-03-23</time></p><p class="title"><a href="/Alpaca/">Alpaca (A Strong Instruction-Following Model)</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-20T13:07:50.000Z">2023-02-20</time></p><p class="title"><a href="/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/">SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기</a></p><p class="categories"><a href="/categories/ML/">ML</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 Joosung Yoon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>