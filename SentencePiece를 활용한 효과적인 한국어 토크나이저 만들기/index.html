<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기 - Luke&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Luke&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Luke&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="소개자연어 문장을 컴퓨터가 쉽게 이해하게 만들기 위해서는 다양한 전처리 과정을 거쳐야합니다.그 중 하나로 문장을 토큰 단위로 쪼개서 처리하는 토크나이징 기법이 있습니다.오늘은 SentencePiece를 활용하여 한국어 텍스트를 효과적으로 토크나이징 하는 방법을 소개합니다. SentencePiece는 Google에서 2018년도에 공개한 오픈소스 라이브러리로"><meta property="og:type" content="blog"><meta property="og:title" content="SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기"><meta property="og:url" content="https://eagle705.github.io/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/"><meta property="og:site_name" content="Luke&#039;s Blog"><meta property="og:description" content="소개자연어 문장을 컴퓨터가 쉽게 이해하게 만들기 위해서는 다양한 전처리 과정을 거쳐야합니다.그 중 하나로 문장을 토큰 단위로 쪼개서 처리하는 토크나이징 기법이 있습니다.오늘은 SentencePiece를 활용하여 한국어 텍스트를 효과적으로 토크나이징 하는 방법을 소개합니다. SentencePiece는 Google에서 2018년도에 공개한 오픈소스 라이브러리로"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://user-images.githubusercontent.com/7252598/220215155-57aa7c35-ee6a-4aaf-8b54-d302a093186c.png"><meta property="og:image" content="https://devocean.sk.com/editorImg/2023/2/20/36b51d889790d2602dcc82d50fa77a2a6016af029a2150810a1ffbaf203756ba"><meta property="og:image" content="https://devocean.sk.com/editorImg/2023/2/20/4479c7a1647f4bcec7112ff2e02d60c566811ccddc64c0393294ae61a8559653"><meta property="article:published_time" content="2023-02-20T13:07:50.000Z"><meta property="article:modified_time" content="2023-03-04T14:28:25.788Z"><meta property="article:author" content="Joosung Yoon"><meta property="article:tag" content="nlp"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://user-images.githubusercontent.com/7252598/220215155-57aa7c35-ee6a-4aaf-8b54-d302a093186c.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://eagle705.github.io/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/"},"headline":"SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기","image":["https://user-images.githubusercontent.com/7252598/220215155-57aa7c35-ee6a-4aaf-8b54-d302a093186c.png"],"datePublished":"2023-02-20T13:07:50.000Z","dateModified":"2023-03-04T14:28:25.788Z","author":{"@type":"Person","name":"Joosung Yoon"},"publisher":{"@type":"Organization","name":"Luke's Blog","logo":{"@type":"ImageObject","url":"https://eagle705.github.io/img/eagle705-logo.png"}},"description":"소개자연어 문장을 컴퓨터가 쉽게 이해하게 만들기 위해서는 다양한 전처리 과정을 거쳐야합니다.그 중 하나로 문장을 토큰 단위로 쪼개서 처리하는 토크나이징 기법이 있습니다.오늘은 SentencePiece를 활용하여 한국어 텍스트를 효과적으로 토크나이징 하는 방법을 소개합니다. SentencePiece는 Google에서 2018년도에 공개한 오픈소스 라이브러리로"}</script><link rel="canonical" href="https://eagle705.github.io/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/"><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/monokai.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=UA-110980734-1" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'UA-110980734-1');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script data-ad-client="ca-pub-2655870716902046" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="카탈로그" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-02-20T13:07:50.000Z" title="2/20/2023, 10:07:50 PM">2023-02-20</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2023-03-04T14:28:25.788Z" title="3/4/2023, 11:28:25 PM">2023-03-04</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/ML/">ML</a></span><span class="level-item">21분안에 읽기 (약 3187 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기</h1><div class="content"><h1 id="소개"><a href="#소개" class="headerlink" title="소개"></a>소개</h1><p>자연어 문장을 컴퓨터가 쉽게 이해하게 만들기 위해서는 다양한 전처리 과정을 거쳐야합니다.<br>그 중 하나로 문장을 토큰 단위로 쪼개서 처리하는 토크나이징 기법이 있습니다.<br>오늘은 SentencePiece를 활용하여 한국어 텍스트를 효과적으로 토크나이징 하는 방법을 소개합니다.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">SentencePiece</a>는 Google에서 <a target="_blank" rel="noopener" href="https://aclanthology.org/D18-2012/">2018년도에 공개한</a> 오픈소스 라이브러리로, 다양한 자연어처리 태스크에서 널리 사용되고 있습니다.<br>최근에는 <a target="_blank" rel="noopener" href="https://github.com/huggingface/tokenizers">Huggingface에서 공개한 Tokenizers</a>도 자주 사용되고 있지만 오늘은 Sentencepiece에 대한 내용을 주로 다루도록 하겠습니다.</p>
<h3 id="텍스트-전처리-과정-예시"><a href="#텍스트-전처리-과정-예시" class="headerlink" title="텍스트 전처리 과정 예시"></a>텍스트 전처리 과정 예시</h3><table>
<thead>
<tr>
<th>데이터 전처리</th>
<th>예시</th>
</tr>
</thead>
<tbody><tr>
<td>텍스트 데이터를 모델이 이해하는 벡터로 바꾸려면 다양한 전처리 과정을 거치게 됩니다. 오늘 다루는 주제는 이중에서도 가장 앞단에 있는 텍스트를 토큰 단위로 쪼개는(Split) 전처리 과정을 다룹니다. 오른쪽 그림은 <code>&quot;히어로 무비 중 가장 어둡지만 가장 참신했다.&quot;</code> 라는 문장을 벡터로 변환하는 과정입니다. 텍스트 문장은 word, character, 형태소, subword (char, byte)등 다양한 방법으로 쪼개서 처리될 수 있습니다.</td>
<td><img src="https://user-images.githubusercontent.com/7252598/220215155-57aa7c35-ee6a-4aaf-8b54-d302a093186c.png" alt="image"></td>
</tr>
</tbody></table>
<h1 id="설치방법"><a href="#설치방법" class="headerlink" title="설치방법"></a>설치방법</h1><ul>
<li>pyenv를 통해 sentencepiece를 설치할 환경을 구성합니다</li>
<li>작성시점 기준 비교적 최신인 3.11.x 버전을 설치해줍니다</li>
<li>설치는 pyenv 기준으로 진행하겠습니다 (MacOS 환경입니다)</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ pyenv install -list</span><br><span class="line">Available versions:</span><br><span class="line">  ....</span><br><span class="line">  3.11.1</span><br><span class="line">  3.12.0a4</span><br><span class="line">  3.12-dev</span><br><span class="line">  activepython-3.6.0</span><br><span class="line">  anaconda-1.4.0</span><br><span class="line"></span><br><span class="line">$ pyenv install 3.11.1</span><br><span class="line">python-build: use openssl@1.1 from homebrew</span><br><span class="line">python-build: use readline from homebrew</span><br><span class="line">Downloading Python-3.11.1.tar.xz...</span><br><span class="line">-&gt; https://www.python.org/ftp/python/3.11.1/Python-3.11.1.tar.xz</span><br><span class="line">Installing Python-3.11.1...</span><br><span class="line">python-build: use readline from homebrew</span><br><span class="line">Installed Python-3.11.1 to /Users/사용자계정명/.pyenv/versions/3.11.1  </span><br></pre></td></tr></table></figure>

<ul>
<li>pyenv로 파이썬 가상환경을 구성 후 필요한 패키지를 설치합니다</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ pyenv virtualenv 3.11.1 nlp</span><br><span class="line">$ pyenv activate nlp</span><br><span class="line">$(nlp) pip install sentencepiece</span><br><span class="line">$(nlp) pip install datasets</span><br><span class="line">$(nlp) pip install transformers</span><br></pre></td></tr></table></figure>

<h1 id="학습방법"><a href="#학습방법" class="headerlink" title="학습방법"></a>학습방법</h1><ul>
<li>SentencePiece는 subword 토크나이저로 단어를 subword 단위로 구성하여 학습합니다</li>
<li>주어진 corpus를 subword 단위로 구성하여 subword의 빈도수를 계산해서 높은 빈도수를 가진 subword를 병합하여 모델을 완성합니다</li>
<li>subword의 패턴을 파악하기 위해서는 corpus가 필요합니다</li>
<li>학습할 corpus는 주로 wiki 데이터 또는 모델을 활용하고자 하는 도메인에서 일부를 샘플링해서 구축하지만 실습의 편의를 고려하여 <a target="_blank" rel="noopener" href="https://github.com/e9t/nsmc">nsmc (naver sentiment movie corpus)</a>를 사용하겠습니다</li>
<li>nsmc 데이터는 huggingface dataset hub에서도 다운받을 수 있습니다</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">&#x27;./data&#x27;</span></span><br><span class="line">dataset = load_dataset(<span class="string">&#x27;nsmc&#x27;</span>)</span><br><span class="line">os.makedirs(data_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> split_key <span class="keyword">in</span> dataset.keys():</span><br><span class="line">    doc_path = <span class="string">f&quot;<span class="subst">&#123;data_dir&#125;</span>/<span class="subst">&#123;split_key&#125;</span>.txt&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(doc_path, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> doc <span class="keyword">in</span> dataset[split_key][<span class="string">&#x27;document&#x27;</span>]:</span><br><span class="line">            f.write(doc+<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>데이터를 다운받으면 아래와 같이 학습에 사용할 데이터가 구축됩니다</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── README.md</span><br><span class="line">├── data</span><br><span class="line">     ├── test.txt</span><br><span class="line">     └── train.txt</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ head -n 5 train.txt </span><br><span class="line">아 더빙.. 진짜 짜증나네요 목소리</span><br><span class="line">흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</span><br><span class="line">너무재밓었다그래서보는것을추천한다</span><br><span class="line">교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</span><br><span class="line">사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다</span><br></pre></td></tr></table></figure>

<ul>
<li>데이터가 구축되었으면 sentencepiece 라이브러리를 활용해서 토크나이저를 학습해봅니다<ul>
<li>본 예제에서는 <code>T5</code> 모델을 위한 <code>CBPE(char level BPE)</code> 토크나이저를 학습하겠습니다<ul>
<li>BPE는 Byte-Pair Encoding의 약자로 1994년에 제안된 데이터 압축 알고리즘입니다.<ul>
<li>자연어처리에서는 char-level(CBPE) or byte-level(BBPE)에서 시작해서 점차적으로 vocab을 만들어내는 Bottom-up 방식으로 학습하게 됩니다</li>
<li>자연어 처리에서는 <a target="_blank" rel="noopener" href="https://aclanthology.org/P16-1162/">기계번역 태스크</a>에서 적용되기 시작했습니다</li>
</ul>
</li>
</ul>
</li>
<li>vocab size는 special_tokens (<code>pad</code>, <code>bos</code>, <code>eos</code>, <code>unk</code>, ….) <code>7개</code> + additional_special_tokens (T5를 위한 <code>&lt;extra_id_XX&gt;</code> 토큰) <code>100개</code> + subwords 토큰 <code>31,900개</code>로 구성되어 총 <code>32,000</code> 의 크기를 갖게 셋팅했습니다.</li>
<li>vocab size는 하이퍼파라미터로 정해진 값은 없습니다 (보통은 10,000~52,000 사이지만 모델의 크기에 따라 다름)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">&#x27;./data&#x27;</span></span><br><span class="line">paths = [<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> Path(data_dir).glob(<span class="string">&quot;*.txt&quot;</span>)]</span><br><span class="line">corpus = <span class="string">&quot;,&quot;</span>.join(paths)</span><br><span class="line">prefix = <span class="string">&quot;t5-sp-bpe-nsmc&quot;</span></span><br><span class="line">vocab_size = <span class="number">31900</span>-<span class="number">7</span></span><br><span class="line">spm.SentencePieceTrainer.train(</span><br><span class="line">    <span class="string">f&quot;--input=<span class="subst">&#123;corpus&#125;</span> --model_prefix=<span class="subst">&#123;prefix&#125;</span> --vocab_size=<span class="subst">&#123;vocab_size + <span class="number">7</span>&#125;</span>&quot;</span> + </span><br><span class="line">    <span class="string">&quot; --model_type=bpe&quot;</span> +</span><br><span class="line">    <span class="string">&quot; --max_sentence_length=999999&quot;</span> + <span class="comment"># 문장 최대 길이 (너무 길면 에러발생)</span></span><br><span class="line">    <span class="string">&quot; --pad_id=0 --pad_piece=&lt;pad&gt;&quot;</span> + <span class="comment"># pad (0)</span></span><br><span class="line">    <span class="string">&quot; --unk_id=1 --unk_piece=&lt;unk&gt;&quot;</span> + <span class="comment"># unknown (1)</span></span><br><span class="line">    <span class="string">&quot; --bos_id=2 --bos_piece=&lt;s&gt;&quot;</span> + <span class="comment"># begin of sequence (2)</span></span><br><span class="line">    <span class="string">&quot; --eos_id=3 --eos_piece=&lt;/s&gt;&quot;</span> + <span class="comment"># end of sequence (3)</span></span><br><span class="line">    <span class="string">&quot; --user_defined_symbols=&lt;sep&gt;,&lt;cls&gt;,&lt;mask&gt;&quot;</span>) <span class="comment"># 사용자 정의 토큰</span></span><br></pre></td></tr></table></figure>

<ul>
<li>sentencepiece 학습로그</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29600 all=641749 active=32848 piece=틈없이</span><br><span class="line">bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11 min_freq=5</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29620 all=641915 active=32245 piece=...0</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29640 all=641935 active=32265 piece=▁out</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29660 all=641945 active=32275 piece=▁같더라</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29680 all=641952 active=32282 piece=▁공연을</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29700 all=641961 active=32291 piece=▁기억할</span><br><span class="line">bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11 min_freq=5</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29720 all=641995 active=32128 piece=▁나이든</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29740 all=642015 active=32148 piece=▁누나가</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29760 all=642043 active=32176 piece=▁돌려도</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29780 all=642071 active=32204 piece=▁로코물</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29800 all=642089 active=32222 piece=▁머랄까</span><br><span class="line">bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11 min_freq=5</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29820 all=642124 active=32140 piece=▁미친짓</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29840 all=642148 active=32164 piece=▁범죄가</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29860 all=642164 active=32180 piece=▁봣지만</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29880 all=642172 active=32188 piece=▁사고의</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29900 all=642198 active=32214 piece=▁서로가</span><br><span class="line">bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11 min_freq=5</span><br><span class="line">trainer_interface.cc(685) LOG(INFO) Saving model: t5-sp-bpe-nsmc.model</span><br><span class="line">trainer_interface.cc(697) LOG(INFO) Saving vocabs: t5-sp-bpe-nsmc.vocab</span><br></pre></td></tr></table></figure>

<ul>
<li>sentencepiece의 학습이 끝나면 아래와 같이 학습된 파일이 만들어집니다</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── README.md</span><br><span class="line">├── data</span><br><span class="line">│   ├── test.txt</span><br><span class="line">│   └── train.txt</span><br><span class="line">├── t5-sp-bpe-nsmc.model</span><br><span class="line">├── t5-sp-bpe-nsmc.vocab</span><br><span class="line">└── train.py</span><br></pre></td></tr></table></figure>

<h1 id="학습된-토크나이저-사용하기"><a href="#학습된-토크나이저-사용하기" class="headerlink" title="학습된 토크나이저 사용하기"></a>학습된 토크나이저 사용하기</h1><ul>
<li>학습된 토크나이저 모델파일을 sentencepiece 라이브러리를 통해 다시 로딩해서 쓸 수 있지만, 사용성을 위해 huggingface의 T5Tokenizer로 랩핑해서 사용하겠습니다</li>
<li><code>save_pretrained()</code>함수를 사용하면 최근 많이 사용되는 huggingface의 토크나이저의 포멧으로 저장되어 재사용 할 수 있습니다</li>
<li>huggingface 클래스를 사용할때 실제 필요한 파일은 <code>t5-sp-bpe-nsmc.model</code> 하나만 있으면 됩니다</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> T5Tokenizer</span><br><span class="line">tokenizer = T5Tokenizer(vocab_file=<span class="string">&quot;t5-sp-bpe-nsmc.model&quot;</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;t5-tokenizer-bpe-nsmc&quot;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>huggingface에서 사용하는 토크나이저로 저장된 모습</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">├── README.md</span><br><span class="line">├── data</span><br><span class="line">│   ├── test.txt</span><br><span class="line">│   └── train.txt</span><br><span class="line">├── t5-sp-bpe-nsmc.model</span><br><span class="line">├── t5-sp-bpe-nsmc.vocab</span><br><span class="line">├── t5-tokenizer-bpe-nsmc</span><br><span class="line">│   ├── special_tokens_map.json</span><br><span class="line">│   ├── spiece.model</span><br><span class="line">│   └── tokenizer_config.json</span><br><span class="line">└── train.py</span><br></pre></td></tr></table></figure>

<ul>
<li>이제 학습한 토크나이저를 테스트해보겠습니다</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> T5Tokenizer</span><br><span class="line">tokenizer = T5Tokenizer(vocab_file=<span class="string">&quot;t5-sp-bpe.model&quot;</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;t5-tokenizer-bpe-nsmc&quot;</span>)</span><br><span class="line">lines = [</span><br><span class="line">  <span class="string">&quot;`DEVOCEAN`은 SK그룹의 대표 개발자 커뮤니티이자🧑&quot;</span>,</span><br><span class="line">  <span class="string">&quot;내/외부 개발자 간 소통과 성장을 위한 플랫폼을 상징합니다.👋&quot;</span>,</span><br><span class="line">  <span class="string">&quot;`Developers`&#x27; Ocean 개발자들을 위한 영감의 바다🙏&quot;</span>,</span><br><span class="line">  <span class="string">&quot;`Devotion` 헌신,몰두,전념💯&quot;</span>,</span><br><span class="line">  <span class="string">&quot;`Technology for Everyone` 모두를 위한 기술👍&quot;</span>,  </span><br><span class="line">]</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">    tokens = tokenizer.tokenize(line)</span><br><span class="line">    inputs = tokenizer(line)    </span><br><span class="line">    decoded_sequence = tokenizer.decode(inputs[<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(line) <span class="comment"># 입력 데이터</span></span><br><span class="line">    <span class="built_in">print</span>(tokens)  <span class="comment"># subword로 토큰화된 데이터</span></span><br><span class="line">    <span class="built_in">print</span>(decoded_sequence) <span class="comment"># subword토큰화된 데이터 -&gt; token id -&gt; 복원된데이터</span></span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure>

<ul>
<li>실행 결과 입력 데이터들이 대부분 subword로 잘 분절되어 쪼개지고 다시 decoding 했을때 복원된걸 확인할 수 있습니다<br>  <img src="https://devocean.sk.com/editorImg/2023/2/20/36b51d889790d2602dcc82d50fa77a2a6016af029a2150810a1ffbaf203756ba"></li>
<li>하지만 위 결과에서 한가지 문제가 있습니다</li>
</ul>
<blockquote>
<ul>
<li>학습 corpus에 없었던 🧑👋🙏💯👍와 같은 토큰들은 decoding시에 <code>unk(Unknown Token)</code> 토큰으로 복원되게 됩니다</li>
<li>🧑👋🙏💯👍와 같은 특수문자 또는 외국어등은 학습 corpus에 등장하는 비율이 낮아 토크나이저에서 decoding시에 <code>unk</code>토큰으로 복원될 가능성이 있습니다</li>
</ul>
</blockquote>
<ul>
<li>이러한 문제를 해결하기 위해 등장한것이 <code>BBPE (Byte-level BPE)</code>입니다 (<code>GPT</code> 계열의 모델이 사용함)<ul>
<li>subword 학습을 char level이 아닌 byte level 단위로 수행하는 것입니다</li>
<li>이러한 방법을 사용했을때는 사람이 육안으로 토큰들을 확인하는건 불편하지만, <code>OOV(Out-Of-Vocabulary)</code>를 줄일 수 있다는 장점이 있습니다</li>
<li>sentencepice에서는 공식적으로 BBPE를 지원하지 않지만, 비슷한 효과를 내는 <code>&quot; --byte_fallback=true&quot;</code> 옵션이 있습니다</li>
<li><code>&quot; --byte_fallback=true&quot;</code> 옵션은 <code>unk</code> 토큰을 만났을때 해당 토큰을 utf-8 byte level로 쪼개서 처리해줍니다 (<a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece/issues/621#issuecomment-777269803">참고</a>)</li>
</ul>
</li>
<li><code>&quot; --byte_fallback=true&quot;</code> 옵션을 적용해서 재학습후 평가해보겠습니다</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line">paths = [<span class="built_in">str</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> Path(data_dir).glob(<span class="string">&quot;*.txt&quot;</span>)]</span><br><span class="line">corpus = <span class="string">&quot;,&quot;</span>.join(paths)</span><br><span class="line">prefix = <span class="string">&quot;t5-sp-bpe-nsmc-byte-fallback&quot;</span></span><br><span class="line">vocab_size = <span class="number">31900</span>-<span class="number">7</span></span><br><span class="line">spm.SentencePieceTrainer.train(</span><br><span class="line">    <span class="string">f&quot;--input=<span class="subst">&#123;corpus&#125;</span> --model_prefix=<span class="subst">&#123;prefix&#125;</span> --vocab_size=<span class="subst">&#123;vocab_size + <span class="number">7</span>&#125;</span>&quot;</span> + </span><br><span class="line">    <span class="string">&quot; --model_type=bpe&quot;</span> +</span><br><span class="line">    <span class="string">&quot; --max_sentence_length=999999&quot;</span> + <span class="comment"># 문장 최대 길이 -&gt; 이게 너무 길면 에러발생함</span></span><br><span class="line">    <span class="string">&quot; --pad_id=0 --pad_piece=&lt;pad&gt;&quot;</span> + <span class="comment"># pad (0)</span></span><br><span class="line">    <span class="string">&quot; --unk_id=1 --unk_piece=&lt;unk&gt;&quot;</span> + <span class="comment"># unknown (1)</span></span><br><span class="line">    <span class="string">&quot; --bos_id=2 --bos_piece=&lt;s&gt;&quot;</span> + <span class="comment"># begin of sequence (2)</span></span><br><span class="line">    <span class="string">&quot; --eos_id=3 --eos_piece=&lt;/s&gt;&quot;</span> + <span class="comment"># end of sequence (3)</span></span><br><span class="line">    <span class="string">&quot; --byte_fallback=true&quot;</span> + <span class="comment"># add byte_fallback for unk tokens</span></span><br><span class="line">    <span class="string">&quot; --user_defined_symbols=&lt;sep&gt;,&lt;cls&gt;,&lt;mask&gt;&quot;</span>) <span class="comment"># 사용자 정의 토큰</span></span><br></pre></td></tr></table></figure>

<ul>
<li>실행 로그</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11 min_freq=5</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29820 all=642124 active=32140 piece=▁미친짓</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29840 all=642148 active=32164 piece=▁범죄가</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29860 all=642164 active=32180 piece=▁봣지만</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29880 all=642172 active=32188 piece=▁사고의</span><br><span class="line">bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11 size=29900 all=642198 active=32214 piece=▁서로가</span><br><span class="line">bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11 min_freq=5</span><br><span class="line">trainer_interface.cc(685) LOG(INFO) Saving model: t5-sp-bpe-nsmc-byte-fallback.model</span><br><span class="line">trainer_interface.cc(697) LOG(INFO) Saving vocabs: t5-sp-bpe-nsmc-byte-fallback.vocab</span><br></pre></td></tr></table></figure>

<ul>
<li>학습한 모델 로딩 후 테스트</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> T5Tokenizer</span><br><span class="line">tokenizer = T5Tokenizer(vocab_file=<span class="string">&quot;t5-sp-bpe-nsmc-byte-fallback.model&quot;</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;t5-tokenizer-bpe-nsmc-byte-fallback&quot;</span>)</span><br><span class="line">lines = [</span><br><span class="line">  <span class="string">&quot;`DEVOCEAN`은 SK그룹의 대표 개발자 커뮤니티이자🧑&quot;</span>,</span><br><span class="line">  <span class="string">&quot;내/외부 개발자 간 소통과 성장을 위한 플랫폼을 상징합니다.👋&quot;</span>,</span><br><span class="line">  <span class="string">&quot;`Developers`&#x27; Ocean 개발자들을 위한 영감의 바다🙏&quot;</span>,</span><br><span class="line">  <span class="string">&quot;`Devotion` 헌신,몰두,전념💯&quot;</span>,</span><br><span class="line">  <span class="string">&quot;`Technology for Everyone` 모두를 위한 기술👍&quot;</span>,  </span><br><span class="line"><span class="comment">#   &quot;🧜‍♂️🧚‍♀️🧚🧚‍♂️👼🤰🧟‍♂️🧞‍♀️🧞🧞‍♂️🧜‍♀️🧜🧝‍♂️🧛‍♀️🧛ꎐదꁯᛠ፨ꏍ &quot;</span></span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">    tokens = tokenizer.tokenize(line)</span><br><span class="line">    inputs = tokenizer(line)    </span><br><span class="line">    decoded_sequence = tokenizer.decode(inputs[<span class="string">&#x27;input_ids&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(line)</span><br><span class="line">    <span class="built_in">print</span>(tokens)    </span><br><span class="line">    <span class="built_in">print</span>(decoded_sequence)</span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure>

<h1 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h1><p><img src="https://devocean.sk.com/editorImg/2023/2/20/4479c7a1647f4bcec7112ff2e02d60c566811ccddc64c0393294ae61a8559653" alt="image.png"></p>
<ul>
<li>기존과 달리 🧑👋🙏💯👍와 같은 토큰들이 제대로 복원된 것을 확인할 수 있습니다</li>
</ul>
<h1 id="맺으며"><a href="#맺으며" class="headerlink" title="맺으며"></a>맺으며</h1><p>오늘은 자연어처리에서 가장 많이 쓰이는 라이브러리중 하나인 SentencePiece와 Huggingface의 transformers를 통해 토크나이저를 학습하고 OOV 문제를 해결하는 방법을 다뤘습니다. 가독성 있는 토큰화된 결과와 OOV 이슈를 해소하기 원한다면 <code>SentencePiece CBPE</code> + <code>byte_fallback=true</code> 옵션을 고려해보실 수 있을 것 같습니다.</p>
<h2 id="참고자료"><a href="#참고자료" class="headerlink" title="참고자료"></a>참고자료</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">google&#x2F;sentencepiece</a></li>
<li><a target="_blank" rel="noopener" href="https://wikidocs.net/22592">딥 러닝을 이용한 자연어 처리 입문</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기</p><p><a href="https://eagle705.github.io/SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기/">https://eagle705.github.io/SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Joosung Yoon</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-02-20</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-03-04</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/nlp/">nlp</a></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=63297c6228f9450019a5f574&amp;product=sop" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/Alpaca/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Alpaca (A Strong Instruction-Following Model)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/Toolformer/"><span class="level-item">Toolformer: Language Models Can Teach Themselves to Use Tools</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://eagle705.github.io/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/';
            this.page.identifier = 'SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eagle705-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/eagle705-logo.png" alt="Joosung Yoon"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Joosung Yoon</p><p class="is-size-6 is-block">Machine Learning Engineer</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>There and Back Again</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">54</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">10</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/eagle705" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/eagle705"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/hisdevelopers"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/JSYoon53859120"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/joosung-yoon/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/ML/"><span class="level-start"><span class="level-item">ML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/cslog/"><span class="level-start"><span class="level-item">cslog</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/categories/photo/"><span class="level-start"><span class="level-item">photo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">5월 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">3월 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">2월 2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">1월 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">12월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">11월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">10월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">9월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">8월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">5월 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">12월 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">10월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6월 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/12/"><span class="level-start"><span class="level-item">12월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/11/"><span class="level-start"><span class="level-item">11월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/10/"><span class="level-start"><span class="level-item">10월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/09/"><span class="level-start"><span class="level-item">9월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/08/"><span class="level-start"><span class="level-item">8월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/07/"><span class="level-start"><span class="level-item">7월 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/05/"><span class="level-start"><span class="level-item">5월 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/04/"><span class="level-start"><span class="level-item">4월 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/11/"><span class="level-start"><span class="level-item">11월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/06/"><span class="level-start"><span class="level-item">6월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/05/"><span class="level-start"><span class="level-item">5월 2018</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/04/"><span class="level-start"><span class="level-item">4월 2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ML/"><span class="tag">ML</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/drone/"><span class="tag">drone</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/grpc/"><span class="tag">grpc</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/linux/"><span class="tag">linux</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">43</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp-kb/"><span class="tag">nlp, kb</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%83%9D%EA%B0%81%EC%A0%95%EB%A6%AC/"><span class="tag">생각정리</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="adsense"><div class="card-content"><div class="menu"><h3 class="menu-label">광고</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-2655870716902046" data-ad-slot="2764253071" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">카탈로그</h3><ul class="menu-list"><li><a class="level is-mobile" href="#소개"><span class="level-left"><span class="level-item">1</span><span class="level-item">소개</span></span></a><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#텍스트-전처리-과정-예시"><span class="level-left"><span class="level-item">1.1.1</span><span class="level-item">텍스트 전처리 과정 예시</span></span></a></li></ul></ul></li><li><a class="level is-mobile" href="#설치방법"><span class="level-left"><span class="level-item">2</span><span class="level-item">설치방법</span></span></a></li><li><a class="level is-mobile" href="#학습방법"><span class="level-left"><span class="level-item">3</span><span class="level-item">학습방법</span></span></a></li><li><a class="level is-mobile" href="#학습된-토크나이저-사용하기"><span class="level-left"><span class="level-item">4</span><span class="level-item">학습된 토크나이저 사용하기</span></span></a></li><li><a class="level is-mobile" href="#결과"><span class="level-left"><span class="level-item">5</span><span class="level-item">결과</span></span></a></li><li><a class="level is-mobile" href="#맺으며"><span class="level-left"><span class="level-item">6</span><span class="level-item">맺으며</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#참고자료"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">참고자료</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:53.000Z">2023-05-09</time></p><p class="title"><a href="/pythia/">Pythia (A Suite for Analyzing Large Language Models Across Training and Scaling)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:39.000Z">2023-05-09</time></p><p class="title"><a href="/llama/">LLaMA (Open and Efficient Foundation Language Models)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-05-09T02:54:01.000Z">2023-05-09</time></p><p class="title"><a href="/ia3/">(IA3) Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-03-23T08:14:37.000Z">2023-03-23</time></p><p class="title"><a href="/Alpaca/">Alpaca (A Strong Instruction-Following Model)</a></p><p class="categories"><a href="/categories/paper/">paper</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-20T13:07:50.000Z">2023-02-20</time></p><p class="title"><a href="/SentencePiece%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EA%B3%BC%EC%A0%81%EC%9D%B8%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80%20%EB%A7%8C%EB%93%A4%EA%B8%B0/">SentencePiece를 활용한 효과적인 한국어 토크나이저 만들기</a></p><p class="categories"><a href="/categories/ML/">ML</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/eagle705-logo.png" alt="Luke&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 Joosung Yoon</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>